{"id": "kp20k_training_0", "title": "virtually enhancing the perception of user actions", "abstract": "This paper proposes using virtual reality to enhance the perception of actions by distant users on a shared application. Here, distance may refer either to space ( e.g. in a remote synchronous collaboration) or time ( e.g. during playback of recorded actions). Our approach consists in immersing the application in a virtual inhabited 3D space and mimicking user actions by animating avatars. We illustrate this approach with two applications, the one for remote collaboration on a shared application and the other to playback recorded sequences of user actions. We suggest this could be a low cost enhancement for telepresence", "keywords": ["telepresence", "animation", "avatars", "application sharing", "collaborative virtual environments"]}
{"id": "kp20k_training_1", "title": "Dynamic range improvement of multistage multibit Sigma Delta modulator for low oversampling ratios", "abstract": "This paper presents an improved architecture of the multistage multibit sigma-delta modulators (EAMs) for wide-band applications. Our approach is based on two resonator topologies, high-Q cascade-of-resonator-with-feedforward (HQCRFF) and low-Q cascade-of-integrator-with-feedforward (LQCEFF). Because of in-band zeros introduced by internal loop filters, the proposed architecture enhances the suppression of the in-band quantization noise at a low OSR. The HQCRFF-based modulator with single-bit quantizer has two modes of operation, modulation and oscillation. When the HQCRFF-based modulator is operating in oscillation mode, the feedback path from the quantizer output to the input summing node is disabled and hence the modulator output is free of the quantization noise terms. Although operating in oscillation mode is not allowed for single-stage SigmaDeltaM, the oscillation of HQCRFF-based modulator can improve dynamic range (DR) of the multistage (MASH) SigmaDeltaM. The key to improving DR is to use HQCRFF-based modulator in the first stage and have the first stage oscillated. When the first stage oscillates, the coarse quantization noise vanishes and hence circuit nonidealities, such as finite op-amp gain and capacitor mismatching, do not cause leakage quantization noise problem. According to theoretical and numerical analysis, the proposed MASH architecture can inherently have wide DR without using additional calibration techniques", "keywords": ["sigma delta modulators", "analog-to-digital converters ", "multistage ", "multibit quantizer", "dynamic range improvement"]}
{"id": "kp20k_training_2", "title": "An ontology modelling perspective on business reporting", "abstract": "In this paper, we discuss the motivation and the fundamentals of an ontology representation of business reporting data and metadata structures as defined in the eXtensible business reporting language (XBRL) standard. The core motivation for an ontology representation is the enhanced potential for integrated analytic applications that build on quantitative reporting data combined with structured and unstructured data from additional sources. Applications of this kind will enable significant enhancements in regulatory compliance management, as they enable business analytics combined with inference engines for statistical, but also for logical inferences. In order to define a suitable ontology representation of business reporting language structures, an analysis of the logical principles of the reporting metadata taxonomies and further classification systems is presented. Based on this analysis, a representation of the generally accepted accounting principles taxonomies in XBRL by an ontology provided in the web ontology language (OWL) is proposed. An additional advantage of this representation is its compliance with the recent ontology definition metamodel (ODM) standard issued by OMG", "keywords": ["enterprise information integration and interoperability", "languages for conceptual modelling", "ontological approaches to content and knowledge management", "ontology-based software engineering for enterprise solutions", "domain engineering"]}
{"id": "kp20k_training_3", "title": "The self-organizing map", "abstract": "An overview of the self-organizing map algorithm, on which the papers in this issue are based, is presented in this article", "keywords": ["self-organizing map", "learning vector quantization"]}
{"id": "kp20k_training_4", "title": "The Amygdala and Development of the Social Brain", "abstract": "The amygdala comprises part of an extended network of neural circuits that are critically involved in the processing of socially salient stimuli. Such stimuli may be explicitly social, such as facial expressions, or they may be only tangentially social, such as abstract shapes moving with apparent intention relative to one another. The coordinated interplay between neural activity in the amygdala and other brain regions, especially the medial prefrontal cortex, the occipitofrontal cortex, the fusiform gyrus, and the superior temporal sulcus, allows us to develop social responses and to engage in social behaviors appropriate to our species. The harmonious functioning of this integrated social cognitive network may be disrupted by congenital or acquired lesions, by genetic anomalies, and by exceptional early experiences. Each form of disruption is associated with a slightly different outcome, dependent on the timing of the experience, the location of the lesion, or the nature of the genetic anomaly. Studies in both humans and primates concur; the dysregulation of basic emotions, especially the processing of fear and anger, is an almost invariable consequence of such disruption. These, in turn, have direct or indirect consequences for social behavior", "keywords": ["social brain", "amygdala", "behavior", "facial expression"]}
{"id": "kp20k_training_5", "title": "Modeling and Cost Analysis of an Improved Movement-Based Location Update Scheme in Wireless Communication Networks", "abstract": "A movement based location update (MBLU) scheme is an LU scheme, under which a user equipment (UE) performs an LU when the number of cells crossed by the UE reaches a movement threshold. The MBLU scheme suffers from ping-pong LU effect. The ping-pong LU effect arises when the UE that moves repetitively between two adjacent cells performs LUs in the same way as in the case of straight movement. To tackle the ping-pong LU effect encountered by the original MBLU (OMBLU) scheme, an improved MBLU (IMBLU) scheme was proposed in the literature. Under the IMBLU scheme, the UE performs an LU when the number of different cells, rather than the number of cells, visited by the UE reaches the movement threshold. In this paper we develop an embedded Markov chain model to calculate the signaling cost of the IMBLU scheme. We derive analytical formulas for the signaling cost, whose accuracy is tested through simulation. It is observed from a numerical study based on these formulas that 1) the signaling cost is a downward convex function of the movement threshold, implying the existence of an optimal movement threshold that can minimize the signaling cost, and that 2) the reduction in the signaling cost achieved by the IMBLU scheme relative to the OMBLU scheme is more prominent in the case of stronger repetitiveness in the UE movement. The model developed and the formulas derived in this paper can guide the implementation of the IMBLU scheme in wireless communication networks", "keywords": ["movement-based location update", "ping-pong lu effect", "modeling", "embedded markov chain"]}
{"id": "kp20k_training_6", "title": "A modified offset roll printing for thin film transistor applications", "abstract": "In order to realize a high resolution and high throughput printing method for thin film transistor application, a modified offset roll printing was studied. This roll printing chiefly consists of a blanket with low surface energy and a printing plate (clich) with high surface energy. In this study, a finite element analysis was done to predict the blanket deformation and to find the optimal angle of clichs sidewall. Various etching methods were investigated to obtain a high resolution clich and the surface energy of the blanket and clich was analyzed for ink transfer. A high resolution clich with the sidewall angle of 90 and the intaglio depth of 13?m was fabricated by the deep reactive ion etching method. Based on the surface energy analysis, we extracted the most favorable condition to transfer inks from a blanket to a clich, and thus thin films were deposited on a Si-clich to increase the surface energy. Through controlling roll speed and pressure, two inks, etch-resist and silver paste, were printed on a rigid substrate, and the fine patterns of 10?m width and 6?m line spacing were achieved. By using this printing process, the top gate amorphous indiumgalliumzinc-oxide TFTs with channel width/length of 12/6?m were successfully fabricated by printing etch-resists", "keywords": ["printing plate ", "surface energy", "offset roll printing", "tft"]}
{"id": "kp20k_training_8", "title": "Hyperspectral image segmentation through evolved cellular automata", "abstract": "Efficient segmentation of hyperspectral images through the use of cellular automata. The rule set for the CA is automatically obtained using an evolutionary algorithm. Synthetic images of much lower dimensionality are used to evolve the automata. The CA works with spectral information but do not project it onto a lower dimension. The CA-based classification outperforms reference techniques", "keywords": ["hyperspectral imaging", "evolution", "cellular automata", "segmentation"]}
{"id": "kp20k_training_9", "title": "Analytical and empirical evaluation of the impact of Gaussian noise on the modulations employed by Bluetooth Enhanced Data Rates", "abstract": "Bluetooth (BT) is a leading technology for the deployment of wireless Personal Area Networks and Body Area Networks. Versions 2.0 and 2.1 of the standard, which are massively implemented in commercial devices, improve the throughput of the BT technology by enabling the so-called Enhanced Data Rates (EDR). EDRs are achieved by utilizing new modulation techniques (?/4-DQPSK and 8-DPSK), apart from the typical Gaussian Frequency Shift Keying modulation supported by previous versions of BT. This manuscript presents and validates a model to characterize the impact of white noise on the performance of these modulations. The validation is systematically accomplished in a testbed with actual BT interfaces and a calibrated white noise generator", "keywords": ["bluetooth", "bit error rate", "modulation", "white noise"]}
{"id": "kp20k_training_10", "title": "Spectral analysis of irregularly-sampled data: Paralleling the regularly-sampled data approaches", "abstract": "The spectral analysis of regularly-sampled (RS) data is a well-established topic, and many useful methods are available for performing it under different sets of conditions. The same cannot be said about the spectral analysis of irregularly-sampled (IS) data: despite a plethora of published works on this topic, the choice of a spectral analysis method for IS data is essentially limited, on either technical or computational grounds, to the periodogram and its variations. In our opinion this situation is far from satisfactory, given the importance of the spectral analysis of IS data for a considerable number of applications in such diverse fields as engineering, biomedicine, economics, astronomy, seismology, and physics, to name a few. In this paper we introduce a number of IS data approaches that parallel the methods most commonly used for spectral analysis of RS data: the periodogram (PER), the Capon method (CAP), the multiple-signal characterization method (MUSIC), and the estimation of signal parameters via rotational invariance technique (ESPRIT). The proposed IS methods are as simple as their RS counterparts, both conceptually and computationally. In particular, the fast algorithms derived for the implementation of the RS data methods can be used mutatis mutandis to implement the proposed parallel IS methods. Moreover, the expected performance-based ranking of the IS methods is the same as that of the parallel RS methods: all of them perform similarly on data consisting of well-separated sinusoids in noise, MUSIC and ESPRIT outperform the other methods in the case of closely-spaced sinusoids in white noise, and CAP outperforms PER for data whose spectrum has a small-to-medium dynamic range (MUSIC and ESPRIT should not be used in the latter case", "keywords": ["spectral analysis", "irregular sampling", "nonuniform sampling", "sinusoids in noise", "carma signals"]}
{"id": "kp20k_training_11", "title": "Time-Series Data Mining", "abstract": "In almost every scientific field, measurements are performed over time. These observations lead to a collection of organized data called time series. The purpose of time-series data mining is to try to extract all meaningful knowledge from the shape of data. Even if humans have a natural capacity to perform these tasks, it remains a complex problem for computers. In this article we intend to provide a survey of the techniques applied for time-series data mining. The first part is devoted to an overview of the tasks that have captured most of the interest of researchers. Considering that in most cases, time-series task relies on the same components for implementation, we divide the literature depending on these common aspects, namely representation techniques, distance measures, and indexing methods. The study of the relevant literature has been categorized for each individual aspects. Four types of robustness could then be formalized and any kind of distance could then be classified. Finally, the study submits various research trends and avenues that can be explored in the near future. We hope that this article can provide a broad and deep understanding of the time-series data mining research field", "keywords": ["algorithms", "performance", "distance measures", "data indexing", "data mining", "query by content", "sequence matching", "similarity measures", "stream analysis", "temporal analysis", "time series"]}
{"id": "kp20k_training_12", "title": "A small hybrid JIT for embedded systems", "abstract": "Just-In-Time (JIT) Compilation is a technology used to improve speed of virtual machines that support dynamic loading of applications. It is better known as a technique that accelerates Java programs. Current JIT compilers are either huge in size or compile complete methods of the application requiring large amounts of memory for their functioning. This has made Java Virtual Machines for embedded systems devoid of JIT compilers. This paper explains a simple technique of combining interpretation with compilation to get a hybrid interpretation strategy. It also describes a new code generation technique that works using its self-code. The combination gives a JIT compiler that is very small (10K) and suitable for Java Virtual Machines for embedded systems", "keywords": ["java", "dynamic compilation", "jit", "embedded system", "code generation"]}
{"id": "kp20k_training_13", "title": "Rationality of induced ordered weighted operators based on the reliability of the source of information in group decision-making", "abstract": "The aggregation of preference relations in group decision-making (GDM) problems can be carried out based on either the reliability of the preference values to be aggregated, as is the case with ordered weighted averaging operators, or on the reliability of the source of information that provided the preferences, as is the case with weighted mean operators. In this paper, we address the problem of aggregation based on the reliability of the source of information, with a double aim: a) To provide a general framework for induced ordered weighted operators based upon the source of information, and b) to provide a study of their rationality. We study the conditions which need to be verified by an aggregation operator in order to maintain the rationality assumptions on the individual preferences in the aggregation phase of the selection process of alternatives. In particular, we show that any aggregation operator based on the reliability of the source of information does verify these conditions", "keywords": ["aggregation operators", "induced aggregation", "group decision-making", "preference relations", "rationality", "consistency"]}
{"id": "kp20k_training_14", "title": "Digital preservation of knowledge in the public sector: a pre-ingest tool", "abstract": "This paper describes the need for coordinating pre-ingest activities in digital preservation of archival records. As a result of the wide use of electronic records management systems (ERMS) in agencies, the focus is on several issues relating to the interaction of the agencys ERMS and public repositories. This paper indicates the importance of using digital recordkeeping metadata to meet more precisely and at the same time semi-automatically the criteria set by memory institutions. The paper provides an overview of one prospective solution and describes the Estonian National Archives universal archiving module (UAM). A case study reports the use of the UAM in preserving the digital records of the Estonian Minister for Population and Ethnic Affairs. In this project, the preparation and transfer of archival records was divided into ten phases, starting from the description of the archival creator and ending with controlled transfer. The case study raises questions about how much recordkeeping metadata can be used in archival description and how the interaction of the agencys ERMS and ingest by the archives could be more automated. The main issues (e.g. classification, metadata elements variations, mapping, and computer files conversions) encountered during that project are discussed. Findings show that the Open Archival Information System functional models ingest part should be reconceptualised to take into account preparatory work. Adding detailed metadata about the structure, context and relationships in the right place at the right time could get one step closer to digital codified knowledge archiving by creating synergies with various other digital repositories", "keywords": ["digital preservation", "ingest", "pre-ingest", "universal archiving module", "estonia"]}
{"id": "kp20k_training_15", "title": "The Regulation of SERCA-Type Pumps by Phospholamban and Sarcolipin", "abstract": "Both sarcolipin (SLN) and phospholamban (PLN) lower the apparent affinity of either SERCA1a or SERCA2a for Ca2+. Since SLN and PLN are coexpressed in the heart, interactions among these three proteins were investigated. When SERCA1a or SERCA2a were coexpressed in HEK-293 cells with both SLN and PLN, superinhibition resulted. The ability of SLN to elevate the content of PLN monomers accounts, at least in part, for the superinhibitory effects of SLN in the presence of PLN. To evaluate the role of SLN in skeletal muscle, SLN cDNA was injected directly into rat soleus muscle and force characteristics were analyzed. Overexpression of SLN resulted in significant reductions in both twitch and tetanic peak force amplitude and maximal rates of contraction and relaxation and increased fatigability with repeated electrical stimulation. Ca2+ uptake in muscle homogenates was impaired, suggesting that overexpression of SLN may reduce the sarcoplasmic reticulum Ca2+ store. SLN and PLN appear to bind to the same regulatory site in SERCA. However, in a ternary complex, PLN occupies the regulatory site and SLN binds to the exposed side of PLN and to SERCA", "keywords": ["ca2+-atpase", "sarcolipin", "phospholamban", "cardiomyopathy", "regulatory molecules"]}
{"id": "kp20k_training_16", "title": "Bootstrap confidence intervals for principal response curves", "abstract": "The principal response curve (PRC) model is of use to analyse multivariate data resulting from experiments involving repeated sampling in time. The time-dependent treatment effects are represented by PRCs, which are functional in nature. The sample PRCs can be estimated using a raw approach, or the newly proposed smooth approach. The generalisability of the sample PRCs can be judged using confidence bands. The quality of various bootstrap strategies to estimate such confidence bands for PRCs is evaluated. The best coverage was obtained with BCa BC a intervals using a non-parametric bootstrap. The coverage appeared to be generally good, except for the case of exactly zero population PRCs for all conditions. Then, the behaviour is irregular, which is caused by the sign indeterminacy of the PRCs. The insights obtained into the optimal bootstrap strategy are useful to apply in the PRC model, and more generally for estimating confidence intervals in singular value decomposition based methods", "keywords": ["resampling", "singular value decomposition"]}
{"id": "kp20k_training_17", "title": "A concurrent specification of Brzozowski's DFA construction algorithm", "abstract": "In this paper two concurrent versions of Brzozowski's deterministic finite automaton (DFA) construction algorithm are developed from first principles, the one being a slight refinement of the other. We rely on Hoare's CSP as our notation. The specifications that are proposed of the Brzozowski algorithm are in terms of the concurrent composition of a number of top-level processes, each participating process itself composed of several other concurrent processes. After considering a number of alternatives, this particular overall architectural structure seemed like a natural and elegant mapping from the sequential algorithm's structure. While we have carefully argued the reasons for constructing the concurrent versions as proposed in the paper, there are of course, a large range of alternative design choices that could be made. There might also be scope for a more fine-grained approach to updating sets or checking for similarity of regular expressions. At this stage, we have chosen to abstract away from these considerations, and leave their exploration for a subsequent step in our research", "keywords": ["automaton construction", "concurrency", "csp", "regular expressions"]}
{"id": "kp20k_training_18", "title": "collaborative multimedia learning environments", "abstract": "I use the term \"collaborative\", to identify a way that enables conversation to occur in, about, and around the digital medium, therefore making the \"digital artifacts\" contributed by all individuals a key element of a conversation as opposed to consecutive, linear presentations used by most faculty at the Design School.Installations of collaborative multimedia in classrooms at the Harvard University Graduate School of Design show an enhancement of the learning process via shared access to media resources and enhanced spatial conditions within which these resources are engaged. Through observation and controlled experiments I am investigating how the use of shared, collaborative interfaces for interaction with multiple displays in a co-local environment enhances the learning process. The multiple spatial configurations and formats of learning mandate that with more effective interfaces and spaces for sharing digital media with fellow participants, the classroom can be used much more effectively and thus, learning and interaction with multimedia can be improved", "keywords": ["shared interfaces", "multiple displays", "digital artifacts", "rich control interfaces", "beneficial interruption"]}
{"id": "kp20k_training_19", "title": "clustering multi-way data via adaptive subspace iteration", "abstract": "Clustering multi-way data is a very important research topic due to the intrinsic rich structures in real-world datasets. In this paper, we propose the subspace clustering algorithm on multi-way data, called ASI-T (Adaptive Subspace Iteration on Tensor). ASI-T is a special version of High Order SVD (HOSVD), and it simultaneously performs subspace identification using 2DSVD and data clustering using K-Means. The experimental results on synthetic data and real-world data demonstrate the effectiveness of ASI-T", "keywords": ["tensor", "multi-way data", "subspace", "clustering"]}
{"id": "kp20k_training_20", "title": "SAT-based model-checking for security protocols analysis", "abstract": "We present a model checking technique for security protocols based on a reduction to propositional logic. At the core of our approach is a procedure that, given a description of the protocol in a multi-set rewriting formalism and a positive integer k, builds a propositional formula whose models (if any) correspond to attacks on the protocol. Thus, finding attacks on protocols boils down to checking a propositional formula for satisfiability, problem that is usually solved very efficiently by modern SAT solvers. Experimental results indicate that the approach scales up to industrial strength security protocols with performance comparable with (and in some cases superior to) that of other state-of-the-art protocol analysers", "keywords": ["security protocols", "bounded model checking", "sat-based model checking", "multi-set rewriting"]}
{"id": "kp20k_training_21", "title": "Existence of positive solutions for 2n 2 n th-order singular superlinear boundary value problems", "abstract": "This paper investigates the existence of positive solutions for 2n th-order (n>1 ) singular superlinear boundary value problems. A necessary and sufficient condition for the existence of C2n?2[0,1] as well as C2n?1[0,1] positive solutions is given by constructing a special cone and with the e-Norm", "keywords": ["singular boundary value problem", "positive solution", "e-norm", "cone", "fixed-point theorem"]}
{"id": "kp20k_training_22", "title": "Embedding the Internet in the lives of college students - Online and offline Behavior", "abstract": "The Internet is increasingly becoming embedded in the lives of most American citizens. College students constitute a group who have made particularly heavy use of the technology for everything from downloading music to distance education to instant messaging. Researchers know a lot about the uses made of the Internet by this group of people but less about the relationship between their offline activities and online behavior. This study reports the results of a web survey of a group of university undergraduates exploring the nature of both online and offline in five areas-the use of news and information, the discussion of politics, the seeking of health information, the use of blogs, and the downloading of media and software", "keywords": ["internet", "college students", "information technology", "online behavior", "news", "blogs", "downloading"]}
{"id": "kp20k_training_23", "title": "A capacitive tactile sensor array for surface texture discrimination", "abstract": "This paper presents a silicon MEMS based capacitive sensing array, which has the ability to resolve forces in the sub mN range, provides directional response to applied loading and has the ability to differentiate between surface textures. Texture recognition is achieved by scanning surfaces over the sensing array and assessing the frequency spectrum of the sensor outputs", "keywords": ["mems", "tactile sensor", "biomimetic", "capacitive sensor"]}
{"id": "kp20k_training_24", "title": "Segmenting, modeling, and matching video clips containing multiple moving objects", "abstract": "This paper presents a novel representation for dynamic scenes composed of multiple rigid objects that may undergo different motions and are observed by a moving camera. Multiview constraints associated with groups of affine-covariant scene patches and a normalized description of their appearance are used to segment a scene into its rigid components, construct three-dimensional models of these components, and match instances of models recovered from different image sequences. The proposed approach has been applied to the detection and matching of moving objects in video sequences and to shot matching, i.e., the identification of shots that depict the same scene in a video clip", "keywords": ["affine-covariant patches", "structure from motion", "motion segmentation", "shot matching", "video retrieval"]}
{"id": "kp20k_training_25", "title": "Weighted fuzzy interpolative reasoning systems based on interval type-2 fuzzy sets", "abstract": "In this paper, we present a weighted fuzzy interpolative reasoning method for sparse fuzzy rule-based systems based on interval type-2 fuzzy sets. We also apply the proposed weighted fuzzy interpolative reasoning method to deal with the truck backer-upper control problem. The proposed method satisfies the seven evaluation indices for fuzzy interpolative reasoning. The experimental results show that the proposed method outperforms the existing methods. It provides us with a useful way for dealing with fuzzy interpolative reasoning in sparse fuzzy rule-based systems", "keywords": ["fuzzy interpolative reasoning", "interval type-2 fuzzy sets", "sparse fuzzy rule-based systems", "weighted fuzzy interpolative reasoning"]}
{"id": "kp20k_training_26", "title": "Vestibular PREHAB", "abstract": "A sudden unilateral loss or impairment of vestibular function causes vertigo, dizziness, and impaired postural function. In most occasions, everyday activities supported or not by vestibular rehabilitation programs will promote a compensation and the symptoms subside. As the compensatory process requires sensory input, matching performed motor activity, both motor learning of exercises and matching to sensory input are required. If there is a simultaneous cerebellar lesion caused by the tumor or the surgery of the posterior cranial fossa, there may be a risk of a combined vestibulocerebellar lesion, with reduced compensatory abilities and with prolonged or sometimes permanent disability. On the other hand, a slow gradual loss of unilateral function occurring as the subject continues well-learned everyday activities may go without any prominent symptoms. A pretreatment plan was therefore implemented before planned vestibular lesions, that is, PREHAB. This was first done in subjects undergoing gentamicin treatment for morbus Mnire. Subjects would perform vestibular exercises for 14 days before the first gentamicin installation, and then continue doing so until free of symptoms. Most subjects would only experience slight dizziness while losing vestibular function. The approachwhich is reported herewas then expanded to patients with pontine-angle tumors requiring surgery, but with remaining vestibular function to ease postoperative symptoms and reduce risk of combined cerebellovestibular lesions. Twelve patients were treated with PREHAB and had gentamicin installations transtympanically. In all cases there was a caloric loss, loss of VOR in head impulse tests, and impaired subjective vertical and horizontal. Spontaneous, positional nystagmus, subjective symptoms, and postural function were normalized before surgery and postoperative recovery was swift. Pretreatment training with vestibular exercises continued during the successive loss of vestibular function during gentamicin treatment, and pre-op gentamicin ablation of vestibular function offers a possibility to reduce malaise and speed up recovery", "keywords": ["vestibular", "compensation", "prehab", "rehabilitation", "schwannoma", "recovery"]}
{"id": "kp20k_training_27", "title": "SW1PerS: Sliding windows and 1-persistence scoring; discovering periodicity in gene expression time series data", "abstract": "Identifying periodically expressed genes across different processes (e.g. the cell and metabolic cycles, circadian rhythms, etc) is a central problem in computational biology. Biological time series may contain (multiple) unknown signal shapes of systemic relevance, imperfections like noise, damping, and trending, or limited sampling density. While there exist methods for detecting periodicity, their design biases (e.g. toward a specific signal shape) can limit their applicability in one or more of these situations", "keywords": ["periodicity", "gene expression", "time series", "sliding windows", "persistent homology"]}
{"id": "kp20k_training_28", "title": "Parallel generation of unstructured surface grids", "abstract": "In this paper, a new grid generation system is presented for the parallel generation of unstructured triangular surface grids. The object-oriented design and implementation of the system, the internal components and the parallel meshing process itself are described. Initially in a rasterisation stage, the geometry to be meshed is analysed and a smooth distribution of local element sizes in 3-D space is set up automatically and stored in a Cartesian mesh. This background mesh is used by the advancing front surface mesher as spacing definition for the triangle generation. Both the rasterisation and the meshing are MPI-parallelised. The underlying principles and strategies will be outlined together with the advantages and limitations of the approach. The paper will be concluded with examples demonstrating the capabilities of the presented approach", "keywords": ["unstructured surface mesh generation", "geometry rasterisation", "mpi-parallel", "automatic", "object-orientation"]}
{"id": "kp20k_training_29", "title": "H structured model reduction algorithms for linear discrete systems via LMI-based optimisation", "abstract": "In this article, H structured model reduction is addressed for linear discrete systems. Two important classes of systems are considered for structured model reduction, i.e. Markov jump systems and uncertain systems. The problem we deal with is the development of algorithms with the flexibility to allow any structure in the reduced-order system design, such as the structure of an original system, decentralisation of a networked system, pole assignment of the reduced system, etc. The algorithms are derived such that an associated model reduction error guarantees to satisfy a prescribed H norm-bound constraint. A new condition for the existence of desired reduced-order models preserving a certain structure is presented in a set of linear matrix inequalities (LMI) and non-convex equality constraints. Effective computational algorithms involving LMI are suggested to solve the matrix inequalities characterising a solution of the structured model reduction problem. Numerical examples demonstrate the advantages of the proposed model reduction method", "keywords": ["h design", "structured model reduction", "markovian jump linear systems", "uncertain systems", "bilinear matrix inequality", "linear matrix inequality"]}
{"id": "kp20k_training_30", "title": "A power-aware code-compression design for RISC/VLIW architecture", "abstract": "We studied the architecture of embedded computing systems from the viewpoint of power consumption in memory systems and used a selective-code-compression (SCC) approach to realize our design. Based on the LZW (Lempel-Ziv-Welch) compression algorithm, we propose a novel cost effective compression and decompression method. The goal of our study was to develop a new SCC approach with an extended decision policy based on the prediction of power consumption. Our decompression method had to be easily implemented in hardware and to collaborate with the embedded processor. The hardware implementation of our decompression engine uses the TSMC 0.18 mu m-2p6m model and its cell-based libraries. To calculate power consumption more accurately, we used a static analysis method to estimate the power overhead of the decompression engine. We also used variable sized branch blocks and considered several features of very long instruction word (VLIW) processors for our compression, including the instruction level parallelism (ILP) technique and the scheduling of instructions. Our code-compression methods are not limited to VLIW machines, and can be applied to other kinds of reduced instruction set computer (RISC) architecture", "keywords": ["lzw compression", "cell-based libraries", "instruction level parallelism ", "vliw processors"]}
{"id": "kp20k_training_31", "title": "Globallocal negotiations for implementing configurable packages: The power of initial organizational decisions", "abstract": "The purpose of this paper is to draw attention to the critical influence that initial organizational decisions regarding power and knowledge balance between internal members and external consultants have on the globallocal negotiation that characterizes configurable packages implementation. To do this, we conducted an intensive research study of a configurable information technology (IT) implementation project in a Canadian firm", "keywords": ["configurable technology", "erp implementation", "critical discourse analysis", "temporal bracketing analysis", "intensive research", "qualitative research methods", "global/local negotiation", "power/knowledge balance"]}
{"id": "kp20k_training_32", "title": "Communication in Random Geometric Radio Networks with Positively Correlated Random Faults", "abstract": "We study the feasibility and time of coin communication in random geometric radio networks, where nodes fail randomly with positive correlation. We consider a set of radio stations with the same communication range, distributed in a random uniform way on a unit square region. In order to capture fault dependencies, we introduce the ranged spot model in which damaging events, called spots, occur randomly and independently on the region, causing faults in all nodes located within distance s from them. Node faults within distance 2s become dependent in this model and are positively correlated. We investigate the impact of the spot arrival rate on the feasibility and the time of communication in the fault-free part of the network. We provide an algorithm which broadcasts correctly with probability l - epsilon in faulty random geometric radio networks of diameter D in time O (D + log l/epsilon", "keywords": ["fault-tolerance", "dependent faults", "broadcast", "crash faults", "random", "geometric radio network"]}
{"id": "kp20k_training_33", "title": "Consumer complaint behaviour in telecommunications: The case of mobile phone users in Spain", "abstract": "Consumer complaint behaviour theory is used to analyze Spanish telecommunications data. The main determinants are the type of problem, socio-demographic factors and the user?s type of contract. Proper complaint handling leads to satisfied, loyal and profitable consumers", "keywords": ["consumer complaint behaviour", "mobile phones", "consumer retention", "consumer satisfaction", "consumer loyalty", "voice", "exit", "service failure", "complainers"]}
{"id": "kp20k_training_34", "title": "Combining OWL ontologies using epsilon-Connections", "abstract": "The standardization of the Web Ontology Language ( OWL) leaves ( at least) two crucial issues for Web-based ontologies unsatisfactorily resolved, namely how to represent and reason with multiple distinct, but linked ontologies, and how to enable effective knowledge reuse and sharing on the Semantic Web. In this paper, we present a solution for these fundamental problems based on E- Connections. We aim to use E- Connections to provide modelers with suitable means for developing Web ontologies in a modular way and to provide an alternative to the owl: imports construct. With such motivation, we present in this paper a syntactic and semantic extension of the Web Ontology language that covers E- Connections of OWL-DL ontologies. We show how to use such an extension as an alternative to the owl: imports construct in many modeling situations. We investigate different combinations of the logics SHIN( D), SHON( D) and SHIO( D) for which it is possible to design and implement reasoning algorithms, well- suited for optimization. Finally, we provide support for E-Connections in both an ontology editor, SWOOP, and an OWL reasoner, Pellet. ", "keywords": ["web ontology language", "integration and combination of ontologies", "combination of knowledge representation formalisms", "description logics reasoning"]}
{"id": "kp20k_training_35", "title": "Model with artificial neural network to predict the relationship between the soil resistivity and dry density of compacted soil", "abstract": "This paper presents a technique to obtain the outcomes of soil dry density and optimum moisture contents with artificial neural network (ANN) for compacted soil monitoring through soil resistivity measurement in geotechnical engineering. The compacted soil monitoring through soil electrical resistivity shows the important role in the construction of highway embankments, earth dams and many other engineering structure. Generally, soil compaction is estimated through the determination of maximum dry density at optimum moisture contents in laboratory test. To estimate the soil compaction in conventional soil monitoring technique is time consuming and costly for the laboratory testing with a lot of samples of compacted soil. In this work, an ANN model is developed for predicting the relationship between dry density of compacted soil and soil electrical resistivity based on experimental data in soil profile. The regression analysis between the output and target values shows that the R-2 values are 0.99 and 0.93 for the training and testing sets respectively for the implementation of ANN in soil profile. The significance of our research is to obtain an intelligent model for getting faster, cost-effective and consistent outcomes in soil compaction monitoring through electrical resistivity for a wide range of applications in geotechnical investigation", "keywords": ["soil compaction", "ann modeling", "electrical resistivity", "dry density"]}
{"id": "kp20k_training_36", "title": "A fuzzy bi-criteria transportation problem", "abstract": "In this paper, a fuzzy bi-criteria transportation problem is studied. Here, the model concentrates on two criteria: total delivery time and total profit of transportation. The delivery times on links are fuzzy intervals with increasing linear membership functions, whereas the total delivery time on the network is a fuzzy interval with a decreasing linear membership function. On the other hand, the transporting profits on links are fuzzy intervals with decreasing linear membership functions and the total profit of transportation is a fuzzy number with an increasing linear membership function. Supplies and demands are deterministic numbers. A nonlinear programming model considers the problem using the max-min criterion suggested by Bellman and Zadeh. We show that the problem can be simplified into two bi-level programming problems, which are solved very conveniently. A proposed efficient algorithm based on parametric linear programming solves the bi-level problems. To explain the algorithm two illustrative examples are provided, systematically.  ", "keywords": ["fuzzy interval", "membership function", "bi-criteria transportation", "fuzzy transportation", "bi-level programming", "parametric programming"]}
{"id": "kp20k_training_37", "title": "Capacity Gain of Mixed Multicast/Unicast Transport Schemes in a TV Distribution Network", "abstract": "This paper presents three approaches to estimate the required resources in an infrastructure where digital TV channels can be delivered in unicast or multicast (broadcast) mode. Such situations arise for example in Cable TV, IPTV distribution networks or in (future) hybrid mobile TV networks. The three approaches presented are an exact calculation, a Gaussian approximation and a simulation tool. We investigate two scenarios that allow saving bandwidth resources. In a static scenario, the most popular channels are multicast and the less popular channels rely on unicast. In a dynamic scenario, the list of multicast channels is dynamic and governed by the users' behavior. We prove that the dynamic scenario always outperforms the static scenario. We demonstrate the robustness, versatility and the limits of our three approaches. The exact calculation application is limited because it is computationally expensive for cases with large numbers of users and channels, while the Gaussian approximation is good exactly for such systems. The simulation tool takes long to yield results for small blocking probabilities. We explore the capacity gain regions under varying model parameters. Finally, we illustrate our methods by discussing some realistic network scenarios using channel popularities based on measurement data as much as possible", "keywords": ["capacity planning", "digital tv/video", "iptv", "mobile tv", "multicast", "streaming", "switched broadcast", "unicast"]}
{"id": "kp20k_training_38", "title": "quantitatively evaluating the influence of online social interactions in the community-assisted digital library", "abstract": "Online social interactions are useful in information seeking from digital libraries, but how to measure their influence on the user's information access actions has not yet been revealed. Studies on this problem give us interesting insights into the workings of human dynamics in the context of information access from digital libraries. On the basis, we wish to improve the technological supports to provide more intelligent services in the ongoing China-America Million Books Digital Library so that it can reach its potential in serving human needs. Our research aims at developing a common framework to model online social interaction process in community-assisted digital libraries. The underlying philosophy of our work is that the online social interaction can be viewed as a dynamic process, and the next state of each participant in this process (e.g., personal information access competency) depends on the value of the previous states of all participants involving interactions in the period. Hence, considering the dynamics of interaction process, we model each participant with a Hidden Markov Model (HMM) chain and then employ the Influence Model , which was developed by C. Asavathiratham as a Dynamic Bayes Net (DBN) of representing the influences a number of Markov chains have on each other, to analyze the effects of participants influencing each other. Therefore, one can think of the entire interaction process as a DBN framework having two levels of structure: the local level and the network level. Each participant i has a local HMM chain &Ggr;( A ) which characterizes the transition of his internal states in the interaction process with state-transition probability &Sum;over j d ij P ( S i t | S j t-1 ) (Here states are his personal information access competence in different periods, while observations are his information access actions). Meanwhile, the network level, which is described by a network graph &Ggr;( D T ) where D ={ d ij } is the influence factor matrix , represents the interacting relations between participants. The strength of each connection, d ij , describes the influence factor of the participant j at its begin on the one i at its end. Hence, this model describes the dynamic inter-influence process of the internal states of all participants involving online interactions. To automatically build the model, we need firstly to extract observed features from the data of online social interactions and information access actions. Obviously, the effects of interactions are stronger if messages are exchanged more frequently, or the participants access more information in the online digital libraries during the period of time. Based on this consideration, we select the interaction measure IM i,j t and the amount of information IA j t as the estimation features of x i t . The interaction measure IA i t and the amount of information parameterize the features calculated automatically from the data of online social interactions between the participants i and j , and the features calculated from the data of information access actions respectively. Secondly, we need to develop a mechanism for learning the parameters d ij and P ( S i t | S j t-1 . Given sequences of observations { x i t } for each chain i , we may easily utilize the Expectation-Maximization algorithm or the gradient-based learning algorithm to get their estimation equations. We ran our experiments in the online digital library of W3C Consortium (www.w3c.org), which contains a mass of news, electronic papers or other materials related to web technologies. Users may access and download any information and materials in this digital library, and also may free discuss on any related technological problems by means of its mailing lists. Six users were selected in our experiments to collaboratively perform paper -gathering tasks related to four given topics. Any user might call for help from the others through the mailing lists when had difficulties in this process. All participants were required to record subjective evaluations of the effects that the others influenced his tasks. Each experiment was scheduled by ten phases. And in each phase, we sampled IM i,j t and IA i t for each participant and then fed them into the learning algorithms to automatically build the influence model. By comparing with the subjective influence graphs, the experimental results show that the influence model can estimate approximately the influences of online social interactions", "keywords": ["community-assisted digital library", "statistical feature extraction", "online social interactions", "the influence model"]}
{"id": "kp20k_training_39", "title": "THE CEO/CIO RELATIONSHIP REVISITED - AN EMPIRICAL-ASSESSMENT OF SATISFACTION WITH IS", "abstract": "The necessity of integrating information systems (IS) into corporate strategy has received widespread attention in recent years. Strategic planning has moved IS from serving primarily as a support function to a point where it may influence corporate strategy. The strength of this influence, however, usually is determined by the nature of the relationship between the chief information officer (CIO) and the CEO. Generally the more satisfied CEOs are with CIOs, the greater the influence IS has on top-level decisions. Results of a nationwide survey of motor carrier CEOs and CIOs indicate that CEOs are generally satisfied with their CIOs' activities, and that CIOs perceive CEOs as placing a high priority on strategic IS plans. However, IS does not appear to be truly a part of corporate strategy formulation", "keywords": ["relationship between ceo and cio", "satisfaction with information systems", "strategic planning", "role of information technology in strategic planning", "strategic information systems planning", "information technology in the motor carrier industry", "ceo satisfaction with is", "cio perceptions of corporate use of is"]}
{"id": "kp20k_training_40", "title": "Simulations of photosynthesis by a K-subset transforming system with membrane", "abstract": "By considering the inner regions of living cells' membranes, P systems with inner regions are introduced. Then, a new type of membrane computing systems are considered, called K-subset transforming systems with membranes, which can treat nonintegral multiplicities of objects. As an application, a K-subset transforming system is proposed in order to model the light reactions of the photosynthesis. The behaviour of such systems is simulated on a computer", "keywords": ["p system", "nonintegral multiplicity", "k-subset", "photosynthesis"]}
{"id": "kp20k_training_41", "title": "Technologische Innovation und die Auswirkung auf Geschftsmodell, Organisation und Unternehmenskultur  Die Transformation der IBM zum global integrierten, dienstleistungsorientierten Unternehmen", "abstract": "Im vorliegenden Beitrag wird der Einfluss von Innovationen der Informations- und Kommunikationstechnologie (IKT) auf die Transformation von Unternehmen untersucht. Zunchst werden die allgemeinen IKT-getriebenen Entwicklungslinien der Globalisierung und der Dienstleistungsorientierung beschrieben. Die nachfolgende Analyse der Transformation der IBM Corporation ber die letzten 50Jahre zu einem global integrierten, dienstleistungsorientierten Unternehmen macht deutlich, dass IKT-Innovationen mit gleichzeitigen Anpassungen des Geschftsmodells, der Organisation und der Unternehmenskultur begegnet werden muss. Die Fhigkeit zu derartiger Adaption gewinnt eine zunehmend zentrale Bedeutung fr Unternehmen", "keywords": ["innovation", "informations- und kommunikationstechnologie", "geschftsmodell", "organisation", "unternehmenskultur", "transformation", "vernderungsmanagement", "ibm", "innovation", "information and communication technology", "business model", "organization", "corporate culture", "transformation", "change management", "ibm"]}
{"id": "kp20k_training_42", "title": "Modular robotics as a tool for education and entertainment", "abstract": "We developed I-BLOCKS, a modular electronic building block system and here we show how this system has proven useful, especially as an educational tool that allows hands-on learning in an easy manner. Through user studies we find limitations of the first I-BLOCKS system, and we show how the system can be improved by introducing a graphical user interface for authoring the contents of the individual I-BLOCK. This is done by developing a new cubic block shape with new physical and electrical connectors, and by including new embedded electronics. We developed and evaluated the I-BLOCKS as a manipulative technology through studies in both schools and hospitals, and in diverse cultures such as in Denmark, Finland, Italy and Tanzania", "keywords": ["constructionism", "developing countries", "educational robots", "educational technology", "entertainment robots"]}
{"id": "kp20k_training_43", "title": "The selective use of redundancy for video streaming over Vehicular Ad Hoc Networks", "abstract": "Video streaming over Vehicular Ad Hoc Networks (VANETs) offers the opportunity to deploy many interesting services. These services, however, are strongly prone to packet loss due to the highly dynamic topology and shared wireless medium inherent in the VANETs. A possible strategy to enhance the delivery rate is to use redundancy for handling packet loss. This is a suitable technique for VANETs as it does not require any interaction between the source and receivers. In this work, we discuss novel approaches for the use of redundancy based on the particularities of video streaming over VANETs. A thorough study on the use of redundancy through Erasure Coding and Network Coding in both video unicast and video broadcast in VANETs is provided. We investigate each strategy, design novel solutions and compare their performance. We evaluated the proposed solutions from the perspective not only of cost as bandwidth utilization, but also the offered receiving rate of unique video content at the application layer. This perspective is fundamental to understanding how redundancy can be used without limiting the video quality that can be displayed to end users. Furthermore, we propose the selective use of redundancy solely on data that is more relevant to the video quality. This approach offers increases in overall video quality without leading to an excessive overhead nor to a substantial decrease in the receiving rate of unique video content", "keywords": ["video streaming", "vanets", "redundancy", "erasure coding", "network coding"]}
{"id": "kp20k_training_44", "title": "Syntactic recognition of ECG signals by attributed finite automata", "abstract": "A syntactic pattern recognition method of electrocardiograms (ECG) is described in which attributed automata are used to execute the analysis of ECG signals. An ECG signal is first encoded into a string of primitives and then attributed automata are used to analyse the string. We have found that we can perform fast and reliable analysis of ECG signals by attributed automata", "keywords": ["attributed automata", "electrocardiograms", "filtering", "medical computation", "nondeterministic top-down parsing", "primitive extraction", "signal analysis", "syntactic pattern recognition"]}
{"id": "kp20k_training_45", "title": "Lane-mark extraction for automobiles under complex conditions", "abstract": "We proposed a vision based lane-mark extraction method. We used multi-adaptive thresholds for different blocks. Based on the results, our method was robust for complex conditions. The proposed system could operate in real-time", "keywords": ["line fitting", "local edge-orientation", "kalman filter"]}
{"id": "kp20k_training_46", "title": "Cultural differences explaining the differences in results in GSS: implications for the next decade", "abstract": "For the next decade, the support that comes from Group Support Systems (GSS) will be increasingly directed towards culturally diversified groups. While there have been many GSS studies concerning culture and cultural differences, no dedicated review of GSS researches exists for the identification of current gaps and opportunities of doing cross-cultural GSS research. For this purpose, this paper provides a comprehensive review utilizing a taxonomy of six categories: research type, GSS technology used, independent variables, dependent variables, use of culture, and findings. Additionally, this study also aims to illustrate how differences in experimental results arising from comparable studies, but from a different cultural setting, can be explained consistently using Hofstede's dimensions. To do so, we presented a comparative study on the use of GSS in Australia and Singapore and explain the differences in results using Hofstede's [G. Hofstede, Culture's ConsequencesInternational Differences in Work-related Values, Sage, Beverly Hills, CA (1980).] cultural dimensions. Last, but not least, we present the implications of the impact of culture on GSS research for the next decade from the viewpoint of the three GSS stakeholders: the facilitators, GSS software designers, and the GSS researchers. With the above, this paper seeks (i) to prepare a comprehensive map of GSS research involving culture, and (ii) to prepare a picture of what all these mean and where we should be heading in the next decade", "keywords": ["cultural differences", "gss", "implications"]}
{"id": "kp20k_training_47", "title": "Building an IP-based community wireless mesh network: Assessment of PACMAN as an IP address autoconfiguration protocol", "abstract": "Wireless mesh networks are experiencing rapid progress and inspiring numerous applications in different scenarios. due to features such as autoconfiguration, self-healing, connectivity coverage extension and support for dynamic topologies These particular characteristics make wireless mesh networks an appropriate architectural basis for the design of easy-to-deploy community or neighbourhood networks One of the main challenges in building a community network using mesh networks is the minimisation of user intervention in the IP address configuration of the network nodes In this paper we first consider the process of building an IP-based mesh network using typical residential routers, exploring the options for the configuration of their wireless interfaces. Then we focus on IP address autoconfiguration, identifying the specific requirements for community mesh networks and analysing the applicability of existing solutions. As a result of that analysis, we select PACMAN, an efficient distributed address autoconfiguration mechanism originally designed for ad-hoc networks. and we perform an experimental study - using off-the-shelf routers and assuming worst-case scenarios - analysing its behaviour as an IP address autoconfiguration mechanism for community wireless mesh networks The results of the conducted assessment show that PACMAN meets all the identified requirements of the community scenario", "keywords": ["community networks", "wireless mesh networks", "experimental evaluation", "pacman"]}
{"id": "kp20k_training_48", "title": "Approximating partition functions of the two-state spin system", "abstract": "Two-state spin system is a classical topic in statistical physics. We consider the problem of computing the partition function of the system on a bounded degree graph. Based on the self-avoiding tree, we prove the system exhibits strong correlation decay under the condition that the absolute value of inverse temperature is small. Due to strong correlation decay property, an FPTAS for the partition function is presented and uniqueness of Gibbs measure of the two-state spin system on a bounded degree infinite graph is proved, under the same condition. This condition is sharp for Ising model.  ", "keywords": ["approximation algorithms", "two-state spin system", "ising model", "gibbs measure", "uniqueness of gibbs measure", "fptas", "strong correlation decay"]}
{"id": "kp20k_training_49", "title": "Efficient memory utilization for high-speed FPGA-based hardware emulators with SDRAMs", "abstract": "FPGA-based hardware. emulators are often used for the verification of LSI functions. They generally have dedicated external memories, such as SDRAMs, to compensate for the lack of memory capacity in FPGAs. In such a case, access between the FPGAs and the dedicated external memory may represent a major bottleneck with respect to emulation speed since the dedicated external memory may have to emulate a large number of memory blocks. In this paper, we propose three methods, \"Dynamic Clock Control (DCC),\" \"Memory Mapping Optimization (MMO),\" and \"Efficient Access Scheduling (EAS),\" to avoid this bottleneck. DCC controls an emulation clock dynamically in accord with the number of memory accesses within one emulation clock cycle. EAS optimizes the ordering of memory access to the dedicated external memory, and MMO optimizes the arrangement of the dedicated external memory addresses to which respective memories will be emulated. With them, emulation speed can be made 29.0 times faster, as evaluated in actual LSI emulations", "keywords": ["fpga-based hardware emulators", "sdram", "memory controller clock generator"]}
{"id": "kp20k_training_50", "title": "Minimum stress optimal design with the level set method", "abstract": "This paper is devoted to minimum stress design in structural optimization. We propose a simple and efficient numerical algorithm for shape and topology optimization based on the level set method coupled with the topological derivative. We compute a shape derivative, as well as a topological derivative, for a stress-based objective function. Using an adjoint equation we implement a gradient algorithm for the minimization of the objective function. Several numerical examples in 2-d and 3-d are discussed", "keywords": ["level set method", "shape derivative", "topological derivative"]}
{"id": "kp20k_training_51", "title": "Determination of wire recovery length in steel cables and its practical applications", "abstract": "In the presence of relatively significant states of radial pressures between the helical wires of a steel cable (spiral strand and/or wire rope), and significant levels of interwire friction, the individual broken wires tend to take up their appropriate share of the axial load within a certain length from the fractured end, which is called the recovery (or development) length. The paper presents full details of the formulations for determining the magnitude of recovery length in any layer of an axially loaded multi-layered spiral strand with any construction details. The formulations are developed for cases of fully bedded-in (old) spiral strands within which the pattern of interlayer contact forces and associated significant values of line-contact normal forces between adjacent wires in any layer, are fully stabilised, and also for cases when (in the presence of gaps between adjacent wires) hoop line-contact forces do not exist and only radial forces are present. Based on a previously reported extensive series of theoretical parametric studies using a wide range of spiral strand constructions with widely different wire (and cable) diameters and lay angles, a very simple method (aimed at practising engineers) for determining the magnitude of recovery length in any layer of an axially loaded spiral strand with any type of construction details is prestented. Using the final outcome of theoretical parametric studies, the minimum length of test specimens for axial fatigue tests whose test data may safely be used for estimating the axial fatigue lives of the much longer cables under service conditions may now be determined in a straightforward fashion. Moreover, the control length over which one should count the number of broken wires for cable discard purposes is suggested to be equal to one recovery length whose upper bound value for both spiral strands and/or wire ropes with any construction details is theoretically shown to be equal to 2.5 lay lengths", "keywords": ["wire recovery length", "steel cables", "multi-layered spiral strand", "radial forces"]}
{"id": "kp20k_training_52", "title": "Generalized PCM Coding of Images", "abstract": "Pulse-code modulation (PCM) with embedded quantization allows the rate of the PCM bitstream to be reduced by simply removing a fixed number of least significant bits from each codeword. Although this source coding technique is extremely simple, it has poor coding efficiency. In this paper, we present a generalized PCM (GPCM) algorithm for images that simply removes bits from each codeword. In contrast to PCM, however, the number and the specific bits that a GPCM encoder removes in each codeword depends on its position in the bitstream and the statistics of the image. Since GPCM allows the encoding to be performed with different degrees of computational complexity, it can adapt to the computational resources that are available in each application. Experimental results show that GPCM outperforms PCM with a gain that depends on the rate, the computational complexity of the encoding, and the degree of inter-pixel correlation of the image", "keywords": ["binning", "interpolative coding", "pulse-code modulation", "quantization"]}
{"id": "kp20k_training_53", "title": "Influence of motor and converter non-linearities on dynamic properties of DC drive with field weakening range", "abstract": "Improvement of the dynamic properties of DC drive in the field weakening range was the aim of investigation. The non-linear model of the drive system was applied. In the paper results of the comparative analysis of two emf control structures are presented. The classic emf control structure with subordinated excitation current control loop was compared with this one consisting of a non-linear compensation block. For both control structures different kinds of the parameter designing for the emf and excitation controllers are considered. Verification of the theoretical assumptions and synthesis methods of the investigated control structures are made by simulation tests using the PSpice language", "keywords": ["dc drive", "dynamic properties", "field control", "field weakening", "non-linear control"]}
{"id": "kp20k_training_54", "title": "Rental software valuation in IT investment decisions", "abstract": "The growth of application service providers (ASPs) is very rapid, leading to a number of options to organizations interested in developing new information technology services. The advantages of an ASP include spreading out payments over a contract period and flexibility in terms of responding to changes in technology. Likewise, newer risks are associated with ASPs, including pricing variability. Some of the more common capital budgeting models may not be appropriate in this volatile marketplace. However, option models allow for many of the quirks to be considered. Modification of the option pricing model and an analytical solution method incorporated into a spreadsheet for decision support are described and illustrated. The analytical tool allows for better decisions compared to traditional value analysis methods which do not fully account for the entry and exit options of the market", "keywords": ["options", "capital budgeting", "information technology investment", "application service providers", "stochastic processes", "risk analysis"]}
{"id": "kp20k_training_55", "title": "An averaging scheme for macroscopic numerical simulation of nonconvex minimization problems", "abstract": "Averaging or gradient recovery techniques, which are a popular tool for improved convergence or superconvergence of finite element methods in elliptic partial differential equations, have not been recommended for nonconvex minimization problems as the energy minimization process enforces finer and finer oscillations and hence at the first glance, a smoothing step appears even counterproductive. For macroscopic quantities such as the stress field, however, this counterargument is no longer true. In fact, this paper advertises an averaging technique for a surprisingly improved convergence behavior for nonconvex minimization problems. Similar to a finite volume scheme, numerical experiments on a double-well benchmark example provide empirical evidence of superconvergence phenomena in macroscopic numerical simulations of oscillating microstructures", "keywords": ["averaging scheme", "nonconvex minimization", "convexification", "macroscopic numerical simulation", "adaptive mesh refinement"]}
{"id": "kp20k_training_56", "title": "Which App? A recommender system of applications in markets: Implementation of the service for monitoring users' interaction", "abstract": "Users face the information overload problem when downloading applications in markets. This is mainly due to (i) the increasing unmanageable number of applications and (ii) the lack of an accurate and fine-grained categorization of the applications in the markets. To address this issue, we present an integrated solution which recommends to the users applications by considering a big amount of information: that is, according to their previously consumed applications, use pattern, tags used to annotate resources and history of ratings. We focus this paper on the service for monitoring users' interaction.  ", "keywords": ["recommender system", "context-awareness", "mobile applications", "filtering"]}
{"id": "kp20k_training_57", "title": "New statistical features for the design of fiber optic statistical mode sensors", "abstract": "Novel statistical features are proposed for the design of statistical mode sensors. Proposed statistical features are first and second order moments. Features are compared in terms of precision error, non-linearity, and hysteresis", "keywords": ["fiber optic sensor", "statistical mode sensor", "image processing", "statistical features"]}
{"id": "kp20k_training_58", "title": "An efficient indexing method for content-based image retrieval", "abstract": "In this paper, we propose an efficient indexing method for content-based image retrieval. The proposed method introduces the ordered quantization to increase the distinction among the quantized feature descriptors. Thus, the feature point correspondences can be determined by the quantized feature descriptors, and they are used to measure the similarity between query image and database image. To implement the above scheme efficiently, a multi-dimensional inverted index is proposed to compute the number of feature point correspondences, and then approximate RANSAC is investigated to estimate the spatial correspondences of feature points between query image and candidate images returned from the multi-dimensional inverted index. The experimental results demonstrate that our indexing method improves the retrieval efficiency while ensuring the retrieval accuracy in the content-based image retrieval", "keywords": ["content-based image retrieval", "feature point correspondences", "ordered quantization", "multi-dimensional inverted index", "approximate ransac"]}
{"id": "kp20k_training_59", "title": "Prediction intervals in linear regression taking into account errors on both axes", "abstract": "This study reports the expressions for the variances in the prediction of the response and predictor variables calculated with the bivariate least squares (BLS) regression technique. This technique takes into account the errors on both axes. Our results are compared with those of a simulation process based on six different real data sets. The mean error in the results from the new expressions is between 4% and 5%. With weighted least squares, ordinary least squares, the constant variance ratio approach and orthogonal regression, on the other hand, mean errors can be as high as 85%, 277%, 637% and 1697% respectively. An important property of the prediction intervals calculated with BLS is that the results are not affected when the axes are switched. ", "keywords": ["prediction", "linear regression", "errors on both axes", "confidence intervals", "predictor intervals"]}
{"id": "kp20k_training_60", "title": "The PIAM approach to modular integrated assessment modelling", "abstract": "The next generation of integrated assessment modelling is envisaged as being organised as a modular process, in which modules encapsulating knowledge from different scientific disciplines are independently developed at distributed institutions and coupled afterwards in accordance with the question raised by the decision maker. Such a modular approach needs to respect several stages of the model development process, approaching modularisation and integration on a conceptual, numerical, and technical level. The paper discusses the challenges at each level and presents partial solutions developed by the PIAM (Potsdam Integrated Assessment Modules) project at the Potsdam Institute for Climate Impact Research (PIK). The challenges at each level differ greatly in character and in the work done addressing them. At the conceptual level, the notion of conceptual consistency of modular integrated models is discussed. At the numerical level, it is shown how an adequate modularisation of a problem from climateeconomy leads to a modular configuration into which independently developed climate and economic modules can be plugged. At the technical level, a software tool is presented which provides a simple consistent interface for data transfer between modules running on distributed and heterogeneous computer platforms", "keywords": ["modularity", "modular modelling", "model integration", "integrated modelling", "integrated assessment modelling", "climate change"]}
{"id": "kp20k_training_61", "title": "Finding multivariate outliers in fMRI time-series data", "abstract": "Multivariate outlier detection methods are applicable to fMRI time-series data. Removing outliers increases spatial specificity without hurting classification. Simulation shows PCOut is more sensitivity to small outliers than HD BACON", "keywords": ["outlier detection", "fmri", "high dimensional data"]}
{"id": "kp20k_training_62", "title": "Cardinal Consistency of Reciprocal Preference Relations: A Characterization of Multiplicative Transitivity", "abstract": "Consistency of preferences is related to rationality, which is associated with the transitivity property. Many properties suggested to model transitivity of preferences are inappropriate for reciprocal preference relations. In this paper, a functional equation is put forward to model the \"cardinal consistency in the strength of preferences\" of reciprocal preference relations. We show that under the assumptions of continuity and monotonicity properties, the set of representable uninorm operators is characterized as the solution to this functional equation. Cardinal consistency with the conjunctive representable cross ratio uninorm is equivalent to Tanino's multiplicative transitivity property. Because any two representable uninorms are order isomorphic, we conclude that multiplicative transitivity is the most appropriate property for modeling cardinal consistency of reciprocal preference relations. Results toward the characterization of this uninorm consistency property based on a restricted set of (n - 1) preference values, which can be used in practical cases to construct perfect consistent preference relations, are also presented", "keywords": ["consistency", "fuzzy preference relation", "rationality", "reciprocity", "transitivity", "uninorm"]}
{"id": "kp20k_training_63", "title": "Infomarker - A new Internet information service system", "abstract": "As the web grows, the massive increase in information is placing severe burdens on information retrieval and sharing. Automated search engines and directories with small editorial staff are unable to keep up with the increasing submission of web sites. To address the problem, this paper presents Infomarker - an Internet information service system based on Open Directory and Zero-Keyword Inquiry. The Open Directory sets up a net-community in which the increasing net-citizens can each organize a small portion of the web and present it to the others. By means of Zero-Keyword Inquiry, user can get the information he is interested in without inputting any keyword that is often required by search engines. In Infomarker, user can record the web address he likes and can put forward an information request based on his web records. The information matching engine checks the information in the Open Directory to find what fits user's needs and adds it to user's web address records. The key to the matching process is layered keyword mapping. Infomarker provides people with a whole new approach to getting information and shows a wide prospect", "keywords": ["open directory", "zero-keyword inquiry", "information matching engine", "layered keyword mapping"]}
{"id": "kp20k_training_64", "title": "Grain flow measurements with X-ray techniques", "abstract": "The use of low energy X-rays, up to 30 keV, densitometry is demonstrated for grain flow rate measurements through laboratory experiments. Mass flow rates for corn were related to measured X-ray intensity in gray scale units with a 0.99 correlation coefficient for flow rates ranging from 2 to 6 kg/s. Larger flow rate values can be measured by using higher energy or a higher tube current. Measurements were done in real time at a 30 Hz sampling rate. Flow rate measurements are relatively independent of grain moisture due to a negligible change in the X-ray attenuation coefficients at typical moisture content values from 15 to 25%. Grain flow profile changes did not affect measurement accuracy. X-rays easily capture variations in the corn thickness profile. Due to the low energy of the X-ray photons, biological shielding can be accomplished with 2-mm-thick lead foil or 5 mm of steel", "keywords": ["precision farming", "x-ray", "yield monitoring", "yield sensor"]}
{"id": "kp20k_training_65", "title": "Dynamic performance enhancement of microgrids by advanced sliding mode controller", "abstract": "Dynamics are the most important problems in the microgrid operation. In the islanded microgrid, the mismatch of parallel operations of inverters during dynamics can result in the instability. This paper considers severe dynamics which can occur in the microgrid. Microgrid can have different configurations with different load and generation dynamics which are facing voltage disturbances. As a result, microgrid has many uncertainties and is placed in the distribution network where is full of voltage disturbances. Moreover, characteristics of the distribution network and distributed energy resources in the islanded mode make microgrid vulnerable and easily lead to instability. The main aim of this paper is to discuss the suitable mathematical modeling based on microgrid characteristics and to design properly inner controllers to enhance the dynamics of microgrid with uncertain and changing parameters. This paper provides a method for inner controllers of inverter-based distributed energy resources to have a suitable response for different dynamics. Parallel inverters in distribution networks were considered to be controlled by nonlinear robust voltage and current controllers. Theoretical prove beyond simulation results, reveal evidently the effectiveness of the proposed controller", "keywords": ["current controlled-voltage source inverter", "dynamic stability", "microgrid", "sliding mode control", "transients", "disturbances"]}
{"id": "kp20k_training_66", "title": "NEW IDENTIFICATION PROCEDURE FOR CONTINUOUS-TIME RADIO FREQUENCY POWER AMPLIFIER MODEL", "abstract": "In this paper, we present a new method for characterization of radio frequency Power Amplifier (PA) in the presence of nonlinear distortions which affect the modulated signal in Radiocommunication transmission system. The proposed procedure uses a gray box model where PA dynamics are modeled with a MIMO continuous filter and the nonlinear characteristics are described as general polynomial functions, approximated by means of Taylor series. Using the baseband input and output data, model parameters are obtained by an iterative identification algorithm based on Output Error method. Initialization and excitation problems are resolved by an association of a new technique using initial values extraction with a multi-level binary sequence input exciting all PA dynamics. Finally, the proposed estimation method is tested and validated on experimental data", "keywords": ["rf power amplifier", "parameter estimation", "nonlinear distortions", "modeling", "continuous time domain"]}
{"id": "kp20k_training_67", "title": "Manufacturing lead-time rules: Customer retention versus tardiness costs", "abstract": "Inaccurate production backlog information is a major cause of late deliveries, which can result in penalty fees and loss of reputation. We identify conditions when it is particularly worthwhile to improve an information system to provide good lead-time information. We first analyze a sequential decision process model of lead-time decisions at a firm which manufactures standard products to order, and has complete backlog information. There are Poisson arrivals, stochastic processing times, customers may balk in response to quoted delivery dates, and revenues are offset by tardiness penalties. We characterize an optimal policy and show how to accelerate computations. The second part of the paper is a computational comparison of this optimum (with full backlog information) with a lead-time quotation rule that is optimal with statistical shop-status information. This reveals when the partial-information method does well and when it is worth implementing measures to improve information transfer between operations and sales", "keywords": ["dynamic programming", "manufacturing", "markov decision processes", "due-date assignment", "marketing"]}
{"id": "kp20k_training_68", "title": "On the Wiberg algorithm for matrix factorization in the presence of missing components", "abstract": "This paper considers the problem of factorizing a matrix with missing components into a product of two smaller matrices, also known as principal component analysis with missing data (PCAMD). The Wiberg algorithm is a numerical algorithm developed for the problem in the community of applied mathematics. We argue that the algorithm has not been correctly understood in the computer vision community. Although there are many studies in our community, almost every one of which refers to the Wiberg study, as far as we know, there is no literature in which the performance of the Wiberg algorithm is investigated or the detail of the algorithm is presented. In this paper, we present derivation of the algorithm along with a problem in its implementation that needs to be carefully considered, and then examine its performance. The experimental results demonstrate that the Wiberg algorithm shows a considerably good performance, which should contradict the conventional view in our community, namely that minimization-based algorithms tend to fail to converge to a global minimum relatively frequently. The performance of the Wiberg algorithm is such that even starting with random initial values, it converges in most cases to a correct solution, even when the matrix has many missing components and the data are contaminated with very strong noise. Our conclusion is that the Wiberg algorithm can also be used as a standard algorithm for the problems of computer vision", "keywords": ["matrix factorization", "singular value decomposition", "principal component analysis with missing data ", "structure from motion", "numerical algorithm"]}
{"id": "kp20k_training_69", "title": "Semi-supervised local Fisher discriminant analysis for dimensionality reduction", "abstract": "When only a small number of labeled samples are available, supervised dimensionality reduction methods tend to perform poorly because of overfitting. In such cases, unlabeled samples could be useful in improving the performance. In this paper, we propose a semi-supervised dimensionality reduction method which preserves the global structure of unlabeled samples in addition to separating labeled samples in different classes from each other. The proposed method, which we call SEmi-supervised Local Fisher discriminant analysis (SELF), has an analytic form of the globally optimal solution and it can be computed based on eigen-decomposition. We show the usefulness of SELF through experiments with benchmark and real-world document classification datasets", "keywords": ["semi-supervised learning", "dimensionality reduction", "cluster assumption", "local fisher discriminant analysis", "principal component analysis"]}
{"id": "kp20k_training_70", "title": "A stable fluidstructure-interaction solver for low-density rigid bodies using the immersed boundary projection method", "abstract": "Dispersion of low-density rigid particles with complex geometries is ubiquitous in both natural and industrial environments. We show that while explicit methods for coupling the incompressible NavierStokes equations and Newton's equations of motion are often sufficient to solve for the motion of cylindrical particles with low density ratios, for more complex particles  such as a body with a protrusion  they become unstable. We present an implicit formulation of the coupling between rigid body dynamics and fluid dynamics within the framework of the immersed boundary projection method. Similarly to previous work on this method, the resulting matrix equation in the present approach is solved using a block-LU decomposition. Each step of the block-LU decomposition is modified to incorporate the rigid body dynamics. We show that our method achieves second-order accuracy in space and first-order in time (third-order for practical settings), only with a small additional computational cost to the original method. Our implicit coupling yields stable solution for density ratios as low as 10?4. We also consider the influence of fictitious fluid located inside the rigid bodies on the accuracy and stability of our method", "keywords": ["immersed boundary method", "fictitious fluid", "newton's equations of motion", "implicit coupling", "low density ratios", "complex particles"]}
{"id": "kp20k_training_71", "title": "Latent word context model for information retrieval", "abstract": "The application of word sense disambiguation (WSD) techniques to information retrieval (IR) has yet to provide convincing retrieval results. Major obstacles to effective WSD in IR include coverage and granularity problems of word sense inventories, sparsity of document context, and limited information provided by short queries. In this paper, to alleviate these issues, we propose the construction of latent context models for terms using latent Dirichletallocation. We propose building one latent context per word, using a well principled representation of local context based on word features. In particular, context words are weighted using a decaying function according to their distance to the target word, which is learnt from data in an unsupervised manner. The resulting latent features are used to discriminate word contexts, so as to constrict querys semantic scope. Consistent and substantial improvements, including on difficult queries, are observed on TREC test collections, and the techniques combines well with blind relevance feedback. Compared to traditional topic modeling, WSD and positional indexing techniques, the proposed retrieval model is more effective and scales well on large-scale collections", "keywords": ["retrieval models", "word context discrimination ", "word context", "topic models", "word sense disambiguation "]}
{"id": "kp20k_training_72", "title": "Clusterization, frustration and collectivity in random networks", "abstract": "We consider the random Erdos-Renyi network with enhanced clusterization and Ising spins s = +/- 1 at the network nodes. Mutually linked spins interact with energy J. Magnetic properties of the system that are dependent on the clustering coefficient C are investigated with the Monte Carlo heat bath algorithm. For J > 0 the Curie temperature T(c) increases from 3.9 to 5.5 when C increases from almost zero to 0.18. These results deviate only slightly from the mean field theory. For J < 0 the spin-glass phase appears below T(SG); this temperature decreases with C, on the contrary to the mean field calculations. The results are interpreted in terms of social systems", "keywords": ["random networks", "phase transitions"]}
{"id": "kp20k_training_73", "title": "GPS/INS integration utilizing dynamic neural networks for vehicular navigation", "abstract": "Recently, methods based on Artificial Intelligence (AI) have been suggested to provide reliable positioning information for different land vehicle navigation applications integrating the Global Positioning System (GPS) with the Inertial Navigation System (INS). All existing AI-based methods are based on relating the INS error to the corresponding INS output at certain time instants and do not consider the dependence of the error on the past values of INS. This study, therefore, suggests the use of Input-Delayed Neural Networks (IDNN) to model both the INS position and velocity errors based on current and some past samples of INS position and velocity, respectively. This results in a more reliable positioning solution during long GPS outages. The proposed method is evaluated using road test data of different trajectories while both navigational and tactical grade INS are mounted inside land vehicles and integrated with GPS receivers. The performance of the IDNN - based model is also compared to both conventional (based mainly on Kalman filtering) and recently published Al - based techniques. The results showed significant improvement in positioning accuracy especially for cases of tactical grade INS and long GPS outages.  ", "keywords": ["gps", "inertial navigation system ", "data fusion", "dynamic neural network", "ins/gps road tests"]}
{"id": "kp20k_training_74", "title": "A unified probabilistic framework for automatic 3D facial expression analysis based on a Bayesian belief inference and statistical feature models", "abstract": "Textured 3D face models capture precise facial surfaces along with the associated textures, making it possible for an accurate description of facial activities. In this paper, we present a unified probabilistic framework based on a novel Bayesian Belief Network (BBN) for 3D facial expression and Action Unit (AU) recognition. The proposed BBN performs Bayesian inference based on Statistical Feature Models (SFM) and Gibbs-Boltzmann distribution and feature a hybrid approach in fusing both geometric and appearance features along with morphological ones. When combined with our previously developed morphable partial face model (SFAM), the proposed BBN has the capacity of conducting fully automatic facial expression analysis. We conducted extensive experiments on the two public databases, namely the BU-3DFE dataset and the Bosphorus dataset. When using manually labeled landmarks, the proposed framework achieved an average recognition rate of 94.2% and 85.6% for the 7 and 16 AU on face data from the Bosphorus dataset respectively, and 89.2% for the six universal expressions on the BU-3DFE dataset. Using the landmarks automatically located by SFAM, the proposed BBN still achieved an average recognition rate of 84.9% for the six prototypical facial expressions. These experimental results demonstrate the effectiveness of the proposed approach and its robustness in landmark localization errors. Published by Elsevier B.V", "keywords": ["bayesian belief network", "statistical feature model", "3d face", "facial expression recognition", "action units recognition", "automatic landmarking"]}
{"id": "kp20k_training_75", "title": "MPML3D: Scripting Agents for the 3D Internet", "abstract": "The aim of this paper is two-fold. First, it describes a scripting language for specifying communicative behavior and interaction of computer-controlled agents (\"bots\") in the popular three-dimensional (3D) multiuser online world of \"Second Life\" and the emerging \"OpenSimulator\" project. While tools for designing avatars and in-world objects in Second Life exist, technology for nonprogrammer content creators of scenarios involving scripted agents is currently missing. Therefore, we have implemented new client software that controls bots based on the Multimodal Presentation Markup Language 3D (MPML3D), a highly expressive XML-based scripting language for controlling the verbal and nonverbal behavior of interacting animated agents. Second, the paper compares Second Life and OpenSimulator platforms and discusses the merits and limitations of each from the perspective of agent control. Here, we also conducted a small study that compares the network performance of both platforms", "keywords": ["artificial, augmented, and virtual realities", "graphical user interfaces", "synchronous interaction", "visualization", "markup languages", "scripting languages"]}
{"id": "kp20k_training_76", "title": "tlb and snoop energy-reduction using virtual caches in low-power chip-multiprocessors", "abstract": "In our quest to bring down the power consumption in low-power chip-multiprocessors, we have found that TLB and snoop accesses account for about 40% of the energy wasted by all L1 data-cache accesses. We have investigated the prospects of using virtual caches to bring down the number of TLB accesses. A key observation is that while the energy wasted in the TLBs are cut, the energy associated with snoop accesses becomes higher. We then contribute with two techniques to reduce the number of snoop accesses and their energy cost. Virtual caches together with the proposed techniques are shown to reduce the energy wasted in the L1 caches and the TLBs by about 30", "keywords": ["chip multiprocessors", "virtual caches", "snoop", "power consumption", "data cache", "association", "energy", "reduction", "cmp", "account", "virtualization", "caches", "cost", "low-power"]}
{"id": "kp20k_training_77", "title": "extracting tennis statistics from wireless sensing environments", "abstract": "Creating statistics from sporting events is now widespread with most efforts to automate this process using various sensor devices. The problem with many of these statistical applications is that they require proprietary applications to process the sensed data and there is rarely an option to express a wide range of query types. Instead, applications tend to contain built-in queries with predefined outputs. In the research presented in this paper, data from a wireless network is converted to a structured and highly interoperable format to facilitate user queries by expressing high level queries in a standard database language and automatically generating the results required by coaches", "keywords": ["sensor", "ubisense", "query", "xml"]}
{"id": "kp20k_training_78", "title": "Critical success factors of inter-organizational information systems- A case study of Cisco and Xiao Tong in China", "abstract": "This paper reports a case study of an inter-organizational information system (IOS) of Cisco and Xiao Tong in China. We interviewed their senior managers, heads of departments and employees who have been directly affected in their work. Other sources of information are company documents and publicly available background information. The study examines the benefits of the IOS for both corporations. The research also reveals seven critical success factors for the IOS, namely intensive stimulation, shared vision, cross-organizational implementation team, high integration with internal information systems, inter-organizational business process re-engineering, advanced legacy information system and infrastructure and shared industry standard.  ", "keywords": ["inter-organizational information systems", "critical success factors", "china"]}
{"id": "kp20k_training_79", "title": "Maize grain shape approaches for DEM modelling", "abstract": "The shape of a grain of maize was approached using the multi-sphere method. Models with single-spherical particles and with rolling friction were also used. Results from two DEM software codes were compared. Recommendations on the shape approach for DEM modelling were provided", "keywords": ["dem", "maize", "particle shape", "multi-spheres", "rolling friction", "flow"]}
{"id": "kp20k_training_80", "title": "multi-sector antenna performance in dense wireless networks", "abstract": "Sectorized antennas provide an attractive solution to increase wireless network capacity through higher spatial reuse. Despite their increasing popularity, the real-world performance characteristics of such antennas in dense wireless mesh networks are not well understood. In this demo, we demonstrate our multi-sector antenna prototypes and their performance through video streaming over an indoor wireless network in the presence of interfering nodes. We use our graphical tool to vary the sender, receiver, and interferer antenna configurations and the resulting performance is directly visible in the video quality displayed at the receiver", "keywords": ["sectorized antenna", "sector selection", "dense wireless mesh networks", "directional hidden terminal problem"]}
{"id": "kp20k_training_81", "title": "Improvement of 3P and 6R mechanical robots reliability and quality applying FMEA and QFD approaches", "abstract": "In the past few years, extending usage of robotic systems has increased the importance of robot reliability and quality. To improve the robot reliability and quality by applying standard approaches such as Failure Mode and Effect Analysis (FMEA) and Quality Function Deployment (QFD) during the design of robot is necessary. FMEA is a qualitative method which determines the critical failure modes in robot design. In this method Risk Priority Number is used to sort failures with respect to critical situation. Two examples of mechanical robots are analyzed by using this method and critical failure modes are determined for each robot. Corrective actions are proposed for critical items to modify robots reliability and reduce their risks. Finally by using QFD, quality of these robots is improved according to the customers requirements. In this method by making four matrixes, optimum values for all technical parameters are determined and the final product has the desired quality", "keywords": ["robot", "fmea", "qfd", "reliability", "quality", "performance"]}
{"id": "kp20k_training_82", "title": "Informatics methodologies for evaluation research in the practice setting", "abstract": "A continuing challenge in health informatics and health evaluation is to enable access to the practice of health care so that the determinants of successful care and good health outcomes can be measured, evaluated and analysed. Furthermore the results of the analysis should be available to the health care practitioner or to the patient as might be appropriate, so that he or she can use this information for continual improvement of practice and optimisation of outcomes. In this paper we review two experiences, one in primary care, the FAMUS project, and the other in hospital care, the Autocontrol project. Each project demonstrates an informatics approach for evaluation research in the clinical setting and indicates ways in which useful information can be obtained which with appropriate feed-back and education can be used towards the achievement of better health. Emphasis is given to data collection methods compatible with practice and to high quality information feedback, particularly in the team context, to enable the formulation of strategies for practice improvement", "keywords": ["data collection", "evaluation research", "health informatics", "clinical strategies"]}
{"id": "kp20k_training_83", "title": "An improved SOM algorithm and its application to color feature extraction", "abstract": "Reducing the redundancy of dominant color features in an image and meanwhile preserving the diversity and quality of extracted colors is of importance in many applications such as image analysis and compression. This paper presents an improved self-organization map (SOM) algorithm namely MFD-SOM and its application to color feature extraction from images. Different from the winner-take-all competitive principle held by conventional SOM algorithms, MFD-SOM prevents, to a certain degree, features of non-principal components in the training data from being weakened or lost in the learning process, which is conductive to preserving the diversity of extracted features. Besides, MFD-SOM adopts a new way to update weight vectors of neurons, which helps to reduce the redundancy in features extracted from the principal components. In addition, we apply a linear neighborhood function in the proposed algorithm aiming to improve its performance on color feature extraction. Experimental results of feature extraction on artificial datasets and benchmark image datasets demonstrate the characteristics of the MFD-SOM algorithm", "keywords": ["self-organizing map", "color feature extraction", "non-principal component", "competitive mechanism"]}
{"id": "kp20k_training_84", "title": "A Motion Planning System for Mobile Robots", "abstract": "In this paper, a motion planning system for a mobile robot is proposed. Path planning tries to find a feasible path for mobile robots to move from a starting node to a target node in an environment with obstacles. A genetic algorithm is used to generate an optimal path by taking the advantage of its strong optimization ability. Mobile robot, obstacle and target localizations are realized by means of camera and image processing. A graphical user interface (GUI) is designed for the motion planning system that allows the user to interact with the robot system and to observe the robot environment. All the software components of the system are written in MATLAB that provides to use non-predefined accessories rather than the robot firmware has, to avoid confusing in C++ libraries of robot's proprietary software, to control the robot in detail and not to re-compile the programs frequently in real-time dynamic operations", "keywords": ["genetic algorithm", "mobile robot", "motion planning"]}
{"id": "kp20k_training_85", "title": "A unified strategy for search and result representation for an online bibliographical catalogue", "abstract": "Purpose - One of the biggest concerns of modem information retrieval systems is reducing the user effort required for manual traversal and filtering of long matching document lists. Thus, the first goal of this research is to propose an improved scheme for representation of search results. Further, it aims to explore the impact of various user information needs on the searching process with the aim of finding a unified searching approach well suited for different query types and retrieval tasks. Design/methodology/approach - The BoW online bibliographical catalogue is based on a hierarchical concept index to which entries are linked. The key idea is that searching in the hierarchical catalogue should take advantage of the catalogue structure and return matching topics from the hierarchy, rather than just a long list of entries. Likewise, when new entries are inserted, a search for relevant topics to which they should be linked is required. Therefore, a similar hierarchical scheme for query-topic matching can be applied for both tasks. Findings - The experiments show that different query types used for the above tasks are best treated by different topic ranking functions. To further examine this phenomenon a user study was conducted, where various statistical weighting factors were incorporated and their impact on the performance for different query types was measured. Finally, it is found that the mixed strategy that applies the most suitable ranking function to each query type yielded a significant increase in precision relative to the baseline and to employing any examined strategy in isolation on the entire set of user queries. Originality/value - The main contributions of this paper are: the alternative approach for compact and concise representation of search results, which were implemented in the BoW online bibliographical catalogue; and the unified or mixed strategy for search and result representation applying the most suitable ranking function to each query type, which produced superior results compared to different single-strategy-based approaches", "keywords": ["online catalogues", "information retrieval", "technology led strategy"]}
{"id": "kp20k_training_86", "title": "robust multiple-phase switched-capacitor dc-dc converter with digital interleaving regulation scheme", "abstract": "An integrated switched-capacitor (SC) DC-DC converter with a digital interleaving regulation scheme is presented. By interleaving the newly-structured charge pump (CP) cells in multiple phases, the input current ripple and output voltage ripple are reduced significantly. The converter exhibits excellent robustness, even when one of the CP cells fails to operate. A fully digital controller is employed with a hysteretic control algorithm. It features dead-beat system stability and fast transient response. Hspice post-layout simulation shows that, with a 1.5 V input power supply, the SC converter accurately provides an adjustable regulated power output in a range of 1.6 to 2.7 V. The maximum output ripple is 40 mV when a full load of 0.54 W is supplied. Transient response of 1.8 ms is observed when the load current switches from half- to full-load (from 100 to 200 mA", "keywords": ["switched-capacitor dc-dc converter", "interleaving regulation"]}
{"id": "kp20k_training_87", "title": "Teeth recognition based on multiple attempts in mobile device", "abstract": "Most traditional biometric approaches generally utilize a single image for personal identification. However, these approaches sometimes failed to recognize users in practical environment due to false-detected or undetected subject. Therefore, this paper proposes a novel recognition approach based on multiple frame images that are implemented in mobile devices. The aim of this paper is to improve the recognition accuracy and to reduce computational complexity through multiple attempts. Here, multiple attempts denote that multiple frame images are used in time of recognition procedure. Among sequential frame images, an adequate subject, i.e., teeth image, is chosen by subject selection module which is operated based on differential image entropy. The selected subject is then utilized as a biometric trait of traditional recognition algorithms including PCA, LDA, and EHMM. The performance evaluation of proposed method is performed using two teeth databases constructed by a mobile device. Through experimental results, we confirm that the proposed method exhibits improved recognition accuracy of about 3.64.8%, and offers the advantage of lower computational complexity than traditional biometric approaches", "keywords": ["teeth recognition", "multiple attempts", "subject selection", "mobile device"]}
{"id": "kp20k_training_88", "title": "A conceptual approach for the die structure design", "abstract": "A large number of decisions are made during the conceptual design stage which is characterized by a lack of complete geometric information. While existing CAD systems supporting the geometric aspects of design have had little impact at the conceptual design stage. To support the conceptual die design and the top-down design process, a new concept called conceptual assembly modeling framework (CAMF) is presented in this paper. Firstly, the framework employs the zigzag function-symbol mapping to implement the function design of the die. From the easily understood analytical results of the function-symbol mapping matrix, the designer can evaluate the quality of a proposed die concept. Secondly, a new method-logic assembly modeling is proposed using logic components in this framework to satisfy the characteristic of the conceptual die design. Representing shapes and spatial relations in logic can provide a natural, intuitive method of developing complete computer systems for reasoning about die construction design at the conceptual stage. The logic assembly which consists of logic components is an innovative representation that provides a natural link between the function design of the die and the detailed geometric design", "keywords": ["cad", "conceptual design", "die structure design", "zigzag mapping", "logic component", "logic assembly"]}
{"id": "kp20k_training_89", "title": "Approximation algorithm for coloring of dotted interval graphs", "abstract": "Dotted interval graphs were introduced by Aumann et al. [Y. Aumann, M. Lewenstein, O. Melamud, R. Pinter, Z. Yakhini, Dotted interval graphs and high throughput genotyping, in: ACM-SIAM Symposium on Discrete Algorithms. SODA 2005, pp. 339-348] as a generalization of interval graphs. The problem of coloring these graphs found application in high-throughput genotyping. Jiang [M. Jiang, Approximating minimum coloring and maximum independent set in dotted interval graphs, Information Processing Letters 98 (2006) 29-33] improves the approximation ratio of Aumann et al. [Y. Aumann, M. Lewenstein, O. Melamud, R. Pinter, Z. Yakhini, Dotted interval graphs and high throughput genotyping, in: ACM-SIAM Symposium on Discrete Algorithms, SODA 2005, pp. 339-348]. In this work we improve the approximation ratio of Jiang [M. Jiang, Approximating minimum coloring and maximum independent set in dotted interval graphs, Information Processing Letters 98 (2006) 29-33] and Aumarm et al. [Y. Aumann, M. Lewenstein, O. Melamud, R. Pinter, Z. Yakhini, Dotted interval graphs and high throughput genotyping, in: ACM-SIAM Symposium on Discrete Algorithms, SODA 2005, pp. 339-348]. In the exposition we develop a generalization of the problem of finding the maximum number of non-attacking queens on a triangle.  ", "keywords": ["approximation algorithms", "dotted interval graph", "intersection graph", "minimum coloring", "microsatellite genotyping"]}
{"id": "kp20k_training_90", "title": "Scalable visibility color map construction in spatial databases", "abstract": "Recent advances in 3D modeling provide us with real 3D datasets to answer queries, such as What is the best position for a new billboard? and Which hotel room has the best view? in the presence of obstacles. These applications require measuring and differentiating the visibility of an object (target) from different viewpoints in a dataspace, e.g., a billboard may be seen from many points but is readable only from a few points closer to it. In this paper, we formulate the above problem of quantifying the visibility of (from) a target object from (of) the surrounding area with a visibility color map (VCM). A VCM is essentially defined as a surface color map of the space, where each viewpoint of the space is assigned a color value that denotes the visibility measure of the target from that viewpoint. Measuring the visibility of a target even from a single viewpoint is an expensive operation, as we need to consider factors such as distance, angle, and obstacles between the viewpoint and the target. Hence, a straightforward approach to construct the VCM that requires visibility computation for every viewpoint of the surrounding space of the target is prohibitively expensive in terms of both I/Os and computation, especially for a real dataset comprising thousands of obstacles. We propose an efficient approach to compute the VCM based on a key property of the human vision that eliminates the necessity for computing the visibility for a large number of viewpoints of the space. To further reduce the computational overhead, we propose two approximations; namely, minimum bounding rectangle and tangential approaches with guaranteed error bounds. Our extensive experiments demonstrate the effectiveness and efficiency of our solutions to construct the VCM for real 2D and 3D datasets", "keywords": ["spatial databases", "query processing", "three-dimensional  objects", "visibility color map"]}
{"id": "kp20k_training_91", "title": "Toward a Neurogenetic Theory of Neuroticism", "abstract": "Recent advances in neuroscience and molecular biology have begun to identify neural and genetic correlates of complex traits. Future theories of personality need to integrate these data across the behavioral, neural, and genetic level of analysis and further explain the underlying epigenetic processes by which genes and environmental variables interact to shape the structure and function of neural circuitry. In this chapter, I will review some of the work that has been conducted at the cognitive, neural, and molecular genetic level with respect to one specific personality traitneuroticism. I will focus particularly on individual differences with respect to memory, self-reference, perception, and attention during processing of emotional stimuli and the significance of gene-by-environment interactions. This chapter is intended to serve as a tutorial bridge for psychologists who may be intrigued by molecular genetics and for molecular biologists who may be curious about how to apply their research to the study of personality", "keywords": ["neuroticism", "personality", "complex traits"]}
{"id": "kp20k_training_92", "title": "Technological means of communication and collaboration in archives and records management", "abstract": "This study explores the international collaboration efforts of archivists and records managers starting with the hypothesis that Internet technologies have had a significant impact on both national and international communication for this previously conservative group. The use and importance of mailing lists for this purpose is studied in detail. A quantitative analysis looks globally at the numbers of lists in these fields and the numbers of subscribers. A qualitative analysis of list content is also described. The study finds that archivists and records managers have now created more than 140 mailing lists related to their profession and have been contributing to these lists actively. It also 'estimates' that about half of the profession follows a list relating to their work and that archivists seem to like lists more than records managers do. The study concludes that mailing lists can be seen as a virtual college binding these groups together to develop the field", "keywords": ["archives administration", "records management", "internet", "mailing lists", "forums"]}
{"id": "kp20k_training_93", "title": "Privacy Preserving Decision Tree Learning Using Unrealized Data Sets", "abstract": "Privacy preservation is important for machine learning and data mining, but measures designed to protect private information often result in a trade-off: reduced utility of the training samples. This paper introduces a privacy preserving approach that can be applied to decision tree learning, without concomitant loss of accuracy. It describes an approach to the preservation of the privacy of collected data samples in cases where information from the sample database has been partially lost. This approach converts the original sample data sets into a group of unreal data sets, from which the original samples cannot be reconstructed without the entire group of unreal data sets. Meanwhile, an accurate decision tree can be built directly from those unreal data sets. This novel approach can be applied directly to the data storage as soon as the first sample is collected. The approach is compatible with other privacy preserving approaches, such as cryptography, for extra protection", "keywords": ["classification", "data mining", "machine learning", "security and privacy protection"]}
{"id": "kp20k_training_95", "title": "Fast parameter-free region growing segmentation with application to surgical planning", "abstract": "In this paper, we propose a self-assessed adaptive region growing segmentation algorithm. In the context of an experimental virtual-reality surgical planning software platform, our method successfully delineates main tissues relevant for reconstructive surgery, such as fat, muscle, and bone. We rely on a self-tuning approach to deal with a great variety of imaging conditions requiring limited user intervention (one seed). The detection of the optimal parameters is managed internally using a measure of the varying contrast of the growing region, and the stopping criterion is adapted to the noise level in the dataset thanks to the sampling strategy used for the assessment function. Sampling is referred to the statistics of a neighborhood around the seed(s), so that the sampling period becomes greater when images are noisier, resulting in the acquisition of a lower frequency version of the contrast function. Validation is provided for synthetic images, as well as real CT datasets. For the CT test images, validation is referred to manual delineations for 10 cases and to subjective assessment for another 35. High values of sensitivity and specificity, as well as Dice's coefficient and Jaccard's index on one hand, and satisfactory subjective evaluation on the other hand, prove the robustness of our contrast-based measure, even suggesting suitability for calibration of other region-based segmentation algorithms", "keywords": ["ct", "segmentation", "region growing", "surgical planning", "virtual reality"]}
{"id": "kp20k_training_96", "title": "Accuracy and efficiency in computing electrostatic potential for an ion channel model in layered dielectric/electrolyte media", "abstract": "This paper will investigate the numerical accuracy and efficiency in computing the electrostatic potential for a finite-height cylinder, used in an explicit/implicit hybrid solvation model for ion channel and embedded in a layered dielectric/electrolyte medium representing a biological membrane and ionic solvents. A charge locating inside the cylinder cavity, where ion channel proteins and ions are given explicit atomistic representations, will be influenced by the polarization field of the surrounding implicit dielectric/electrolyte medium. Two numerical techniques, a specially designed boundary integral equation method and an image charge method, will be investigated and compared in terms of accuracy and efficiency for computing the electrostatic potential. The boundary integral equation method based on the three-dimensional layered Green?s functions provides a highly accurate solution suitable for producing a benchmark reference solution, while the image charge method is found to give reasonable accuracy and highly efficient and viable to use the fast multipole method for interactions of a large number of charges in the atomistic region of the hybrid solvation model", "keywords": ["poissonboltzmann equation", "layered electrolytes and dielectrics", "image charge method", "ion channels", "the explicit/implicit hybrid solvation model"]}
{"id": "kp20k_training_97", "title": "The social sharing of emotion (SSE) in online social networks: A case study in Live Journal", "abstract": "Using content analysis, we gauge the occurrence of social sharing of emotion (SSE) in Live Journal. We present a theoretical model of a three-cycle process for online SSE. A large part of emotional blog posts showed full initiation of social sharing. Affective feedback provided empathy, emotional support and admiration. This study is the first one to empirically assess the occurrence and structure of online SSE", "keywords": ["social sharing of emotion", "online communication", "social networking sites", "blog", "social interaction", "emotion"]}
{"id": "kp20k_training_98", "title": "Non-testing approaches under REACH - help or hindrance? Perspectives from a practitioner within industry", "abstract": "Legislation such as REACH strongly advocates the use of alternative approaches including invitro, (Q)SARs, and chemical categories as a means to satisfy the information requirements for risk assessment. One of the most promising alternative approaches is that of chemical categories, where the underlying hypothesis is that the compounds within the category are similar and therefore should have similar biological activities. The challenge lies in characterizing the chemicals, understanding the mode/mechanism of action for the activity of interest and deriving a way of relating these together to form inferences about the likely activity outcomes. (Q)SARs are underpinned by the same hypothesis but are packaged in a more formalized manner. Since the publication of the White Paper for REACH, there have been a number of efforts aimed at developing tools, approaches and techniques for (Q)SARs and read-across for regulatory purposes. While technical guidance is available, there still remains little practical guidance about how these approaches can or should be applied in either the evaluation of existing (Q)SARs or in the formation of robust categories. Here we provide a perspective of how some of these approaches have been utilized to address our in-house REACH requirements", "keywords": ["reach", "sar", "chemical category", "qmrf", "qprf"]}
{"id": "kp20k_training_99", "title": "Realtime performance analysis of different combinations of fuzzyPID and bias controllers for a two degree of freedom electrohydraulic parallel manipulator", "abstract": "Development of a 2 DOF electrohydraulic motion simulator as a parallel manipulator. Control of heave, pitch and combined heave and pitch motion of the parallel manipulator. Design of PID, fuzzyPID, self-tuning fuzzyPID and self-tuning fuzzyPID with bias controllers. Use of different combinations of fuzzyPID and bias controllers for study of real time control performance. Best control response found for the self-tuning fuzzyPID with bias controller", "keywords": ["electrohydraulic systems", "real-time control", "parallel manipulator", "fuzzy control"]}
{"id": "kp20k_training_100", "title": "On the depth distribution of linear codes", "abstract": "The depth distribution of a linear code was recently introduced by Etzion. In this correspondence, a number of basic and interesting properties for the depth of finite words and the depth distribution of linear codes are obtained. In addition, we study the enumeration problem of counting the number of linear subcodes with the prescribed depth constraints, and derive some explicit and interesting enumeration formulas. Furthermore, we determine the depth distribution of Reed-Muller code RM (m, r). Finally, we show that there are exactly nine depth-equivalence classes for the ternary [11, 6, 5] Golay codes", "keywords": ["depth", "depth distribution", "depth-equivalence classes", "derivative", "linear codes", "reed-muller codes", "ternary golay code"]}
{"id": "kp20k_training_101", "title": "Are we there yet", "abstract": "Statistical approaches to Artificial Intelligence are behind most success stories of the field in the past decade. The idea of generating non-trivial behaviour by analysing vast amounts of data has enabled recommendation systems, search engines, spam filters, optical character recognition, machine translation and speech recognition, among other things. As we celebrate the spectacular achievements of this line of research, we need to assess its full potential and its limitations. What are the next steps to take towards machine intelligence", "keywords": ["artificial intelligence", "intelligent behaviour", "cybernetics", "statistical learning theory", "data driven ai", "intelligent systems", "pattern analysis", "viterbis algorithm", "history of artificial intelligence"]}
{"id": "kp20k_training_102", "title": "The complexity of the matroid-greedoid partition problem", "abstract": "We show that the maximum matroid-greedoid partition problem is NP-hard to approximate to within 1/2 + epsilon for any epsilon > 0, which matches the trivial factor 1/2 approximation algorithm. The main tool in our hardness of approximation result is an extractor code with polynomial rate, alphabet size and list size, together with an efficient algorithm for list-decoding. We show that the recent extractor construction of Guruswami, Umans and Vadhan [V. Guruswami. C. Umans, S.P. Vadhan, Unbalanced expanders and randomness extractors from Parvaresh-Vardy codes, in: IEEE Conference on Computational Complexity, IEEE Computer Society, 2007, pp. 96-108] can be used to obtain a code with these properties. We also show that the parameterized matroid-greedoid partition problem is fixed-parameter tractable.  ", "keywords": ["matroid", "greedoid", "matroid partition problem", "extractor codes", "fixed-parameter complexity"]}
{"id": "kp20k_training_103", "title": "Exploring the ncRNAncRNA patterns based on bridging rules", "abstract": "ncRNAs play an important role in the regulation of gene expression. However, many of their functions have not yet been fully discovered. There are complicated relationships between ncRNAs in different categories. Finding these relationships can contribute to identify ncRNAs functions and properties. We extend the association rule to represent the relationship between two ncRNAs. Based on this rule, we can speculate the ncRNAs function when it interacts with other ncRNAs. We propose two measures to explore the relationships between ncRNAs in different categories. Entropy theory is to calculate how close two ncRNAs are. Association rule is to represent the interactions between ncRNAs. We use three datasets from miRBase and RNAdb. Two from miRBase are designed for finding relationships between miRNAs; the other from RNAdb is designed for relationships among miRNA, snoRNA and piRNA. We evaluate our measures from both biological significance and performance perspectives. All the cross-species patterns regarding miRNA that we found are proven correct using miRNAMap 2.0. In addition, we find novel cross-genomes patterns such as (hsa-mir-190b?hsa-mir-153-2). According to the patterns we find, we can (1) explore one ncRNAs function from another with known function and (2) speculate the functions of both of them based on the relationship even we do no understand either of them. Our methods merits also include: (1) they are suitable for any ncRNA datasets and (2) they are not sensitive to the parameters", "keywords": ["ncrnas", "bridging rules", "entropy", "mirna", "joint entropy", "mutual information"]}
{"id": "kp20k_training_104", "title": "Gaussian mixture modelling to detect random walks in capital markets", "abstract": "In this paper, Gaussian mixture modelling is used to detect random walks in capital markets with the Kolmogorov-Smirnov test. The main idea is to use Gaussian mixture modelling to fit asset return distributions and then use the Kolmogorov-Smirnov test to determine the number of components. Several quantities are used to characterize Gaussian mixture models and ascertain whether random walks exist in capital markets. Empirical studies on China securities markets and Forex markets are used to demonstrate the proposed procedure.  ", "keywords": ["gaussian mixture modelling", "the random walks hypothesis", "asset return distributions", "em algorithm", "the kolmogorov-smirnov test"]}
{"id": "kp20k_training_105", "title": "Scientific design rationale", "abstract": "Design rationale should be regarded both as a tool for the practice of design, and as a method to enable the science of design. Design rationale answers questions about why a given design takes the form that it does. Answers to these why questions represent a significant portion of the knowledge generated from design research. This knowledge, along with that from empirical studies of designs in use, contributes to what Simon called the sciences of the artificial. Most research on the nature and use of design rationale has been analytic or theoretical. In this article, we describe an empirical study of the roles that design rationale can play in the conduct of design research. We report results from an interview study with 16 design researchers investigating how they construe and carry out design as research. The results include an integrated framework of the affordances design rationale can contribute to design research. The framework and supporting qualitative data provide insight into how design rationale might be more effectively leveraged as a first-class methodology for research into the creation and use of artifacts", "keywords": ["affordances", "design rationale", "design research", "design research methodology"]}
{"id": "kp20k_training_106", "title": "High flowability monomer resists for thermal nanoimprint lithography", "abstract": "In this paper, we have been using polymer and thermally curable monomer resists in a full 8in. wafer thermal nanoimprint lithography process. Using exactly the same imprinting conditions, we observed that a monomer solution provides a much larger resist redistribution than a polymer resist. Imprinting Fresnel zone plates, composed of micro- and nano-meter features, was possible only with the monomer resist. In order to reduce the shrinkage ratio of the monomer resists, acrylatesilsesquioxane materials were synthesised. With a simple diffusion-like model, we could extract a mean free path of 1.1mm for the monomer resist, while a polymer flows only on distances below 10?m in the same conditions", "keywords": ["nanoimprint lithography", "monomer resists", "flow properties", "polyhedral silsesquioxane"]}
{"id": "kp20k_training_107", "title": "Binarized Support Vector Machines", "abstract": "The widely used support vector machine (SVM) method has shown to yield very good results in supervised classification problems. Other methods such as classification trees have become more popular among practitioners than SVM thanks to their interpretability, which is an important issue in data mining. In this work, we propose an SVM-based method that automatically detects the most important predictor variables and the role they play in the classifier. In particular, the proposed method is able to detect those values and intervals that are critical for the classification. The method involves the optimization of a linear programming problem in the spirit of the Lasso method with a large number of decision variables. The numerical experience reported shows that a rather direct use of the standard column generation strategy leads to a classification method that, in terms of classification ability, is competitive against the standard linear SVM and classification trees. Moreover, the proposed method is robust; i.e., it is stable in the presence of outliers and invariant to change of scale or measurement units of the predictor variables. When the complexity of the classifier is an important issue, a wrapper feature selection method is applied, yielding simpler but still competitive classifiers", "keywords": ["supervised classification", "binarization", "column generation", "support vector machines"]}
{"id": "kp20k_training_108", "title": "Ambrosio-Tortorelli Segmentation of Stochastic Images: Model Extensions, Theoretical Investigations and Numerical Methods", "abstract": "We discuss an extension of the Ambrosio-Tortorelli approximation of the Mumford-Shah functional for the segmentation of images with uncertain gray values resulting from measurement errors and noise. Our approach yields a reliable precision estimate for the segmentation result, and it allows us to quantify the robustness of edges in noisy images and under gray value uncertainty. We develop an ansatz space for such images by identifying gray values with random variables. The use of these stochastic images in the minimization of energies of Ambrosio-Tortorelli type leads to stochastic partial differential equations for a stochastic smoothed version of the original image and a stochastic phase field for the edge set. For the discretization of these equations we utilize the generalized polynomial chaos expansion and the generalized spectral decomposition (GSD) method. In contrast to the simple classical sampling technique, this approach allows for an efficient determination of the stochastic properties of the output image and edge set by computations on an optimally small set of random variables. Also, we use an adaptive grid approach for the spatial dimensions to further improve the performance, and we extend an edge linking method for the classical Ambrosio-Tortorelli model for use with our stochastic model. The performance of the method is demonstrated on artificial data and a data set from a digital camera as well as real medical ultrasound data. A comparison of the intrusive GSD discretization with a stochastic collocation and a Monte Carlo sampling is shown", "keywords": ["image processing", "ambrosio-tortorelli model", "segmentation", "uncertainty", "stochastic images", "stochastic partial differential equations", "polynomial chaos", "generalized spectral decomposition", "adaptive grid", "edge linking"]}
{"id": "kp20k_training_109", "title": "A provably convergent heuristic for stochastic bicriteria integer programming", "abstract": "We propose a general-purpose algorithm APS (Adaptive Pareto-Sampling) for determining the set of Pareto-optimal solutions of bicriteria combinatorial optimization (CO) problems under uncertainty, where the objective functions are expectations of random variables depending on a decision from a finite feasible set. APS is iterative and population-based and combines random sampling with the solution of corresponding deterministic bicriteria CO problem instances. Special attention is given to the case where the corresponding deterministic bicriteria CO problem can be formulated as a bicriteria integer linear program (ILP). In this case, well-known solution techniques such as the algorithm by Chalmet et al. can be applied for solving the deterministic subproblem. If the execution of APS is terminated after a given number of iterations, only an approximate solution is obtained in general, such that APS must be considered a metaheuristic. Nevertheless, a strict mathematical result is shown that ensures, under rather mild conditions, convergence of the current solution set to the set of Pareto-optimal solutions. A modification replacing or supporting the bicriteria ILP solver by some metaheuristic for multicriteria CO problems is discussed. As an illustration, we outline the application of the method to stochastic bicriteria knapsack problems by specializing the general framework to this particular case and by providing computational examples", "keywords": ["combinatorial optimization", "convergence proof", "integer programming", "metaheuristics", "stochastic optimization"]}
{"id": "kp20k_training_110", "title": "The impact of a simulation game on operations management education", "abstract": "This study presents a new simulation game and analyzes its impact on operations management education. The proposed simulation was empirically tested by comparing the number of mistakes during the first and second halves of the game. Data were gathered from 100 teams of four or five undergraduate students in business administration, taking their first course in operations management. To assess learning, instead of relying solely on an overall performance measurement, as is usually done in the skill-based learning literature, we analyzed the evolution of different types of mistakes that were made by students in successive rounds of play. Our results show that although simple decision-making skills can be acquired with traditional teaching methods, simulation games are more effective when students have to develop decision-making abilities for managing complex and dynamic situations.  ", "keywords": ["simulations", "interactive learning environment", "applications in operations management", "post-secondary education"]}
{"id": "kp20k_training_111", "title": "Covering a set of points in a plane using two parallel rectangles", "abstract": "In this paper we consider the problem of finding two parallel rectangles in arbitrary orientation for covering a given set of n points in a plane, such that the area of the larger rectangle is minimized. We propose an algorithm that solves the problem in O(n(3)) time using O(n(2)) space. Without altering the complexity, our approach can be used to solve another optimization problem namely, minimize the sum of the areas of two arbitrarily oriented parallel rectangles covering a given set of points in a plane.  ", "keywords": ["algorithms", "computational geometry", "covering", "optimization", "rectangles"]}
{"id": "kp20k_training_112", "title": "Investigating the extreme programming system - An empirical study", "abstract": "In this paper we discuss our empirical study about the advantages and difficulties 15 Greek software companies experienced applying Extreme Programming (XP) as a holistic system in software development. Based on a generic XP system including feedback influences and using a cause-effect model including social-technical affecting factors, as our research tool, the study statistically evaluates the application of XP practices in the software companies being studied. Data were collected from 30 managers and developers, using the sample survey technique with questionnaires and interviews, in a time period of six months. Practices were analysed individually, using Descriptive Statistics (DS), and as a whole by building up different models using stepwise Discriminant Analysis (DA). The results have shown that companies, facing various problems with common code ownership, on-site customer, 40-hour week and metaphor, prefer to develop their own tailored XP method and way of working-practices that met their requirements. Pair programming and test-driven development were found to be the most significant success factors. Interactions and hidden dependencies for the majority of the practices as well as communication and synergy between skilled personnel were found to be other significant success factors. The contribution of this preliminary research work is to provide some evidence that may assist companies in evaluating whether the XP system as a holistic framework would suit their current situation", "keywords": ["agile methods", "extreme programming system", "cause-effect model", "feedback model", "developer perception", "manager perception", "empirical study", "stepwise discriminant analysis", "planning game", "pair programming", "test-driven development", "refactoring", "simple design", "common code ownership", "continuous integration", "on-site customer", "short release cycles", "40-hour-week", "coding standards", "metaphor"]}
{"id": "kp20k_training_113", "title": "An optimal GTS scheduling algorithm for time-sensitive transactions in IEEE 802.15.4 networks", "abstract": "IEEE 802.15.4 is a new enabling standard for low-rate wireless personal area networks and has been widely accepted as a de facto standard for wireless sensor networking. While primary motivations behind 802.15.4 are low power and low cost wireless communications, the standard also supports time and rate sensitive applications because of its ability to operate in TDMA access modes. The TDMA mode of operation is supported via the Guaranteed Time Slot (GTS) feature of the standard. In a beacon-enabled network topology, the Personal Area Network (PAN) coordinator reserves and assigns the GTS to applications on a first-come-first-served (FCFS) basis in response to requests from wireless sensor nodes. This fixed FCFS scheduling service offered by the standard may not satisfy the time constraints of time-sensitive transactions with delay deadlines. Such operating scenarios often arise in wireless video surveillance and target detection applications running on sensor networks. In this paper, we design an optimal work-conserving scheduling algorithm for meeting the delay constraints of time-sensitive transactions and show that the proposed algorithm outperforms the existing scheduling model specified in IEEE 802.15.4", "keywords": ["gts", "scheduling", "lr-wpan", "schedulability", "edf"]}
{"id": "kp20k_training_114", "title": "CONTROLLED DENSE CODING WITH CLUSTER STATE", "abstract": "Two schemes for controlled dense coding with a one-dimensional four-particle cluster state are investigated. In this protocol, the supervisor (Cliff) can control the channel and the average amount of information transmitted from the sender (Alice) to the receiver (Bob) by adjusting the local measurement angle theta. It is shown that the results for the average amounts of information are unique from the different two schemes", "keywords": ["controlled dense coding", "cluster state", "average amount of information", "povm"]}
{"id": "kp20k_training_115", "title": "Slope stability analysis using the limit equilibrium method and two finite element methods", "abstract": "In this paper, the factors of safety and critical slip surfaces obtained by the limit equilibrium method (LEM) and two finite element methods (the enhanced limit strength method (ELSM) and strength reduction method (SRM)) are compared. Several representative two-dimensional slope examples are analysed. Using the associated flow rule, the results showed that the two finite element methods were generally in good agreement and that the LEM yielded a slightly lower factor of safety than the two finite element methods did. Moreover, a key condition regarding the stress field is shown to be necessary for ELSM analysis", "keywords": ["lem limit equilibrium method", "srm strength reduction method", "elsm enhanced limit strength method", "fos factor of safety", "srf strength reduction factor", "pso particle swarm optimisation"]}
{"id": "kp20k_training_116", "title": "Deformation invariant attribute vector for deformable registration of longitudinal brain MR images", "abstract": "This paper presents a novel approach to define deformation invariant attribute vector (DIAV) for each voxel in 3D brain image for the purpose of anatomic correspondence detection. The DIAV method is validated by using synthesized deformation in 3D brain MRI images. Both theoretic analysis and experimental studies demonstrate that the proposed DIAV is invariant to general nonlinear deformation. Moreover, our experimental results show that the DIAV is able to capture rich anatomic information around the voxels and exhibit strong discriminative ability. The DIAV has been integrated into a deformable registration algorithm for longitudinal brain MR images, and the results on both simulated and real brain images are provided to demonstrate the good performance of the proposed registration algorithm based on matching of DIAVs", "keywords": ["deformable registration", "longitudinal imaging", "brain mri", "deformation invariant attribute vector"]}
{"id": "kp20k_training_117", "title": "Carbapenem-resistant Enterobacteriaceae: biology, epidemiology, and management", "abstract": "Introduced in the 1980s, carbapenem antibiotics have served as the last line of defense against multidrug-resistant Gram-negative organisms. Over the last decade, carbapenem-resistant Enterobacteriaceae (CRE) have emerged as a significant public health threat. This review summarizes the molecular genetics, natural history, and epidemiology of CRE and discusses approaches to prevention and treatment", "keywords": ["carbapenem-resistant enterobacteriaceae", "antimicrobial resistance", "carbapenemases", "molecular genetics", "infection control", "treatment"]}
{"id": "kp20k_training_118", "title": "hypergraph-based inductive learning for generating implicit key phrases", "abstract": "This paper presents a novel approach to generate implicit key phrases which are ignored in previous researches. Recent researches prefer to extract key phrases with semi-supervised transductive learning methods, which avoid the problem of training data. In this paper, based on a transductive learning method, we formulate the phrases in the document as a hypergraph and expand the hypergraph to include implicit phrases, which are ranked by an inductive learning approach. The highest ranked phrases are seen as implicit key phrases, and experimental results demonstrate the satisfactory performance of this approach", "keywords": ["hypergraph", "implicit key phrase", "inductive semi-supervised learning", "key phrase generation", "transductive learning"]}
{"id": "kp20k_training_119", "title": "Strategic commitment to price to stimulate downstream innovation in a supply chain", "abstract": "It is generally in a firms interest for its supply chain partners to invest in innovations. To the extent that these innovations either reduce the partners variable costs or stimulate demand for the end product, they will tend to lead to higher levels of output for all of the firms in the chain. However, in response to the innovations of its partners, a firm may have an incentive to opportunistically increase its own prices. The possibility of such opportunistic behavior creates a hold-up problem that leads supply chain partners to underinvest in innovation. Clearly, this hold-up problem could be eliminated by a pre-commitment to price. However, by making an advance commitment to price, a firm sacrifices an important means of responding to demand uncertainty. In this paper we examine the trade-off that is faced when a firms channel partner has opportunities to invest in either cost reduction or quality improvement, i.e. demand enhancement. Should it commit to a price in order to encourage innovation, or should it remain flexible in order to respond to demand uncertainty. We discuss several simple wholesale pricing mechanisms with respect to this trade-off", "keywords": ["channel coordination", "channels of distribution", "industrial organization", "cost reducing r&d"]}
{"id": "kp20k_training_120", "title": "mutation-based software testing using program schemata", "abstract": "Mutation analysis is a powerful technique for assessing the quality of test data used in unit testing software. Unfortunately, current automated mutation analysis systems suffer from severe performance problems. In this paper the principles of mutation analysis are reviewed, current automation approaches are described, and a new method of performing mutation analysis is outlined. Performance improvements of over 300% are reported and other advantages of this new method are highlighted", "keywords": ["software testing", "software", "quality", "method", "systems", "fault-based testing", "test", "unit test", "analysis", "performance", "mutation", "mutation analysis", "program schemata", "data", "paper", "automation"]}
{"id": "kp20k_training_121", "title": "Bamboo: A Data-Centric, Object-Oriented Approach to Many-core Software", "abstract": "Traditional data-oriented programming languages such as dataflow languages and stream languages provide a natural abstraction for parallel programming. In these languages, a developer focuses on the flow of data through the computation and these systems free the developer from the complexities of low-level, thread-oriented concurrency primitives. This simplification comes at a cost-traditional data-oriented approaches restrict the mutation of state and, in practice, the types of data structures a program can effectively use. Bamboo borrows from work in typestate and software transactions to relax the traditional restrictions of data-oriented programming models to support mutation of arbitrary data structures. We have implemented a compiler for Bamboo which generates code for the TILEPro64 many-core processor. We have evaluated this implementation on six benchmarks: Tracking, a feature tracking algorithm from computer vision; KMeans, a K-means clustering algorithm; MonteCarlo, a Monte Carlo simulation; FilterBank, a multi-channel filter bank; Fractal, a Mandelbrot set computation; and Series, a Fourier series computation. We found that our compiler generated implementations that obtained speedups ranging from 26.2x to 61.6x when executed on 62 cores", "keywords": ["algorithms", "languages", "many-core programming", "data-centric languages"]}
{"id": "kp20k_training_122", "title": "Performance optimization problem in speculative prefetching", "abstract": "Speculative prefetching has been proposed to improve the response time of network access. Previous studies in speculative prefetching focus on building and evaluating access models for the purpose of access prediction. This paper investigates a complementary area which has been largely ignored, that of performance modeling. We analyze the performance of a prefetcher that has uncertain knowledge about future accesses. Our performance metric is the improvement in access time, for which we derive a formula in terms of resource parameters (time available and time required for prefetching) and speculative parameters (probabilities for next access). We develop a prefetch algorithm to maximize the improvement in access time. The algorithm is based on finding the best solution to a stretch knapsack problem, using theoretically proven apparatus to reduce the search space. An integration between speculative prefetching and caching is also investigated", "keywords": ["speculative prefetching", "caching"]}
{"id": "kp20k_training_123", "title": "inspiring collaboration through the use of videoconferencing technology", "abstract": "At the beginning of 2007 the University of Washington opened the Odegaard Videoconference Studio which allowed groups on campus to communicate with colleagues that were physically in different locations. The opening of this facility inspired all sorts of collaborating on a more frequent basis as traveling, and more importantly the time and expense involved with traveling, was now not as necessary in order to have a meeting. Many boundaries for collaboration were removed through the use of different types of technology that allowed for video and audio conferencing, and, data and application sharing. This provided for a way to share ideas in more detail, make decisions, and receive feedback quicker, making the overall process more efficient, personal, and overall more effective", "keywords": ["collaboration technologies", "videoconferencing"]}
{"id": "kp20k_training_124", "title": "expanders, sorting in rounds and superconcentrators of limited depth", "abstract": "Expanding graphs and superconcentrators are relevant to theoretical computer science in several ways. Here we use finite geometries to construct explicitly highly expanding graphs with essentially the smallest possible number of edges. Our graphs enable us to improve significantly previous results on a parallel sorting problem, by describing an explicit algorithm to sort n elements in k time units using &Ogr;( n &agr;k ) processors, where, e.g., &agr; 2 = 7/4. Using our graphs we can also construct efficient n-superconcentrators of limited depth. For example, we construct an n superconcentrator of depth 3 with &Ogr;( n 4/3 ) edges; better than the previous known results", "keywords": ["processor", "computer science", "graph", "sorting", "relevance", "timing", "use", "examples", "efficiency", "algorithm", "parallel"]}
{"id": "kp20k_training_125", "title": "Synchrony and frequency regulation by synaptic delay in networks of self-inhibiting neurons", "abstract": "We show that a pair of mutually coupled self-inhibitory neurons can display stable synchronous oscillations provided only that the delay to the onset of inhibition is sufficiently long. The frequency of these oscillations is determined either entirely by the length of the synaptic delay, or by the synaptic delay and intrinsic time constants. We also show how cells can exhibit transient synchronous oscillations where the length of the transients is determined by the synaptic delay, but where the frequency is largely independent of the delay", "keywords": ["synchronous oscillations", "inhibition", "synaptic delay"]}
{"id": "kp20k_training_126", "title": "minimizing power dissipation during write operation to register files", "abstract": "This paper presents a power reduction mechanism for the write operation in register files (RegFiles), which adds a conditional charge-sharing structure to the pair of complementary bit-lines in each column of the RegFile. Because the read and write ports for the RegFile are separately implemented, it is possible to avoid pre-charging the bit-line pair for consecutive writes. More precisely, when writing same values to some cells in the same column of the RegFile, it is possible to eliminate energy consumption due to precharging of the bit-line pair. At the same time, when writing opposite values to some cells in the same column of the RegFile, it is possible to reduce energy consumed in charging the bit-line pair thanks to charge-sharing. Motivated by these observations, we modify the bit-line structure of the write ports in the RegFile such that i) we remove per-cycle bitline pre-charging and ii) we employ conditional data dependent charge-sharing. Experimental results on a set of SPEC2000INT / MediaBench benchmarks show an average of 61.5% energy savings with 5.1% area overhead and 16.2% increase in write access delay", "keywords": ["power", "write operation", "register file"]}
{"id": "kp20k_training_127", "title": "A decision support framework for metrics selection in goal-based measurement programs: GQM-DSFMS", "abstract": "Complex GQM-based measurement programs lead to the need for decision support in metric selection. We provide an decision support framework in choosing an optimal set of metrics to maximize measurement goal achievement for a given budget. The framework was evaluated by comparison with expert opinion in a CMMI Level 3 company. Extent of addressing information needs under a fixed budged was higher when selecting metrics using the framework", "keywords": ["software measurement program", "goal based measurement", "goal question metric", "gqm", "decision support", "optimization", "prioritization"]}
{"id": "kp20k_training_128", "title": "energy/area/delay trade-offs in the physical design of on-chip segmented bus architecture", "abstract": "The increasing gap between design productivity and chip complexity, and the emerging Systems-On-Chip (SOC) architectural template have led to the wide utilization of reusable Intellectual Property (IP) cores. The physical design implementation of the macro cells (IP blocks or pre-designed blocks) in general needs to find a well balanced solution among chip area, on-chip interconnect energy and critical path delay. We are especially interested in the entire trade-off curve among these three criteria at the floorplanning stage. We show this concept for a real communication scheme based on segmented bus, rather than just an extreme solution. A fast exploration design flow from the memory organization to the final layout is introduced to explore the design space", "keywords": ["communication", "floorplanning", "physical design", "design space", "intellectual property", "design", "delay", "layout", "concept", "general", "reusability", "exploration", "trade-offs", "organization", "product", "macros", "segmented bus", "architecture", "energy", "implementation", "memorialized", "flow", "interconnect", "system on-chip", "complexity", "critic", "template", "scheme"]}
{"id": "kp20k_training_129", "title": "System integration of a miniature rotorcraft for aerial tele-operation research", "abstract": "This paper describes the development and integration of the systems required for research into human interaction with a tele-operated miniature rotorcraft. Because of the focus on vehicles capable of operating indoors, the size of the vehicle was limited to 35 cm, and therefore the hardware had to be carefully chosen to meet the ensuing size and weight constraints, while providing sufficient flight endurance. The components described in this work include the flight hardware, electronics, sensors, and software necessary to conduct tele-operation experiments. The integration tasks fall into three main areas. First, the paper discusses the choice of rotorcraft platform best suited for indoor operation addressing the issues of size, payload capabilities, and power consumption. The second task was to determine what electronics and sensing could be integrated into a rotorcraft with significant payload limitations. Finally, the third task involved characterizing the various components both individually and as a complete system. The paper concludes with an overview of ongoing tele-operation research performed with the embedded rotorcraft platform.  ", "keywords": ["miniature rotorcraft", "embedded systems", "indoor navigation"]}
{"id": "kp20k_training_130", "title": "Rank inclusion in criteria hierarchies", "abstract": "This paper presents a method called Rank Inclusion in Criteria Hierarchies (RICH) for the analysis of incomplete preference information in hierarchical weighting models. In RICH, the decision maker is allowed to specify subsets of attributes which contain the most important attribute or, more generally, to associate a set of rankings with a given set of attributes. Such preference statements lead to possibly non-convex sets of feasible attribute weights, allowing decision recommendations to be obtained through the computation of dominance relations and decision rules. An illustrative example on the selection of a subcontractor is presented, and the computational properties of RICH are considered", "keywords": ["multiple criteria analysis", "decision analysis", "hierarchical weighting models", "incomplete preference information"]}
{"id": "kp20k_training_131", "title": "Automatic relative orientation of large scale imagery over urban areas using Modified Iterated Hough Transform", "abstract": "The automation of relative orientation (RO) has been the major focus of the photogrammetric research community in the last decade. Despite the reported progress, there is no reliable (robust) approach that can perform automatic relative orientation (ARO) using large-scale imagery over urban areas. A reliable and general method for solving matching problems in various photogrammetric activities has been developed at The Ohio State University. This approach has been used to solve single photo resection using free-form linear features, surface matching and relative orientation. The approach estimates the parameters of a mathematical model relating the entities of two datasets when the correspondence of the involved entities is unknown. When applied to relative orientation, the coplanarity model is used to relate extracted edge pixels and/or feature points from a stereo-pair. In its execution, the relative orientation parameters are solved sequentially, using the coplanarity model to evaluate all possible pairings of the input primitives and choosing the most probable solution. As a result of this technique, the matched entities that correspond to the parameter solution are implicitly determined. Experiments using real data conclude that this is a robust method for relative orientation for both urban and rural scenes", "keywords": ["matching", "robust parameter estimation", "hough transform", "automatic relative orientation"]}
{"id": "kp20k_training_132", "title": "Emergency railway wagon scheduling by hybrid biogeography-based optimization", "abstract": "Railway transportation plays an important role in many disaster relief and other emergency supply chains. Based on the analysis of several recent disaster rescue operations in China, the paper proposes a mathematical model for emergency railway wagon scheduling, which considers multiple target stations requiring relief supplies, source stations for providing supplies, and central stations for allocating railway wagons. Under the emergency environment, the aim of the problem is to minimize the weighted time for delivering all the required supplies to the targets. For efficiently solving the problem, we develop a new hybrid biogeography-based optimization (BBO) algorithm, which uses a local ring topology of population to avoid premature convergence, includes the differential evolution (DE) mutation operator to perform effective exploration, and takes some problem-specific mechanisms for fine-tuning the search process and handling the constraints. Computational experiments show that our algorithm is robust and scalable, and outperforms some state-of-the-art heuristic algorithms on a set of problem instances", "keywords": ["emergency relief supply", "railway wagon scheduling", "biogeography-based optimization ", "ring topology", "differential evolution "]}
{"id": "kp20k_training_133", "title": "Biasvariance analysis in estimating true query model for information retrieval", "abstract": "We study the retrieval effectiveness-stability tradeoff in query model estimation. This tradeoff is investigated through a novel angle, i.e., biasvariance tradeoff. We formulate the performance biasvariance and estimation biasvariance. We investigate various query estimation methods using biasvariance analysis. Experiments have been conducted to verify hypotheses on biasvariance analysis", "keywords": ["information retrieval", "query language model", "biasvariance"]}
{"id": "kp20k_training_134", "title": "Qualitative constraint satisfaction problems: An extended framework with landmarks", "abstract": "Dealing with spatial and temporal knowledge is an indispensable part of almost all aspects of human activity. The qualitative approach to spatial and temporal reasoning, known as Qualitative Spatial and Temporal Reasoning (QSTR), typically represents spatial/temporal knowledge in terms of qualitative relations (e.g., to the east of, after), and reasons with spatial/temporal knowledge by solving qualitative constraints. When formulating qualitative constraint satisfaction problems (CSPs), it is usually assumed that each variable could be \"here, there and everywhere\".(1) Practical applications such as urban planning, however, often require a variable to take its value from a certain finite domain, i.e. it is required to be 'here or there, but not everywhere'. Entities in such a finite domain often act as reference objects and are called \"landmarks\" in this paper. The paper extends the classical framework of qualitative CSPs by allowing variables to take values from finite domains. The computational complexity of the consistency problem in this extended framework is examined for the five most important qualitative calculi, viz. Point Algebra, Interval Algebra, Cardinal Relation Algebra, RCC5, and RCC8. We show that all these consistency problems remain in NP and provide, under practical assumptions, efficient algorithms for solving basic constraints involving landmarks for all these calculi.  ", "keywords": ["qualitative spatial and temporal reasoning", "qualitative calculi", "constraint satisfaction", "landmarks"]}
{"id": "kp20k_training_135", "title": "the interaction of software prefetching with ilp processors in shared-memory systems", "abstract": "Current microprocessors aggressively exploit instruction-level parallelism (ILP) through techniques such as multiple issue, dynamic scheduling, and non-blocking reads. Recent work has shown that memory latency remains a significant performance bottleneck for shared-memory multiprocessor systems built of such processors.This paper provides the first study of the effectiveness of software-controlled non-binding prefetching in shared memory multiprocessors built of state-of-the-art ILP-based processors. We find that software prefetching results in significant reductions in execution time (12% to 31%) for three out of five applications on an ILP system. However, compared to previous-generation system, software prefetching is significantly less effective in reducing the memory stall component of execution time on an ILP system. Consequently, even after adding software prefetching, memory stall time accounts for over 30% of the total execution time in four out of five applications on our ILP system.This paper also investigates the interaction of software prefetching with memory consistency models on ILP-based multiprocessors. In particular, we seek to determine whether software prefetching can equalize the performance of sequential consistency (SC) and release consistency (RC). We find that even with software prefetching, for three out of five applications, RC provides a significant reduction in execution time (15% to 40%) compared to SC", "keywords": ["prefetching", "applications", "generation", "performance", "reduction", "art", "timing", "account", "instruction-level parallelism", "model", "paper", "sequential consistency", "component", "processor", "interaction", "shared memory", "software", "latency", "systems", "exploit", "shared memory multiprocessors", "memorialized", "binding", "consistency", "ilp", "effect", "dynamic scheduling"]}
{"id": "kp20k_training_136", "title": "A scalable and extensible framework for query answering over RDF", "abstract": "The Semantic Web is gaining increasing interest to fulfill the need of sharing, retrieving, and reusing information. In this context, the Resource Description Framework (RDF) has been conceived to provide an easy way to represent any kind of data and metadata, according to a lightweight model and syntaxes for serialization (RDF/XML, N3, etc.). Despite RDF has the advantage of being general and simple, it cannot be used as a storage model as it is, since it can be easily shown that even simple management operations involve serious performance limitations. In this paper we present a framework which provides a flexible and persistent layer relying on a novel storage model that guarantees good scalability and performance of query evaluation. The approach is based on the notion of construct, that represents a concept of the domain of interest. This makes the approach easily extensible and independent from the specific knowledge representation language. Based on this representation, reasoning capabilities are supported by a rule-based engine. Finally we present experimental results over real world scenarios to demonstrate the feasibility of the approach", "keywords": ["rdf", "rdfs", "query answering", "metamodel"]}
{"id": "kp20k_training_137", "title": "Using interactive 3-D visualization for public consultation", "abstract": "3-D models are often developed to aid the design and development of indoor and outdoor environments. This study explores the use of interactive 3-D visualization for public consultation for outdoor environments. Two visualization techniques (interactive 3-D visualization and static visualization) were compared using the method of individual testing. Visualization technique had no effect on the perception of the represented outdoor environment, but there was a preference for using interactive 3-D. Previously established mechanisms for a preference for interactive 3-D visualization in other domains were confirmed in the perceived strengths and weaknesses of visualization techniques. In focus-group discussion, major preferences included provision of more information through interactive 3-D visualization and wider access to information for public consultation. From a users' perspective, the findings confirm the strong potential of interactive 3-D visualization for public consultation.  ", "keywords": ["virtual reality", "visualization", "public consultation", "outdoor environment", "e-government"]}
{"id": "kp20k_training_138", "title": "Polymorphic nodal elements and their application in discontinuous Galerkin methods", "abstract": "In this work, we discuss two different but related aspects of the development of efficient discontinuous Galerkin methods on hybrid element grids for the computational modeling of gas dynamics in complex geometries or with adapted grids. In the first part, a recursive construction of different nodal sets for hp finite elements is presented. They share the property that the nodes along the sides of the two-dimensional elements and along the edges of the three-dimensional elements are the LegendreGaussLobatto points. The different nodal elements are evaluated by computing the Lebesgue constants of the corresponding Vandermonde matrix. In the second part, these nodal elements are applied within the modal discontinuous Galerkin framework. We still use a modal based formulation, but introduce a nodal based integration technique to reduce computational cost in the spirit of pseudospectral methods. We illustrate the performance of the scheme on several large scale applications and discuss its use in a recently developed space-time expansion discontinuous Galerkin scheme", "keywords": ["discontinuous galerkin", "nodal", "modal", "polynomial interpolation", "hp finite elements", "lebesgue constants", "quadrature free", "unstructured", "triangle", "quadrilateral", "polygonal", "tetrahedron", "hexahedron", "prism", "pentahedron", "pyramid"]}
{"id": "kp20k_training_139", "title": "Deployment-Based Solution for Prolonging Lifetime in Sensor Networks with Multiple Mobile Sinks", "abstract": "Enhancing sensor network lifetime is an important research topic for wireless sensor networks. Solutions based on linear programming, clustering, controlled non-uniform node distributions and mobility are presented separately in the literature. Even thought, the problem is still open and not fully solved. Drawbacks exist for all the above solutions when considered separately. Perhaps a solution that is able to provide composite benefits of some of them could better solve the problem. In this paper, we introduce a solution for prolonging the lifetime of sensor networks. The proposed solution is based on a deployment strategy of multiple mobile sinks. In our proposal, data traffic is directed away from the network center toward the network peripheral where sinks would be initially deployed. Sinks stay stationary while collecting the data reports that travel over the network perimeter toward them. Eventually perimeter nodes would be exposed to a peeling phenomenon which results in partitioning one or more sinks from their one-hop neighbors. The partitioned sinks move discrete steps following the direction of the progressive peeling towards the network center. The mechanism maintains the network connectivity and delays the occurrence of partition. Moreover, it balances the load among nodes and reduces the energy consumption. The performance of the proposed protocol is evaluated using intensive simulations. The results show the efficiency (in terms of both reliability and connectivity) of our deployment strategy with the associated data collection protocol", "keywords": ["sensor networks", "data collection", "mobile sinks", "deployment"]}
{"id": "kp20k_training_140", "title": "design and applications of an algorithm benchmark system in a computational problem solving environment", "abstract": "Benchmark tests are often used to evaluate the quality of products by a set of common criteria. In this paper we describe a computational problem solving environment based on open source codes and an algorithm benchmark system, which is embedded in the environment as a plug-in system. The algorithm benchmark system can be used to compare the performance of various algorithms or to evaluate the behavior of an algorithm with different input instances. The current implementation allows users to compare or evaluate algorithms written in C/C++. Some examples of the algorithm benchmark system that evaluates the memory utilization, time complexity and the output of algorithms are also presented. Algorithm benchmark impresses the learning effect; students can not only comprehend the performance of respective algorithms but also write their own programs to challenge the best known results", "keywords": ["problem-solving environment", "algorithm visualization", "benchmark", "knowledge portal"]}
{"id": "kp20k_training_141", "title": "automated performance tuning", "abstract": "This tutorial presents automated techniques for implementing and optimizing numeric and symbolic libraries on modern computing platforms including SSE, multicore, and GPU. Obtaining high performance requires effective use of the memory hierarchy, short vector instructions, and multiple cores. Highly tuned implementations are difficult to obtain and are platform dependent. For example, Intel Core i7 980 XE has a peak floating point performance of over 100 GFLOPS and the NVIDIA Tesla C870 has a peak floating point performance of over 500 GFLOPS, however, achieving close to peak performance on such platforms is extremely difficult. Consequently, automated techniques are now being used to tune and adapt high performance libraries such as ATLAS (math-atlas.sourceforge.net), PLASMA (icl.cs.utk.edu/plasma) and MAGMA (icl.cs.utk.edu/magma) for dense linear algebra, OSKI (bebop.cs.berkeley.edu/oski) for sparse linear algebra, FFTW (www.fftw.org) for the fast Fourier transform (FFT), and SPIRAL (www.spiral.net) for wide class of digital signal processing (DSP) algorithms. Intel currently uses SPIRAL to generate parts of their MKL and IPP libraries", "keywords": ["autotuning", "high-performance computing", "vectorization", "code generation and optimization", "parallelism"]}
{"id": "kp20k_training_142", "title": "Explicit solutions for a class indirect pharmacodynamic response models", "abstract": "Explicit solutions for four, ordinary differential equation (ODE)-based, types of indirect response models are presented. These response models were introduced by Dayneka et aL in 1993 [J. Pharmacokinet. Biopharm. 21 (1993) 457] to describe pharmacodynamic responses utilizing inhibitory or stimulatory Em,x type functions. The explicit solutions are expressed in terms of hypergeometric F-2(1) functions and their analytical continuations. A practical application is demonstrated for modeling the kinetics of drug action for ibandronate, a potent bisphosphonate that suppresses bone turnover resulting in a reduction in the markers of bone turnover. Ten times shorter model evaluation times, with the explicit solution compared with the differential equation implementation, may enhance situations where a large number of model evaluations are needed, such as clinical trial simulations and parameter estimation.  ", "keywords": ["indirect response model", "explicit solution", "hypergeometric function f-2", "nonmem"]}
{"id": "kp20k_training_143", "title": "a web-based consumer-oriented intelligent decision support system for personalized e-services", "abstract": "Due to the rapid advancement of electronic commerce and web technologies in recent years, the concepts and applications of decision support systems have been significantly extended. One quickly emerging research topic is the consumer-oriented decision support system that provides functional supports to consumers for efficiently and effectively making personalized decisions. In this paper we present an integrated framework for developing web-based consumer-oriented intelligent decision support systems to facilitate all phases of consumer decision-making process in business-to-consumer e-services applications. Major application functional modules comprised in the system framework include consumer and personalized management, navigation and search, evaluation and selection, planning and design, community and collaboration management, auction and negotiation, transactions and payments, quality and feedback control, as well as communications and information distributions. System design and implementation methods will be illustrated using an example. Also explored are various potential e-services application domains including e-tourism and e-investment", "keywords": ["personalization", "decision making process", "intelligent decision support system", "e-services"]}
{"id": "kp20k_training_144", "title": "Efficient segment-based video transcoding proxy for mobile multimedia services", "abstract": "To support various bandwidth requirements for mobile multimedia services for future heterogeneous mobile environments, such as portable notebooks, personal digital assistants (PDAs), and 3G cellular phones, a transcoding video proxy is usually necessary to provide mobile clients with adapted video streams by not only transcoding videos to meet different needs on demand, but also caching them for later use. Traditional proxy technology is not applicable to a video proxy because it is less cost-effective to cache the complete videos to fit all kinds of clients in the proxy. Since transcoded video objects have inheritance dependency between different bit-rate versions, we can use this property to amortize the retransmission overhead from transcoding other objects cached in the proxy. In this paper, we propose the object relation graph (ORG) to manage the static relationships between video versions and an efficient replacement algorithm to dynamically manage video segments cached in the proxy. Specifically, we formulate a transcoding time constrained profit function to evaluate the profit from caching each version of an object. The profit function considers not only the sum of the costs of caching individual versions of an object, but also the transcoding relationship among these versions. In addition, an effective data structure, cached object relation tree (CORT), is designed to facilitate the management of multiple versions of different objects cached in the transcoding proxy. Experimental results show that the proposed algorithm outperforms companion schemes in terms of the byte-hit ratios and the startup latency", "keywords": ["transcoding", "segment caching", "multimedia", "mobile network"]}
{"id": "kp20k_training_145", "title": "Automated process planning method to machine A B-Spline free-form feature on a mill-turn center", "abstract": "In this paper, we present a methodology for automating the process planning and NC code generation for a widely encountered class of free-form features that can be machined on a 3-axis mill-turn center. The free-form feature family that is considered is that of extruded protrusions whose cross-section is a closed, periodic B-Spline curve. in this methodology, for machining a part with B-Spline protrusion located at the free end, the part is first rough turned to the maximum profile diameter of the B-Spline, followed by rough profile cutting and finish profiling with axially mounted end mill tools. The identification and sequencing of machining volumes is completely automated, as is the generation of actual NC code. The approach supports both convex and non-convex profiles. In the case of non-convex profiles, the process planning algorithm ensures that there is no gouging of the work piece by the tool. The algorithm also identifies when sections of the tool path lie outside the work piece and utilizes rapid traverses in these regions to reduce cutting time. This methodology presents an integrated turn-mill process planning where by making the process fully automated from design with no user intervention making the overall process planning efficient. The algorithm was tested on several examples and test parts using the unmodified NC code obtained from the implementation were run on a Moriseiki mill-turn center. The parts that were produced met the dimensional specifications of the desired part.  ", "keywords": ["computer-aided process planning", "feature-based design", "computer-aided manufacturing"]}
{"id": "kp20k_training_146", "title": "Stability results for two classes of linear time-delay and hybrid systems", "abstract": "The stability of linear time-delay systems with point internal delays is difficult to deal with in practice because of the fact that their characteristic equation is usually of transcendent type rather than of polynomial type. This feature causes usually the system to possess an infinite number of poles. In this paper, stability tests for this class of systems are obtained either based on extensions of classical tests applicable to delay-free systems or on approaches within the framework of two-dimensional digital filters. Some of those two-dimensional stability tests are also proved to be useful for stability testing of a common class of linear hybrid systems which involve coupled continuous and digital substates after a slight \"ad-hoc\" adaptation of the tests for that situation", "keywords": ["stability ", "man-machine systems", "time series analysis"]}
{"id": "kp20k_training_147", "title": "A pseudo-nearest-neighbor approach for missing data recovery on Gaussian random data sets", "abstract": "Missing data handling is an important preparation step for most data discrimination or mining tasks. Inappropriate treatment of missing data may cause large errors or false results. In this paper, we study the effect of a missing data recovery method, namely the pseudo-nearest-neighbor substitution approach, on Gaussian distributed data sets that represent typical cases in data discrimination and data mining applications. The error rate of the proposed recovery method is evaluated by comparing the clustering results of the recovered data sets to the clustering results obtained on the originally complete data sets. The results are also compared with that obtained by applying two other missing data handling methods, the constant default value substitution and the missing data ignorance (non-substitution) methods. The experiment results provided a valuable insight to the improvement of the accuracy for data discrimination and knowledge discovery on large data sets containing missing values", "keywords": ["missing data", "missing data recovery", "data imputation", "data clustering", "gaussian data distribution", "data mining"]}
{"id": "kp20k_training_148", "title": "A Lagrangian relaxation approach to the edge-weighted clique problem", "abstract": "The b-clique polytope CPnb is the convex hull of the node and edge incidence vectors of all subcliques of size at most b of a complete graph on n nodes. Including the Boolean quadric polytope QPn=CPnn as a special case and being closely related to the quadratic knapsack polytope, it has received considerable attention in the literature. In particular, the max-cut problem is equivalent with optimizing a linear function over CPnn. The problem of optimizing linear functions over CPnb has so far been approached via heuristic combinatorial algorithms and cutting-plane methods. We study the structure of CPnb in further detail and present a new computational approach to the linear optimization problem based on the idea of integrating cutting planes into a Lagrangian relaxation of an integer programming problem that Balas and Christofides had suggested for the traveling salesman problem. In particular, we show that the separation problem for tree inequalities becomes polynomial in our Lagrangian framework. Finally, computational results are presented", "keywords": ["mathematical programming", "clique polytope", "cut polytope", "cutting plane", "boolean quadric polytope", "quadratic knapsack polytope", "lagrangian relaxation"]}
{"id": "kp20k_training_149", "title": "resource aware programming in the pixie os", "abstract": "This paper presents Pixie, a new sensor node operating system designed to support the needs of data-intensive applications. These applications, which include high-resolution monitoring of acoustic, seismic, acceleration, and other signals, involve high data rates and extensive in-network processing. Given the fundamentally resource-limited nature of sensor networks, a pressing concern for such applications is their ability to receive feedback on, and adapt their behavior to, fluctuations in both resource availability and load. The Pixie OS is based on a dataflow programming model based on the concept of resource tickets, a core abstraction for representing resource availability and reservations. By giving the system visibility and fine-grained control over resource management, a broad range of policies can be implemented. To shield application programmers from the burden of managing these details, Pixie provides a suite of resource brokers, which mediate between low-level physical resources and higher-level application demands. Pixie is implemented in NesC and supports limited backwards compatibility with TinyOS. We describe Pixie in the context of two applications: limb motion analysis for patients undergoing treatment for motion disorders, and acoustic target detection using a network of microphones. We present a range of experiments demonstrating Pixie's ability to accurately account for resource availability at runtime and enable a range of both generic and application-specific adaptations", "keywords": ["network", "sensor", "motion analysis", "applications", "policy", "signaling", "context", "acceleration", "sensor networks", "concept", "experience", "account", "paper", "resource reservations", "resource-aware programming", "runtime", "motion", "control", "program modelling", "management", "visibility", "program", "wireless sensor networks", "availability", "detection", "systems", "abstraction", "behavior", "operating system", "data", "process", "support", "physical", "feedback", "core", "monitor", "compatibility", "resource management", "dataflow", "resource", "programmer", "generic", "tinyos"]}
{"id": "kp20k_training_150", "title": "Highly Undersampled Magnetic Resonance Image Reconstruction via Homotopic l(0)-Minimization", "abstract": "In clinical magnetic resonance imaging (MRI), any reduction in scan time offers a number of potential benefits ranging from high-temporal-rate observation of physiological processes to improvements in patient comfort. Following recent developments in compressive sensing (CS) theory, several authors have demonstrated that certain classes of MR images which possess sparse representations in some transform domain can be accurately reconstructed from very highly undersampled K-space data by solving a convex l(1)-minimization problem. Although l(1)-based techniques are extremely powerful, they inherently require a degree of oversampling above the theoretical minimum sampling rate to guarantee that exact reconstruction can be achieved. In this paper, we propose a generalization of the CS paradigm based on homotopic approximation of the l(0) quasi-norm and show how MR image reconstruction can be pushed even further below the Nyquist limit and significantly closer to the theoretical bound. Following a brief review of standard CS methods and the developed theoretical extensions, several example MRI reconstructions from highly undersampled K-space data are presented", "keywords": ["compressed sensing", "compressive sensing ", "image reconstruction", "magnetic resonance imaging ", "nonconvex optimization"]}
{"id": "kp20k_training_151", "title": "An incremental verification algorithm for real-time systems", "abstract": "We present an incremental algorithm for model checking the red-time systems against the requirements specified in the real-time extension of modal mu-calculus. Using this algorithm, we avoid the repeated construction and analysis of the whole state-space during the course of evolution of the system from time to time. We use a finite representation of the system, like most other algorithms on real-time systems. We construct and update a graph (called TSG) that is derived from the region graph and the formula. This allows us to halt the construction of this graph when enough nodes have been explored to determine the truth of the formula. TSG is minimal in the sense of partitioning the infinite state space into regions and it expresses a relation on the set of regions of the partition. We use the structure of the formula to derive this partition. When a change is applied to the timed automaton of the system, we find a new partition from the current partition and the TSG with minimum cost", "keywords": ["model-checking", "timed mu-calculus", "timed automata", "requirements specification", "labeled transition systems"]}
{"id": "kp20k_training_152", "title": "A Survey on Transport Protocols for Wireless Multimedia Sensor Networks", "abstract": "Wireless networks composed of multimedia-enabled resource-constrained sensor nodes have enriched a large set of monitoring sensing applications. In such communication scenario, however, new challenges in data transmission and energy-efficiency have arisen due to the stringent requirements of those sensor networks. Generally, congested nodes may deplete the energy of the active congested paths toward the sink and incur in undesired communication delay and packet dropping, while bit errors during transmission may negatively impact the end-to-end quality of the received data. Many approaches have been proposed to face congestion and provide reliable communications in wireless sensor networks, usually employing some transport protocol that address one or both of these issues. Nevertheless, due to the unique characteristics of multimedia-based wireless sensor networks, notably minimum bandwidth demand, bounded delay and reduced energy consumption requirement, communication protocols from traditional scalar wireless sensor networks are not suitable for multimedia sensor networks. In the last decade, such requirements have fostered research in adapting existing protocols or proposing new protocols from scratch. We survey the state of the art of transport protocols for wireless multimedia sensor networks, addressing the recent developments and proposed strategies for congestion control and loss recovery. Future research directions are also discussed, outlining the remaining challenges and promising investigation areas", "keywords": ["wireless multimedia sensor networks", "transport protocols", "congestion control", "loss recovery", "survey"]}
{"id": "kp20k_training_153", "title": "Two integrable couplings of the Tu hierarchy and their Hamiltonian structures", "abstract": "The double integrable couplings of the Tu hierarchy are worked out by use of Vector loop algebras G  6 and G  9 respectively. Also the Hamiltonian structures of the obtained system are given by the quadratic-form identity", "keywords": ["tu hierarchy", "vector loop algebra", "integrable couplings", "quadratic-form identity", "hamiltonian structure"]}
{"id": "kp20k_training_154", "title": "Dynamic simulation of bioreactor systems using orthogonal collocation on finite elements", "abstract": "The dynamics of continuous biological processes is addressed in this paper. Numerical simulation of a conventional activated sludge process shows that despite the large differences in the dynamics of the species investigated. the orthogonal collocation on finite element technique with three internal collocation and four elements (OCFE-34) gives excellent numerical results for bioreactor models up to a Peclet number of 50. It is shown that there is little improvement in numerical accuracy when a much larger internal collocation point is introduced. Over and above Peclet number of 50, considered to be large for this process. simulation with the global orthogonal collocation (GOC) technique is infeasible. Due to the banded nature of its structural matrix, the method of lines (MOL) technique requires the lowest computing time, typically four times less than that required by the OCFE-34. Validation of the hydraulics of an existing pilot-scale subsurface flow (SSF) constructed wetland process using the aforementioned numerical techniques suggested that the OCFE is superior to the MOL and GOC in terms of numerical stability,  ", "keywords": ["activated sludge", "orthogonal collocation on finite element", "global orthogonal collocation", "method of lines", "peclet number", "ssf constructed wetland"]}
{"id": "kp20k_training_155", "title": "Detecting regularities on grammar-compressed strings", "abstract": "We address the problems of detecting and counting various forms of regularities in a string represented as a straight-line program (SLP) which is essentially a context free grammar in the Chomsky normal form. Given an SLP of size n that represents a string s of length N, our algorithm computes all runs and squares in s  in O(n3h) O ( n 3 h ) time and O(n2) O ( n 2 ) space, where h  is the height of the derivation tree of the SLP. We also show an algorithm to compute all gapped-palindromes in O(n3h+gnhlog?N) O ( n 3 h + g n h log ? N ) time and O(n2) O ( n 2 ) space, where g  is the length of the gap. As one of the main components of the above solution, we propose a new technique called approximate doubling which seems to be a useful tool for a wide range of algorithms on SLPs. Indeed, we show that the technique can be used to compute the periods and covers of the string in O(n2h) O ( n 2 h ) time and O(nh(n+log2?N)) O ( n h ( n + log 2 ? N ) ) time, respectively", "keywords": ["straight-line programs ", "runs", "squares", "gapped palindromes", "compressed string processing algorithms"]}
{"id": "kp20k_training_156", "title": "Achieving reusability and composability with a simulation conceptual model", "abstract": "Reusability and composability (R&C) are two important quality characteristics that have been very difficult to achieve in the Modelling and Simulation (M&S) discipline. Reuse provides many technical and economical benefits. Composability has been increasingly crucial for M&S of a system of systems, in which disparate systems are composed with each other. The purpose of this paper is to describe how R&C can be achieved by using a simulation conceptual model (CM) in a community of interest (COI). We address R&C in a multifaceted manner covering many M&S areas (types). M&S is commonly employed where R&C are very much needed by many COIs. We present how a CM developed for a COI can assist in R&C for the design of any type of large-scale complex M&S application in that COI. A CM becomes an asset for a COI and offers significant economic benefits through its broader applicability and more effective utilization", "keywords": ["composability", "conceptual model", "reusability", "simulation", "simulation model development"]}
{"id": "kp20k_training_157", "title": "Wavelength decomposition approach for computing blocking probabilities in multicast WDM optical networks", "abstract": "We present an approximate analytical method to evaluate the blocking probabilities in multicast Wavelength Division Multiplexing (WDM) networks without wavelength converters. Our approach is based on the wavelength decomposition approach in which the WDM network is divided into layers (colors) and the moment matching method is used to characterize the overflow traffic from one layer to another. Analyzing blocking probabilities for unicast and multicast calls in each layer of the network is derived from an exact approach. We assume static routing with either First-Fit or random wavelength assignment algorithm. Results are presented which indicate the accuracy of our method", "keywords": ["blocking probability", "mutlicast routing", "wdm"]}
{"id": "kp20k_training_158", "title": "A new local meshless method for steady-state heat conduction in heterogeneous materials", "abstract": "In this paper a truly meshless method based on the integral form of energy equation is presented to study the steady-state heat conduction in the anisotropic and heterogeneous materials. The presented meshless method is based on the satisfaction of the integral form of energy balance equation for each sub-particle (sub-domain) inside the material. Moving least square (MLS) approximation is used for approximation of the field variable over the randomly located nodes inside the domain. In the absence of heat generation, the domain integration is eliminated from the formulation of presented method and the computational efforts are reduced substantially with respect to the conventional MLPG method. A direct method is presented for treatment of material discontinuity at the heterogeneous material in the presented meshless method. As a practical problem the heat conduction in fibrous composite material is studied and the steady-state heat conduction in unidirectional fibermatrix composites is investigated. The solution domain includes a small area of the composite system called representative volume element (RVE). Comparison of numerical results shows that the presented meshless method is simple, effective, accurate and less costly method for micromechanical analysis of heat conduction in heterogeneous materials", "keywords": ["truly meshless method", "heat conduction problem", "heterogeneous material", "micromechanical analysis", "fiber reinforced composite"]}
{"id": "kp20k_training_159", "title": "A Territory Defining Multiobjective Evolutionary Algorithms and Preference Incorporation", "abstract": "We have developed a steady-state elitist evolutionary algorithm to approximate the Pareto-optimal frontiers of multiobjective decision making problems. The algorithms define a territory around each individual to prevent crowding in any region. This maintains diversity while facilitating the fast execution of the algorithm. We conducted extensive experiments on a variety of test problems and demonstrated that our algorithm performs well against the leading multiobjective evolutionary algorithms. We also developed a mechanism to incorporate preference information in order to focus on the regions that are appealing to the decision maker. Our experiments show that the algorithm approximates the Pareto-optimal solutions in the desired region very well when we incorporate the preference information", "keywords": ["crowding prevention", "evolutionary algorithms", "guidance", "multiobjective optimization", "preference incorporation"]}
{"id": "kp20k_training_160", "title": "A holistic frame-of-reference for modelling social systems", "abstract": "Purpose - To outline a philosophical system of inquiry that may be used as a frame-of-reference for modelling social systems. Design/methodology/approach - The paper draws on insights from cognitive science, autopoiesis, management cybernetics and non-linear dynamics. Findings - The outcome of this paper is an outline of a frame-of-reference to be used as a starting point (or a frame of orientation) for any problem solving/modelling intent or act. The framework highlights the importance of epistemological reflection and the need to avoid any separation of the process of knowing from that of modelling. It also emphasises the importance of inquiry into the assumptions that underpin the methods, tools and techniques that we employ, and into the tacit beliefs of the human actors who use them. Research limitations/implications - The presented frame-of-reference should be regarded as an evolving system of inquiry, one that seeks to incorporate contemporary human insight. Practical implications - Exactly, how the frame-of-reference presented in this paper should be exploited within an organisational or educational context, is a question to which there is no single \"correct\" answer. What is primarily important, however, is that it should be used to raise the profile of, and disseminate the benefits that accrue from, inquiry which goes beyond the simple application of tools and methods. Originality/value - This paper proposes a new framework-of-reference for modelling social systems that draws on insights from cognitive science, autopoiesis, management cybernetics and non-linear dynamics", "keywords": ["cybernetics", "modelling", "social dynamics"]}
{"id": "kp20k_training_161", "title": "A source-synchronous double-data-rate parallel optical transceiver IC", "abstract": "Source-synchronous double-data-rate (DDR) signaling is widely used in electrical interconnects to eliminate clock recovery and to double communication bandwidth. This paper describes the design of a parallel optical transceiver integrated circuit (IC) that uses source-synchronous DDR optical signaling. On the transmit side, two 8-b electrical inputs are multiplexed, encoded, and sent over two high-speed optical links. On the receive side, the procedure is reversed to produce two 8-b electrical outputs. The proposed IC integrates analog vertical-cavity surface-emitting lasers (VCSELs), drivers and optical receivers with digital DDR multiplexing, serialization, and deserialization circuits. It was fabricated in a 0.5-mu m silicon-on-sapphire (SOS) complementary metal-oxide-semiconductor (CMOS) process. Linear arrays of quad VCSELs and photodetectors were attached to the proposed transceiver IC using Hip-chip bonding. A free-space optical link system was constructed to demonstrate correct IC functionality. The test results show successful transceiver operation at a data rate of 500 Mb/s with a 250-MHz DDR clock, achieving a gigabit of aggregate bandwidth. While the proposed DDR scheme is well suited for low-skew fiber-ribbon, free-space, and waveguide optical links, it can also be extended to links with higher skew with the addition of skew-compensation circuitry. To the authors' knowledge, this is the first demonstration of parallel optical transceivers that use source-synchronous DDR signaling", "keywords": ["flip-chip", "high-speed-interconnect", "optical interconnects", "optoelectronic-integrated circuits", "source-synchronous signaling"]}
{"id": "kp20k_training_162", "title": "FPCODE: AN EFFICIENT APPROACH FOR MULTI-MODAL BIOMETRICS", "abstract": "Although face recognition technology has progressed substantially, its performance is still not satisfactory due to the challenges of great variations in illumination, expression and occlusion. This paper aims to improve the accuracy of personal identification, when only few samples are registered as templates, by integrating multiple modal biometrics, i.e. face and palmprint. We developed in this paper a feature code, namely FPCode, to represent the features of both face and palmprint. Though feature code has been used for palmprint recognition in literature, it is first applied in this paper for face recognition and multi-modal biometrics. As the same feature is used, fusion is much easier. Experimental results show that both feature level and decision level fusion strategies achieve much better performance than single modal biometrics. The proposed approach uses fixed length 1/0 bits coding scheme that is very efficient in matching, and at the same time achieves higher accuracy than other fusion methods available in literature", "keywords": ["face recognition", "palmprint recognition", "gabor feature", "fusion code", "feature fusion"]}
{"id": "kp20k_training_163", "title": "Kinetics and energetics during uphill and downhill carrying of different weights", "abstract": "During physically heavy work tasks the musculoskeletal tissues are exposed to both mechanical and metabolic loading. The aim of the present study was to test a biomechanical model for prediction of whole-body energy turnover from kinematic and anthropometric data during load carrying. Total loads of 0, 10 and 20kg were carried symmetrically or asymmetrically in the hands, while walking on a treadmill (4.5kmh?1) horizontally, uphill, or downhill the slopes being 8%. Mean values for the directly measured oxygen uptake ranged for all trials from 0.5 to 2.1 l O2min?1, and analysis of variance showed significant differences regarding slope, load carried, and symmetry. The calculated values of oxygen uptake based on the biomechanical model correlated significantly with the directly measured values, fitting to the line Y=0.990X+0.144 , where Y is the estimated and X is the measured oxygen uptake in lmin?1. The close relationship between energy turnover rate measured directly and estimated based on a biomechanical model justifies the assessment of the metabolic load from kinematic data", "keywords": ["biomechanics", "manual material handling"]}
{"id": "kp20k_training_164", "title": "Granular prototyping in fuzzy clustering", "abstract": "We introduce a logic-driven clustering in which prototypes are formed and evaluated in a sequential manner. The way of revealing a structure in data is realized by maximizing a certain performance index (objective function) that takes into consideration an overall level of matching (to be maximized) and a similarity level between the prototypes (the component to be minimized). The prototypes identified in the process come with the optimal weight vector that serves to indicate the significance of the individual features (coordinates) in the data grouping represented by the prototype. Since the topologies of these groupings are in general quite diverse the optimal weight vectors are reflecting the anisotropy of the feature space, i.e., they show some local ranking of features in the data space. Having found the prototypes we consider an inverse similarity problem and show how the relevance of the prototypes translates into their granularity", "keywords": ["direct and inverse matching problem", "granular prototypes", "information granulation", "logic-based clustering", "similarity index", "t- and s-norms"]}
{"id": "kp20k_training_165", "title": "empirical evaluation of latency-sensitive application performance in the cloud", "abstract": "Cloud computing platforms enable users to rent computing and storage resources on-demand to run their networked applications and employ virtualization to multiplex virtual servers belonging to different customers on a shared set of servers. In this paper, we empirically evaluate the efficacy of cloud platforms for running latency-sensitive multimedia applications. Since multiple virtual machines running disparate applications from independent users may share a physical server, our study focuses on whether dynamically varying background load from such applications can interfere with the performance seen by latency-sensitive tasks. We first conduct a series of experiments on Amazon's EC2 system to quantify the CPU, disk, and network jitter and throughput fluctuations seen over a period of several days. We then turn to a laboratory-based cloud and systematically introduce different levels of background load and study the ability to isolate applications under different settings of the underlying resource control mechanisms. We use a combination of micro-benchmarks and two real-world applications--the Doom 3 game server and Apple's Darwin Streaming Server--for our experimental evaluation. Our results reveal that the jitter and the throughput seen by a latency-sensitive application can indeed degrade due to background load from other virtual machines. The degree of interference varies from resource to resource and is the most pronounced for disk-bound latency-sensitive tasks, which can degrade by nearly 75% under sustained background load. We also find that careful configuration of the resource control mechanisms within the virtualization layer can mitigate, but not eliminate, this interference", "keywords": ["cloud computing", "virtualization", "multimedia", "resource isolation"]}
{"id": "kp20k_training_166", "title": "The role of ChineseAmerican scientists in ChinaUS scientific collaboration: a study in nanotechnology", "abstract": "In this paper, we use bibliometric methods and social network analysis to analyze the pattern of ChinaUS scientific collaboration on individual level in nanotechnology. Results show that ChineseAmerican scientists have been playing an important role in ChinaUS scientific collaboration. We find that ChinaUS collaboration in nanotechnology mainly occurs between Chinese and ChineseAmerican scientists. In the co-authorship network, ChineseAmerican scientists tend to have higher betweenness centrality. Moreover, the series of polices implemented by the Chinese government to recruit oversea experts seems to contribute a lot to ChinaUS scientific collaboration", "keywords": ["scientific collaboration", "chineseamerican", "nanotechnology", "collaboration network"]}
{"id": "kp20k_training_167", "title": "Localization of spherical fruits for robotic harvesting", "abstract": "The orange picking robot (OPR) is a project for developing a robot that is able to harvest oranges automatically. One of the key tasks in this robotic application is to identify the fruit and to measure its location in three dimensions. This should be performed using image processing techniques which must be sufficiently robust to cope with variations in lighting conditions and a changing environment. This paper describes the image processing system developed so far to guide automatic harvesting of oranges, which here has been integrated in the first complete full-scale prototype OPR", "keywords": ["fruit harvesting", "color clustering", "stereo matching", "visual tracking"]}
{"id": "kp20k_training_168", "title": "game based learning for computer science education", "abstract": "Today, learners increasingly demand for innovative and motivating learning scenarios that strongly respond to their habits of using media. One of the many possible solutions to this demand is the use of computer games to support the acquisition of knowledge. This paper reports on chances and challenges of applying a game-based learning scenario for the acquisition of IT knowledge as realized by the German BMBF project SpITKom. After briefly describing the learning potential of Multiplayer Browser Games as well as the educational objectives and target group of the SpITKom project, we will present the main results of a study that was carried out in the first phase of the project to guide the game design. In the course of the study, data were collected regarding (a) the computer game preferences of the target group and (b) the target group's competencies in playing computer games. We will then introduce recommendations that were deduced from the study's findings and that outline the concept and the prototype of the game", "keywords": ["game design", "game based learning", "it knowledge", "learners difficult to reach"]}
{"id": "kp20k_training_169", "title": "Efficient evaluation functions for evolving coordination", "abstract": "This paper presents fitness evaluation functions that efficiently evolve coordination in large multi-component systems. In particular, we focus on evolving distributed control policies that are applicable to dynamic and stochastic environments. While it is appealing to evolve such policies directly for an entire system, the search space is prohibitively large in most cases to allow such an approach to provide satisfactory results. Instead, we present an approach based on evolving system components individually where each component aims to maximize its own fitness function. Though this approach sidesteps the exploding state space concern, it introduces two new issues: (1) how to create component evaluation functions that are aligned with the global evaluation function; and (2) how to create component evaluation functions that are sensitive to the fitness changes of that component, while remaining relatively insensitive to the fitness changes of other components in the system. If the first issue is not addressed, the resulting system becomes uncoordinated; if the second issue is not addressed, the evolutionary process becomes either slow to converge or worse, incapable of converging to good solutions. This paper shows how to construct evaluation functions that promote coordination by satisfying these two properties. We apply these evaluation functions to the distributed control problem of coordinating multiple rovers to maximize aggregate information collected. We focus on environments that are highly dynamic (changing points of interest), noisy (sensor and actuator faults), and communication limited (both for observation of other rovers and points of interest) forcing the rovers to evolve generalized solutions. On this difficult coordination problem, the control policy evolved using aligned and component-sensitive evaluation functions outperforms global evaluation functions by up to 400%. More notably, the performance improvements increase when the problems become more difficult (larger, noisier, less communication). In addition we provide an analysis of the results by quantifying the two characteristics (alignment and sensitivity discussed above) leading to a systematic study of the presented fitness functions", "keywords": ["evolution strategies", "distributed control", "fitness evaluation"]}
{"id": "kp20k_training_170", "title": "Contention-free communication scheduling for array redistribution", "abstract": "Array redistribution is required often in programs on distributed memory parallel computers. It is essential to use efficient algorithms for redistribution, otherwise the performance of the programs may degrade considerably. The redistribution overheads consist of two parts: index computation and interprocessor communication. If there is no communication scheduling in a redistribution algorithm, the communication contention may occur, which increases the communication waiting time. In order to solve this problem, in this paper, we propose a technique to schedule the communication so that it becomes contention-free. Our approach initially generates a communication table to represent the communication relations among sending nodes and receiving nodes. According to the communication table, we then generate another table named communication scheduling table. Each column of communication scheduling table is a permutation of receiving node numbers in each communication step. Thus the communications in our redistribution algorithm are contention-free. Our approach can deal with multi-dimensional shape changing redistribution", "keywords": ["parallelizing compilers", "hpf", "array redistribution", "communication scheduling", "distributed memory machines"]}
{"id": "kp20k_training_171", "title": "Quadratic weighted median filters for edge enhancement of noisy images", "abstract": "Quadratic Volterra filters are effective in image sharpening applications. The linear combination of polynomial terms, however, yields poor performance in noisy environments. Weighted median (WM) filters, in contrast, are well known for their outlier suppression and detail preservation properties. The WM sample selection methodology is naturally extended to the quadratic sample case, yielding a filter structure referred to as quadratic weighted median (QWM) that exploits the higher order statistics of the observed samples while simultaneously being robust to outliers arising in the higher order statistics of environment noise. Through statistical analysis of higher order samples, it is shown that, although the parent Gaussian distribution is light tailed, the higher order terms exhibit heavy-tailed distributions. The optimal combination of terms contributing to a quadratic system, i.e., cross and square, is approached from a maximum likelihood perspective which yields the WM processing of these terms. The proposed QWM filter structure is analyzed through determination of the output variance and breakdown probability. The studies show that the QWM exhibits lower variance and breakdown probability indicating the robustness of the proposed structure. The performance of the QWM filter is tested on constant regions, edges and real images, and compared to its weighted-sum dual, the quadratic Volterra filter. The simulation results show that the proposed method simultaneously suppresses the noise and enhances image details. Compared with the quadratic Volterra sharpener, the QWM filter exhibits superior qualitative and quantitative performance in noisy image sharpening", "keywords": ["asymptotic tail mass", "maximum likelihood estimation", "robust image sharpening", "unsharp masking", "volterra filtering", "weighted median  filtering"]}
{"id": "kp20k_training_172", "title": "ELRA - European language resources association-background, recent developments and future perspectives", "abstract": "The European Language Resources Association (ELRA) was founded in 1995 with the mission of providing language resources (LR) to European research institutions and companies. In this paper we describe the background, the mission and the major activities since then", "keywords": ["evaluation", "language resources", "production", "standards", "validation"]}
{"id": "kp20k_training_173", "title": "MULTIPLE CONCURRENCE OF MULTI-PARTITE QUANTUM SYSTEM", "abstract": "We propose a new way of description of the global entanglement property of a multi-partite pure state quantum system. Based on the idea of bipartite concurrence, by dividing the multi-partite quantum system into two subsystems, a combination of all the bipartite concurrences of a multipartite quantum system is used to describe the entanglement property of the multi-partite system. We derive the analytical results for GHZ-state, W-state with arbitrary number of qubits, and cluster state with the number of particles no greater than 6", "keywords": ["multiple concurrence of multi-partite quantum system", "entanglement", "ghz-state", "w-state", "cluster state"]}
{"id": "kp20k_training_174", "title": "Tolerant information retrieval with backpropagation networks", "abstract": "Neural networks can learn fi-om human decisions and preferences. Especially in, human-computer interaction, adaptation to the behaviour and expectations of the user is necessary. Ih information retrieval, an important area within human-computer interaction, expectations are difficult to meet. The inherently vague nature of information retrieval has bed to the application of vague processing techniques. Neural networks seem to have great potential to model the cognitive processes involved more appropriately. Current models based on neural networks and their implications for human-computer interaction ar-e analysed. COSIMIR (Cognitive Similarity Learning in Information Retrieval), an innovative model integrating human knowledge into the core of the retrieval process, is presented. It applies backpropagation to information retrieval, integrating human-centred and soft and tolerant computing into the core of the retrieval process. A further backpropagation model, the transformation network for heterogeneous data sources, is discussed. Empirical evaluations have provided promising results", "keywords": ["backpropagation", "human-computer interaction", "information retrieval", "neural networks", "similarity", "spreading activation"]}
{"id": "kp20k_training_175", "title": "Approximation of mean time between failure when a system has periodic maintenance", "abstract": "This paper describes a simple technique for estimating the mean time between failure (MTBF) of a system that has periodic maintenance at regular intervals. This type of maintenance is typically found in high reliability, mission-oriented applications where it is convenient to perform maintenance after the completion of the mission. This approximation technique can greatly simplify the MTBF analysis for large systems. The motivation for this analysis was to understand the nature of the error in the approximation and to develop a means for quantifying that error. This paper provides the derivation of the equations that bound the error that can result when using this approximation method. It shows that, for most applications, the MTBF calculations can be greatly simplified with only a very small sacrifice in accuracy", "keywords": ["mean time between failure ", "periodic maintenance", "reliability modeling"]}
{"id": "kp20k_training_176", "title": "Supplying Web 2.0: An empirical investigation of the drivers of consumer transmutation of culture-oriented digital information goods", "abstract": "This paper describes an empirical study of behaviors associated with consumers' creative modification of digital information goods found in Web 2.0 and elsewhere. They are products of culture such as digital images, music, video, news and computer games. We will refer to them as \"digital culture products\". How do consumers who transmute such products differ from those who do not, and from each other? This study develops and tests a theory of consumer behavior in transmuting digital culture products, separating consumers into different groups based on how and why they transmute. With our theory, we posit these groups as having differences of motivation, as measured by product involvement and innovativeness, and of ability as measured by computer skills. A survey instrument to collect data from Internet-capable computer users on the relevant constructs, and on their transmutation activities, is developed and distributed using a web-based survey hosting service. The data are used to test hypotheses that consumers' enduring involvement and innovativeness are positively related to transmutation behaviors, and that computer self-efficacy moderates those relationships. The empirical results support the hypotheses that enduring involvement and innovativeness do motivate transmutation behavior. The data analysis also supports the existence of a moderating relationship of computer self-efficacy with respect to enduring involvement, but not of computer self-efficacy with respect to innovativeness. The findings further indicate that transmutation activities should be expected to impact Web 2.0-oriented companies, both incumbents and start-ups, as they make decisions about how to incorporate consumers into their business models not only as recipients of content, but also as its producers", "keywords": ["information goods", "culture products", "digital entertainment", "creativity", "digital mashup", "remix", "media products", "consumer behavior"]}
{"id": "kp20k_training_177", "title": "Polynomial cost for solving IVP for high-index DAE", "abstract": "We show that the cost of solving initial value problems for high-index differential algebraic equations is polynomial in the number of digits of accuracy requested. The algorithm analyzed is built on a Taylor series method developed by Pryce for solving a general class of differential algebraic equations. The problem may be fully implicit, of arbitrarily high fixed index and contain derivatives of any order. We give estimates of the residual which are needed to design practical error control algorithms for differential algebraic equations. We show that adaptive meshes are always more efficient than non-adaptive meshes. Finally, we construct sufficiently smooth interpolants of the discrete solution", "keywords": ["differential algebraic equations", "initial value problem", "adaptive step-size control", "taylor series", "structural analysis", "automatic differentiation", "holder mean"]}
{"id": "kp20k_training_178", "title": "A Novel Wavelength Hopping Passive Optical Network (WH-PON) for Provision of Enhanced Physical Security", "abstract": "A novel secure wavelength hopping passive optical network (WH-PON) is presented in which physical layer security is introduced to the access network. The WH-PON design uses a pair of matched tunable lasers in the optical line terminal to create a time division multiplexed signal in which each data frame is transmitted at a unique wavelength. The transmission results for a 32-channel WH-PON operating at a data rate of 2.5 Gb/s are presented in this paper. The inherent security of the WH-PON design is verified through an attempted cross-channel eavesdropping attempt at an optical network unit. The results presented verify that the WH-PON provides secure broadband service in the access network", "keywords": ["access network", "broadband", "fiber-to-the-x", "passive optical network", "tunable laser", "wavelength hopping"]}
{"id": "kp20k_training_179", "title": "On the Information Flow Required for Tracking Control in Networks of Mobile Sensing Agents", "abstract": "We design controllers that permit mobile agents with distributed or networked sensing capabilities to track (follow) desired trajectories, identify what trajectory information must be distributed to each agent for tracking, and develop methods to minimize the communication needed for the trajectory information distribution", "keywords": ["cooperative control", "dynamical networks", "tracking"]}
{"id": "kp20k_training_180", "title": "Analysis of timing-based mutual exclusion with random times", "abstract": "Various timing-based mutual exclusion algorithms have been proposed that guarantee mutual exclusion if certain timing assumptions hold. In this paper, we examine how these algorithms behave when the time for the basic operations is governed by probability distributions. In particular, we are concerned with how often such algorithms succeed in allowing a processor to obtain a critical region and how this success rate depends on the random variables involved. We explore this question in the case where operation times are governed by exponential and gamma distributions, using both theoretical analysis and simulations", "keywords": ["mutual exclusion", "timed mutual exclusion", "markov chains", "locks"]}
{"id": "kp20k_training_181", "title": "Modeling virtual worlds in databases", "abstract": "A method of modeling virtual worlds in databases is presented. The virtual world model is conceptually divided into several distinct elements, which are separately represented in a database. The model pen-nits to dynamically generate virtual scenes. ", "keywords": ["databases", "data structures", "modeling", "virtual reality"]}
{"id": "kp20k_training_182", "title": "An efficient scheduling algorithm for scalable video streaming over P2P networks", "abstract": "During recent years, the Internet has witnessed rapid advancement in peer-to-peer (P2P) media streaming. In these applications, an important issue has been the block scheduling problem, which deals with how each node requests the media data blocks from its neighbors. In most streaming systems, peers are likely to have heterogeneous upload/download bandwidths, leading to the fact that different peers probably perceive different streaming quality. Layered (or scalable) streaming in P2P networks has recently been proposed to address the heterogeneity of the network environment. In this paper, we propose a novel block scheduling scheme that is aimed to address the P2P layered video streaming. We define a soft priority function for each block to be requested by a node in accordance with the blocks significance for video playback. The priority function is unique in that it strikes good balance between different factors, which makes the priority of a block well represent the relative importance of the block over a wide variation of block size between different layers. The block scheduling problem is then transformed to an optimization problem that maximizes the priority sum of the delivered video blocks. We develop both centralized and distributed scheduling algorithms for the problem. Simulation of two popular scalability types has been conducted to evaluate the performance of the algorithms. The simulation results show that the proposed algorithm is effective in terms of bandwidth utilization and video quality", "keywords": ["p2p streaming", "scalable video coding", "block scheduling algorithm"]}
{"id": "kp20k_training_183", "title": "A Threshold for a Polynomial Solution of #2SAT", "abstract": "The #SAT problem is a classical #P-complete problem even for monotone, Horn and two conjunctive formulas (the last known as #2SAT). We present a novel branch and bound algorithm to solve the #2SAT problem exactly. Our procedure establishes a new threshold where #2SAT can be computed in polynomial time. We show that for any 2-CF formula F with n variables where #2SAT(F) <= p(n), for some polynomial p, #2SAT(F) is computed in polynomial time. This is a new way to measure the degree of difficulty for solving #2SAT and, according to such measure our algorithm allows to determine a boundary between 'hard' and 'easy' instances of the #2SAT problem", "keywords": ["2sat problem", "branch-bound algorithm", "polynomial thresholds", "efficient counting"]}
{"id": "kp20k_training_184", "title": "Bagging and Boosting statistical machine translation systems", "abstract": "In this article we address the issue of generating diversified translation systems from a single Statistical Machine Translation (SMT) engine for system combination. Unlike traditional approaches, we do not resort to multiple structurally different SMT systems, but instead directly learn a strong SMT system from a single translation engine in a principled way. Our approach is based on Bagging and Boosting which are two instances of the general framework of ensemble learning. The basic idea is that we first generate an ensemble of weak translation systems using a base learning algorithm, and then learn a strong translation system from the ensemble. One of the advantages of our approach is that it can work with any of current SMT systems and make them stronger almost \"for free\". Beyond this, most system combination methods are directly applicable to the proposed framework for generating the final translation system from the ensemble of weak systems. We evaluate our approach on Chinese-English translation in three state-of-the-art SMT systems, including a phrase-based system, a hierarchical phrase-based system and a syntax-based system. Experimental results on the NIST MT evaluation corpora show that our approach leads to significant improvements in translation accuracy over the baselines. More interestingly, it is observed that our approach is able to improve the existing system combination systems. The biggest improvements are obtained by generating weak systems using Bagging/Boosting, and learning the strong system using a state-of-the-art system combination method.  ", "keywords": ["statistical machine translation", "ensemble learning", "system combination"]}
{"id": "kp20k_training_185", "title": "Functional dimensioning and tolerancing software for concurrent engineering applications", "abstract": "This paper describes the development of a prototype software package for solving functional dimensioning and tolerancing (FD&T) problems in a Concurrent Engineering environment. It provides a systematic way of converting functional requirements of a product into dimensional specifications by means of the following steps: firstly, the relationships necessary for solving FD&T problems are represented in a matrix form, known as functional requirements/dimensions (FR/D) matrix. Secondly, the values of dimensions and tolerances are then determined by satisfying all these relationships represented in a FR/D matrix by applying a comprehensive strategy which includes: tolerance allocation strategies for different types of FD&T problems and for determining an optimum solution order for coupled functional equations. The prototype software is evaluated by its potential users, and the results indicate that it can be an effective computer-based tool for solving FD&T problems in a CE environment.  ", "keywords": ["functional dimensioning and tolerancing", "concurrent engineering", "tolerance allocation"]}
{"id": "kp20k_training_186", "title": "Parametric Model-Checking of Stopwatch Petri Nets", "abstract": "At the border between control and verification, parametric verification can be used to synthesize constraints on the parameters to ensure that a system verifies given specifications. In this paper we propose a new framework for the parametric verification of time Petri nets with stopwatches. We first introduce a parametric extension of time Petri nets with inhibitor arcs (ITPNs) with temporal parameters and we define a symbolic representation of the parametric state-space based on the classical state-class graph method. Then, we propose semi-algorithms for the parametric model-checking of a subset of parametric TCTL formulae on ITPNs. These results have been implemented in the tool ROMEO and we illustrate them in a case-study based on a scheduling problem", "keywords": ["time petri nets", "stopwatches", "model-checking", "parameters", "state-class graph"]}
{"id": "kp20k_training_187", "title": "Modelling the interaction of catecholamines with the alpha(1A) Adrenoceptor towards a ligand-induced receptor structure", "abstract": "Adrenoceptors are members of the important G protein coupled receptor family for which the detailed mechanism of activation remains unclear. In this study, we have combined docking and molecular dynamics simulations to model the ligand induced effect on an homology derived human alpha(1A) adrenoceptor. Analysis of agonist/alpha(1A) adrenoceptor complex interactions focused on the role of the charged amine group, the aromatic ring, the N-methyl group of adrenaline, the beta hydroxyl group and the catechol meta and para hydroxyl groups of the catecholamines. The most critical interactions for the binding of the agonists are consistent with many earlier reports and our study suggests new residues possibly involved in the agonist-binding site, namely Thr-174 and Cys-176. We further observe a number of structural changes that occur upon agonist binding including a movement of TM-V away from TM-III and a change in the interactions of Asp-123 of the conserved DRY motif. This may cause Arg-124 to move out of the TM helical bundle and change the orientation of residues in IC-II and IC-III, allowing for increased affinity of coupling to the G-protein", "keywords": ["alpha-adrenoceptor", "agonists", "molecular docking", "molecular dynamics", "receptor activation"]}
{"id": "kp20k_training_188", "title": "probabilistic string similarity joins", "abstract": "Edit distance based string similarity join is a fundamental operator in string databases. Increasingly, many applications in data cleaning, data integration, and scientific computing have to deal with fuzzy information in string attributes. Despite the intensive efforts devoted in processing (deterministic) string joins and managing probabilistic data respectively, modeling and processing probabilistic strings is still a largely unexplored territory. This work studies the string join problem in probabilistic string databases, using the expected edit distance (EED) as the similarity measure. We first discuss two probabilistic string models to capture the fuzziness in string values in real-world applications. The string-level model is complete, but may be expensive to represent and process. The character-level model has a much more succinct representation when uncertainty in strings only exists at certain positions. Since computing the EED between two probabilistic strings is prohibitively expensive, we have designed efficient and effective pruning techniques that can be easily implemented in existing relational database engines for both models. Extensive experiments on real data have demonstrated order-of-magnitude improvements of our approaches over the baseline", "keywords": ["string joins", "probabilistic strings", "approximate string queries"]}
{"id": "kp20k_training_189", "title": "A high performance simulator of the immune response", "abstract": "The application of concepts and methods of statistical mechanics to biological problems is one of the most promising frontiers of computational physics. For instance Cellular Automata (CA), i.e. fully discrete dynamical systems evolving according to Boolean laws, appear to be extremely well suited to the simulation of the immune system dynamics. A prominent example of immunological CA is represented by the CeladaSeiden automaton that has proven capable of providing several new insights into the dynamics of the immune system response. In the present paper we describe a parallel version of the CeladaSeiden automaton. Details on the parallel implementation as well as performance data on the IBM SP2 parallel platform are presented and commented on", "keywords": ["immune response", "cellular automata ", "parallel virtual machine ", "memory management"]}
{"id": "kp20k_training_190", "title": "Speaker adaptation of language and prosodic models for automatic dialog act segmentation of speech", "abstract": "Speaker-dependent modeling has a long history in speech recognition, but has received less attention in speech understanding. This study explores speaker-specific modeling for the task of automatic segmentation of speech into dialog acts (DAs), using a linear combination of speaker-dependent and speaker-independent language and prosodic models. Data come from 20 frequent speakers in the ICSI meeting corpus; adaptation data per speaker ranges from 5k to 115k words. We compare performance for both reference transcripts and automatic speech recognition output. We find that: (1) speaker adaptation in this domain results both in a significant overall improvement and in improvements for many individual speakers, (2) the magnitude of improvement for individual speakers does not depend on the amount of adaptation data, and (3) language and prosodic models differ both in degree of improvement, and in relative benefit for specific DA classes. These results suggest important future directions for speaker-specific modeling in spoken language understanding tasks", "keywords": ["spoken language understanding", "dialog act segmentation", "speaker adaptation", "prosody modeling", "language modeling"]}
{"id": "kp20k_training_191", "title": "An efficient method for electromagnetic scattering analysis", "abstract": "We present a novel method to solve the magnetic field integral equation (MFIE) using the method of moments (MoM) efficiently. This method employs a linear combination of the divergence-conforming RaoWiltonGlisson (RWG) function and the curl-conforming nRWG function to test the MFIE in MoM. The discretization process and the relationship of this new testing function with the previously employed RWG and nRWG testing functions are presented. Numerical results of radar cross section (RCS) data for objects with sharp edges and corners show that accuracy of the MFIE can be improved significantly through the use of the new testing functions. At the same time, only the commonly used RWG basis functions are needed for this method", "keywords": ["combined raowiltonglisson function ", "electromagnetic scattering", "magnetic field integral equation ", "method of moments "]}
{"id": "kp20k_training_192", "title": "Empirical mode decomposition synthesis of fractional processes in 1D-and 2D-space", "abstract": "We report here on image texture analysis and on numerical simulation of fractional Brownian textures based on the newly emerged Empirical Mode Decomposition (EMD). EMD introduced by N.E. Huang et al. is a promising tool to non-stationary signal representation as a sum of zero-mean AM-FM components called Intrinsic Mode Functions (IMF). Recent works published by P. Flandrin et al. relate that, in the case of fractional Gaussian noise (fGn), EMD acts essentially as a dyadic filter bank that can be compared to wavelet decompositions. Moreover, in the context of fGn identification, P. Handrin et al. show that variance progression across IMFs is related to Hurst exponent H through a scaling law. Starting with these recent results, we propose a new algorithm to generate fGn, and fractional Brownian motion (fBm) of Hurst exponent H from IMFs obtained from EMD of a White noise, i.e. ordinary Gaussian noise (fGn with H= 1/2).  ", "keywords": ["empirical mode decomposition", "fractional processes synthesis", "gaussian and brownian texture images"]}
{"id": "kp20k_training_193", "title": "Flow topology in a steady three-dimensional lid-driven cavity", "abstract": "We present in this paper a thorough investigation of three-dimensional flow in a cubical cavity, subject to a constant velocity lid on its roof. In this steady-state analysis, we adopt the mixed formulation on tri-quadratic elements to preserve mass conservation. To resolve difficulties in the asymmetric and indefinite large-size matrix equations, we apply the BiCGSTAB solution solver. To achieve stability, weighting functions are designed in favor of variables on the upstream side. To achieve accuracy, the weighting functions are properly chosen so that false diffusion errors can be largely suppressed by the equipped streamline operator. Our aim is to gain some physical insight into the vortical flow using a theoretically rigorous topological theory. To broaden our understanding of the vortex dynamics in the cavity, we also study in detail the longitudinal spiralling motion in the flow interior.  ", "keywords": ["three-dimensional", "bicgstab solution solver", "topological theory"]}
{"id": "kp20k_training_194", "title": "Image object classification using saccadic search, spatio-temporal pattern encoding and self-organisation", "abstract": "A method for extracting features from photographic images is investigated. The input image is through a saccadic search algorithm divided into a set of sub-images, segmented and coded by a spatio-temporal encoding engine. The input image is thus represented by a set of characteristic pattern signatures, well suited for classification by an unsupervised neural network. A strategy using multiple self-organising feature maps (SOM) in a hierarchical manner is used. With this approach, using a certain degree of user selection, a database of sub-images is grouped according to similarities in signature space", "keywords": ["saccadic eye movement", "foveation", "segmentation", "pcnn time-series", "signatures", "hierarchical som"]}
{"id": "kp20k_training_195", "title": "Theoretical properties of LFSRs for built-in self test", "abstract": "Linear Feedback Shift-Registers have been studied for a long time as interesting solutions for error detection and correction techniques in transmissions. In the test domain, and principally in Built-In Self Test applications, they are often used as generators of pseudo-random test sequences. Conversely, their potential to generate prescribed deterministic test sequences is dealt within more recent works, and nowadays, allows the investigation of efficient test with a pseudo-deterministic BIST technique. Pseudo-deterministic test sequences are composed of both deterministic and pseudo-random test patterns and offer high fault coverage with a tradeoff between test length and hardware cost. In this paper, synthesis techniques for LFSRs that embed such kind of sequences are described", "keywords": ["built-in self test", "linear feedback shift register", "hardware test pattern generator"]}
{"id": "kp20k_training_196", "title": "Two fixed-parameter algorithms for vertex covering by paths on trees", "abstract": "VERTEX COVERING BY PATHS ON TREES with applications in machine translation is the task to cover all vertices of a tree T = (V, E) by choosing a minimum-weight subset of given paths in the tree. The problem is NP-hard and has recently been solved by an exact algorithm running in O(4 ", "keywords": ["graph algorithms", "combinatorial problems", "fixed-parameter tractability", "exact algorithms"]}
{"id": "kp20k_training_197", "title": "Graphical dynamic linear models: specification, use and graphical transformations", "abstract": "In this work, we propose a dynamic graphical model as a tool for Bayesian inference and forecasting in dynamic systems described by a series which is dependent on a state vector evolving according to a Markovian law. We build sequential algorithms for the probabilities propagation. This sequentiality turns out to be represented by the dynamic graphical structure alter carrying out several goal-oriented sequential graphical transformations.   MSG:   ", "keywords": ["graphical models", "dynamic models", "markovian dynamic systems", "learning and forecasting algorithms", "graphical transformations"]}
{"id": "kp20k_training_198", "title": "Alignment with non-overlapping inversions and translocations on two strings", "abstract": "An inversion and a translocation are important in bio sequence analysis and motivate researchers to consider the sequence alignment problem using these operations. Based on inversion and translocation, we introduce a new alignment problem with non-overlapping inversions and translocationsgiven two strings x and y, find an alignment with non-overlapping inversions and translocations for x and y. This problem has interesting application for finding a common sequence from two mutated sequences. We, in particular, consider the alignment problem when non-overlapping inversions and translocations are allowed for both x and y. We design an efficient algorithm that determines the existence of such an alignment and retrieves an alignment, if exists", "keywords": ["sequence alignment", "non-overlapping inversion", "translocation"]}
{"id": "kp20k_training_199", "title": "A twist to partial least squares regression", "abstract": "A modification of the PLS1 algorithm is presented. Stepwise optimization over a set of candidate loading weights obtained by taking powers of the y-X correlations and X standard deviations generalizes the classical PLS1 based on y-X covariances and hence adds flexibility to the modelling. When good linear predictions can be obtained, the suggested approach often finds models with fewer and more interpretable components. Good performance is demonstrated when compared with the classical PLS1 on calibration benchmark data sets. An important part of the comparisons is managed by a novel model selection strategy. The selection is based on choosing the simplest model among those with a cross-validation error smaller than the pre-specified significance limit of a chi(2)-statistic. ", "keywords": ["pls1", "powers of correlations and standard deviations", "cross-validation", "model selection", "model interpretation"]}
{"id": "kp20k_training_200", "title": "A robust and efficient finite volume scheme for the discretization of diffusive flux on extremely skewed meshes in complex geometries", "abstract": "In this paper an improved finite volume scheme to discretize diffusive flux on a non-orthogonal mesh is proposed. This approach, based on an iterative technique initially suggested by Khosla [P.K. Khosla, S.G. Rubin, A diagonally dominant second-order accurate implicit scheme, Computers and Fluids 2 (1974) 207209] and known as deferred correction, has been intensively utilized by Muzaferija [S. Muzaferija, Adaptative finite volume method for flow prediction using unstructured meshes and multigrid approach, Ph.D. Thesis, Imperial College, 1994] and later Fergizer and Peric [J.H. Fergizer, M. Peric, Computational Methods for Fluid Dynamics, Springer, 2002] to deal with the non-orthogonality of the control volumes. Using a more suitable decomposition of the normal gradient, our scheme gives accurate solutions in geometries where the basic idea of Muzaferija fails. First the performances of both schemes are compared for a Poisson problem solved in quadrangular domains where control volumes are increasingly skewed in order to test their robustness and efficiency. It is shown that convergence properties and the accuracy order of the solution are not degraded even on extremely skewed mesh. Next, the very stable behavior of the method is successfully demonstrated on a randomly distorted grid as well as on an anisotropically distorted one. Finally we compare the solution obtained for quadrilateral control volumes to the ones obtained with a finite element code and with an unstructured version of our finite volume code for triangular control volumes. No differences can be observed between the different solutions, which demonstrates the effectiveness of our approach", "keywords": ["finite volume", "diffusive flux discretization", "poisson equation", "deferred correction", "skewed meshes", "distorted grid"]}
{"id": "kp20k_training_201", "title": "Evaluation of Trend Localization with Multi-Variate Visualizations", "abstract": "Multi-valued data sets are increasingly common, with the number of dimensions growing. A number of multi-variate visualization techniques have been presented to display such data. However, evaluating the utility of such techniques for general data sets remains difficult. Thus most techniques are studied on only one data set. Another criticism that could be levied against previous evaluations of multi-variate visualizations is that the task doesn't require the presence of multiple variables. At the same time, the taxonomy of tasks that users may perform visually is extensive. We designed a task, trend localization, that required comparison of multiple data values in a multi-variate visualization. We then conducted a user study with this task, evaluating five multi-variate visualization techniques from the literature (Brush Strokes, Data-Driven Spots, Oriented Slivers, Color Blending, Dimensional Stacking) and juxtaposed grayscale maps. We report the results and discuss the implications for both the techniques and the task", "keywords": ["user study", "multi-variate visualization", "visual task design", "visual analytics"]}
{"id": "kp20k_training_202", "title": "An efficient reconfigurable multiplier architecture for Galois field GF(2m", "abstract": "This paper describes an efficient architecture of a reconfigurable bit-serial polynomial basis multiplier for Galois field GF(2m), where 1<m?M. The value m, of the irreducible polynomial degree, can be changed and so, can be configured and programmed. The value of M determines the maximum size that the multiplier can support. The advantages of the proposed architecture are (i) the high order of flexibility, which allows an easy configuration for different field sizes, and (ii) the low hardware complexity, which results in small area. By using the gated clock technique, significant reduction of the total multiplier power consumption is achieved", "keywords": ["galois field", "polynomial multiplication", "bit-serial", "irreducible polynomial", "all-one polynomial", "linear feedback shift register", "low power", "cryptography", "elliptic curves"]}
{"id": "kp20k_training_203", "title": "Experiences in building a Grid-based platform to serve Earth observation training activities", "abstract": "Earth observation data processing and storing can be done nowadays only using distributed systems. Experiments dealing with a large amount of data are possible within the timeframe of a lesson and can give trainees the freedom to innovate. Following these trends and ideas, we have built a proof-of-the-concept platform, named GiSHEO, for Earth observation educational tasks. It uses Grid computing technologies to analyze and store remote sensing data, and combines them with eLearning facilities. This paper provides an overview of the GiSHEO's platform architecture and of its technical and innovative solutions.  ", "keywords": ["distributed systems", "image processing software", "earth and atmospheric sciences", "computer uses in education"]}
{"id": "kp20k_training_204", "title": "Field D* path-finding on weighted triangulated and tetrahedral meshes", "abstract": "Classic shortest path algorithms operate on graphs, which are suitable for problems that can be represented by weighted nodes or edges. Finding a shortest path through a set of weighted regions is more difficult and only approximate solutions tend to scale well. The Field D* algorithm efficiently calculates an approximate, interpolated shortest path through a set of weighted regions and was designed for navigating robots through terrains with varying characteristics. Field D* operates on unit grid or quad-tree data structures, which require high resolutions to accurately model the boundaries of irregular world structures. In this paper, we extend the Field D* cost functions to 2D triangulations and 3D tetrahedral meshes: structures which model polygonal world structures more accurately. Since robots typically have limited resources available for computation and storage, we pay particular attention to computation and storage overheads when detailing our extensions. We begin by providing analytic solutions to the minimum of each cost function for 2D triangles and 3D tetrahedra. Our triangle implementation provides a 50 % improvement in performance over an existing triangle implementation. While our 3D extension to tetrahedra is the first full analytic extension of Field D* to 3D, previous work only provided an approximate minimization for a single cost function on a 3D cube with unit lengths. Each cost function is expressed in terms of a general function whose characteristics can be exploited to reduce the calculations required to find a minimum. These characteristics can also be exploited to cache the majority of cost functions, producing a speedup of up to 28 % in the 3D tetrahedral case. We demonstrate that, in environments composed of non-grid aligned data, Multi-resolution quad-tree Field D* requires an order of magnitude more faces and between 15 and 20 times more node expansions, to produce a path of similar cost to one produced by a triangle implementation of Field D* on a lower resolution triangulation. We provide examples of 3D pathing through models of complex topology, including pathing through anatomical structures extracted from a medical data set. To summarise, this paper details a robust and efficient extension of Field D* pathing to data sets represented by weighted triangles and tetrahedra, and also provides empirical data which demonstrates the reduction in storage and computation costs that accrue when one chooses such a representation over the more commonly used quad-tree and grid-based alternatives", "keywords": ["artificial intelligence", "problem solving", "control methods and search", "graph and tree search strategies", "vision and scene understanding", "representations", "data structures and transforms", "perceptual reasoning"]}
{"id": "kp20k_training_205", "title": "Correlation analysis of signal flow in a model prefrontal cortical circuit representing multiple target locations", "abstract": "In spite of the recent cross-correlation analyses of the monkey prefrontal cortical neurons performing spatial working memory tasks (J. Neurosci. 21 (2001) 3646; Cerebr. Cortex 10 (2000) 535), it is uncertain as to how much degree the correlation data reflect the circuitry of highly recurrent networks. We did a computer simulation of a model cortical circuit, whose connectivity is fully known, and analyzed the cross-correlations of the spikes of pairs of neurons in the model. The result shows that cross-correlation histograms (CCHs) of pyramidalpyramidal pairs tend to mask higher-order synaptic interactions, yielding CCHs with central peaks or almost flat CCHs. However, CCHs of pyramidalinterneuron pairs show displaced positive and/or negative peaks, depending on the connectivity of these neurons", "keywords": ["circuit", "working memory", "delay-period activity", "prefrontal cortex", "intracortical inhibition"]}
{"id": "kp20k_training_206", "title": "Slow-dynamic finite element simulation of manufacturing processes", "abstract": "Explicit time integration and dynamic finite element formulations are increasingly being used to analyze nonlinear static problems in solid and structural mechanics. This is particularly true in the simulation of sheer metal manufacturing processes. Employment of slow-dynamic, quasi-static techniques in static problems can introduce undesirable dynamic effects that originate from the inertia forces of the governing equations. In this paper, techniques and guidelines are presented, analyzed and demonstrated, which enable the minimization of the undesirable dynamic effects. The effect of the duration and functional form of the time histories of the loads and boundary conditions is quantified by the analysis of a linear spring mass oscillator. The resulting guidelines and techniques are successfully demonstrated in the nonlinear finite element simulation of a sheet metal deep drawing operation. The accuracy of the quasi-static, slow-dynamic finite element analyses is evaluated by comparison to results of laboratory experiments and purely static analyses. Various measures that quantify the dynamic effects, including kinetic energy, also are discussed. ", "keywords": ["dynamic", "explicit", "finite element", "implicit", "nonlinear", "quasi-static", "sheet metal"]}
{"id": "kp20k_training_207", "title": "Improving TCP performance in integrated wireless communications networks", "abstract": "Many analytical and simulation-based studies of TCP performance in wireless environments assume an error-free and congestion-free reverse channel that has the same capacity as the forward channel. Such an assumption does not hold in many real-world scenarios, particularly in the hybrid networks consisting of various wireless LAN (WLAN) and cellular technologies. In this paper, we first study, through extensive simulations, the performance characteristics of four representative TCP schemes, namely TCP New Reno, SACK, Veno, and Westwood, under the network conditions of asymmetric end-to-end link capacities, correlated wireless errors, and link congestion in both forward and reverse directions. We then propose a new TCP scheme, called TCP New Jersey, which is capable of distinguishing wireless packet losses from congestion packet losses, and reacting accordingly. TCP New Jersey consists of two key components, the timestamp-based available bandwidth estimation (TABE) algorithm and the congestion warning (CW) router configuration. TABE is a TCP-sender-side algorithm that continuously estimates the bandwidth available to the connection and guides the sender to adjust its transmission rate when the network becomes congested. TABE is immune to the ACK drops as well as ACK compression. CW is a configuration of network routers such that routers alert end stations by marking all packets when there is a sign of an incipient congestion. The marking of packets by the CW-configured routers helps the sender of the TCP connection to effectively differentiate packet losses caused by network congestion from those caused by wireless link errors. Our simulation results show that TCP New Jersey is able to accurately estimate the available bandwidth of the bottleneck link of an end-to-end path; and the TABE estimator is immune to link asymmetry, bi-directional congestion, and the relative position of the bottleneck link in the multi-hop end-to-end path. The proactive congestion avoidance control mechanism proposed in our scheme minimizes the network congestion, reduces the network volatility, and stabilizes the queue lengths while achieving more throughput than other TCP schemes", "keywords": ["wireless tcp", "bandwidth estimation", "explicit congestion notification", "congestion control", "loss differentiation"]}
{"id": "kp20k_training_208", "title": "A Hopfield neural network based task mapping method", "abstract": "With a prior knowledge of a program, static mapping aims to identify an optimal clustering strategy that can produce the best performance. In this paper we present a static method that uses Hopfield neural network to cluster the tasks of a parallel program for a given system. This method takes into account both load balancing and communication minimization. The method has been tested on a distributed shared memory system against other three clustering methods. Four programs, SOR, N-body, Gaussian Elimination and VQ, are used in the test. The result shows that our method is superior to the other three.  ", "keywords": ["task mapping", "hopfield network", "optimization", "distributed shared memory"]}
{"id": "kp20k_training_209", "title": "Investigating the evolution of code smells in object-oriented systems", "abstract": "Software design problems are known and perceived under many different terms, such as code smells, flaws, non-compliance to design principles, violation of heuristics, excessive metric values and anti-patterns, signifying the importance of handling them in the construction and maintenance of software. Once a design problem is identified, it can be removed by applying an appropriate refactoring, improving in most cases several aspects of quality such as maintainability, comprehensibility and reusability. This paper, taking advantage of recent advances and tools in the identification of non-trivial code smells, explores the presence and evolution of such problems by analyzing past versions of code. Several interesting questions can be investigated such as whether the number of problems increases with the passage of software generations, whether problems vanish by time or only by targeted human intervention, whether code smells occur in the course of evolution of a module or exist right from the beginning and whether refactorings targeting at smell removal are frequent. In contrast to previous studies that investigate the application of refactorings in the history of a software project, we attempt to analyze the evolution from the point of view of the problems themselves. To this end, we classify smell evolution patterns distinguishing deliberate maintenance activities from the removal of design problems as a side effect of software evolution. Results are discussed for two open-source systems and four code smells", "keywords": ["code smell", "refactoring", "software repositories", "software history", "evolution"]}
{"id": "kp20k_training_210", "title": "issues in parallelizing multigrid-based substrate model extraction and analysis", "abstract": "Accurate modeling of coupling effects via the substrate is an increasingly important concern in the design of mixed-signal systems such as communication, biomedical and analog signal processing circuits. Fast-switching digital blocks inject noise into the common substrate hindering the performance of high-precision sensible analog circuitry. Miniaturization effects on ICs complexity inevitably make the accuracy requirements for substrate coupling simulation increase. Due in part to the global nature of such couplings, model extraction and analysis is a computation-intensive task requiring the availability of fast and accurate substrate model extraction and analysis tools. One way to deal with this problem is to take further advantage of available computational technologies and distributed computing emerges as an interesting solution.In this paper we discuss several issues related to the parallelization of a Multigrid-based substrate model extraction and analysis tool. This tool is used as a proxy for generic computations on a 3D discretized volume. The results presented indicate potential avenues for successfully exploiting parallelism as well as pitfalls to avoid in such a quest", "keywords": ["substrate coupling", "multigrid", "grid computing"]}
{"id": "kp20k_training_211", "title": "Hypoxia-induced phrenic long-term facilitation: emergent properties", "abstract": "As in other neural systems, plasticity is a hallmark of the neural system controlling breathing. One spinal mechanism of respiratory plasticity is phrenic long-term facilitation (pLTF) following acute intermittent hypoxia. Although cellular mechanisms giving rise to pLTF occur within the phrenic motor nucleus, different signaling cascades elicit pLTF under different conditions. These cascades, referred to as Q and S pathways to phrenic motor facilitation (pMF), interact via cross-talk inhibition. Whereas the Q pathway dominates pLTF after mild to moderate hypoxic episodes, the S pathway dominates after severe hypoxic episodes. The biological significance of multiple pathways to pMF is unknown. This review will discuss the possibility that interactions between pathways confer emergent properties to pLTF, including pattern sensitivity and metaplasticity. Understanding these mechanisms and their interactions may enable us to optimize intermittent hypoxia-induced plasticity as a treatment for patients that suffer from ventilatory impairment or other motor deficits", "keywords": ["plasticity", "motor neuron", "intermittent hypoxia", "phrenic nerve", "pattern sensitivity", "metaplasticity"]}
{"id": "kp20k_training_212", "title": "Bivariate Mellin convolution operators: Quantitative approximation theorems", "abstract": "In this paper we study some qualitative and quantitative versions of the Voronovskaja approximation formulae for a class of bivariate Mellin convolution operators of type (T(w)f)(x, y) = integral(R+2) K(w)(tx(-1), vy(-1))f(t, v) dtdv/tv. Moreover we apply the general theory to some particular cases leading to various asymptotic formulae and involving various differential operators.  ", "keywords": ["mellin operators", "moments", "k-functional", "voronovskaja formula"]}
{"id": "kp20k_training_213", "title": "E-government evolution in EU local governments: a comparative perspective", "abstract": "Purpose - The purpose of this paper is to describe an empirical study of the advances and trends of e-government in transparency, openness and hence accountability in European Union (EU) local governments to determine the extent to which the internet promotes the convergence towards more transparent and accountable government. The paper also tests the extent to which different factors related to the implementation of information and communication technologies (ICTs), the number of inhabitants and the type of public administration style have influenced e-government developments in the cities studied. Design/methodology/approach - A comprehensive content analysis of 75 local government web sites was conducted using a 73-item evaluation questionnaire. The evaluations were performed in 2004 and 2007 and 15 EU countries were covered (five per country). To analyse the evolution of e-government, several techniques were used: tests of difference of means, multidimensional scaling and cluster analysis. The contribution of the different contextual factors to the development of government web sites was tested with OLS regression analysis. Findings - The results show noticeable progress in the application of ICTs and increasing EU local government concern for bringing government closer to citizens and for giving an image of modernity and responsiveness, although few web sites show clear signs of real openness to encouraging citizen dialogue. The evolution of the e-government initiatives analysed shows that, at present, they are still overlapped with the public administration style of each country as an extension of traditional front offices with potential benefits in speed and accessibility. Originality/value - Although a growing number of e-government studies are appearing, previous research has not analysed the evolution of EU local governments from a comparative perspective", "keywords": ["public administration style", "econometrics", "european union", "local government"]}
{"id": "kp20k_training_214", "title": "Soccer video processing for the detection of advertisement billboards", "abstract": "Billboards are placed on the sides of a soccer field for advertisement during match telecast. Unlike regular commercials, which are introduced during a break, on-field billboards appear on the TV screen at uncertain time instances, in different sizes, and also for different durations. Automated processing of soccer telecasts for detection and analysis of such billboards can provide important information on the effectiveness of this mode of advertising. We propose a method in which shot boundaries are first identified and the type of each shot is determined. Frames within each shot are then segmented to locate possible regions of interests (ROIs)  locations in a frame where billboards are potentially present. Finally, we use a combination of local and global features for detecting individual billboards by matching with a set of given templates", "keywords": ["soccer telecast", "advertisement billboard", "region of interest", "template matching"]}
{"id": "kp20k_training_215", "title": "Multi-component image segmentation in homogeneous regions based on description length minimization: Application to speckle, Poisson and Bernoulli noise", "abstract": "In this article, a minimum description length (MDL) criterion adapted to independent multi-component image segmentation into homogeneous regions is proposed. This approach, based on a deformable polygonal grid, allows us to segment noisy multi-component images perturbed with spatially independent speckle, Poisson or Bernoulli noise. The advantages of using such a multi-component approach rather than a mono-component one is demonstrated on synthetic and real images. This segmentation method is also applicable to multi-component images whose components do not follow the same noise statistics or have not been previously registered", "keywords": ["image segmentation", "minimum description length principle", "polygonal active contours", "multi-component images", "image registration"]}
{"id": "kp20k_training_216", "title": "Language dominance in interpersonal deception in computer-mediated communication", "abstract": "Dominance is not only a complicated social phenomenon that involves interpersonal dynamics, but also an effective strategy used in various applications such as deception detection, negotiation, and online community. The extensive literature on dominance has primarily focused on the personality traits and socio-biological influence, as well as various nonverbal and paralinguistic behaviors associated with dominance. Nonetheless, language dominance manifested through dynamically acquired linguistic capability and strategies has not been fully investigated. The exploration of language dominance in the context of deception is even rarer. With the increasing use of computer-mediated communication (CMC) in all aspects of modern life, language dominance in CMC has emerged as an important issue. This study examines language dominance in the context of deception via CMC. The experimental results show that deceivers: (1) demonstrate a different trend of language dominance from truthtellers over time; (2) manipulate the level of language dominance by initiating communication with low dominance and gradually increasing the level over the course of interaction, and (3) display higher levels of dominance in terms of some linguistic behaviors than truthtellers. They suggest that in CMC, deceivers not only adjust the level of language dominance more frequently, but also change it more remarkably than truthtellers", "keywords": ["dominance", "interpersonal deception", "linguistic behavior", "computer-mediated communication"]}
{"id": "kp20k_training_217", "title": "Benchmarking short sequence mapping tools", "abstract": "The development of next-generation sequencing instruments has led to the generation of millions of short sequences in a single run. The process of aligning these reads to a reference genome is time consuming and demands the development of fast and accurate alignment tools. However, the current proposed tools make different compromises between the accuracy and the speed of mapping. Moreover, many important aspects are overlooked while comparing the performance of a newly developed tool to the state of the art. Therefore, there is a need for an objective evaluation method that covers all the aspects. In this work, we introduce a benchmarking suite to extensively analyze sequencing tools with respect to various aspects and provide an objective comparison", "keywords": ["short sequence mapping", "next-generation sequencing", "benchmark", "sequence analysis"]}
{"id": "kp20k_training_219", "title": "Causality of frontal and occipital alpha activity revealed by directed coherence", "abstract": "Recently there has been increased attention to the causality among biomedical signals. The causality between brain structures involved in the generation of alpha activity is examined based on EEG signals acquired simultaneously in the frontal and occipital regions of the scalp. The concept of directed coherence (DC) is introduced as a means of resolving two-signal observations into the constituent components of original signals, the interaction between signals and the influence of one signal source on the other, through autoregressive modeling. The technique was applied to EEG recorded from 11 normal subjects with eyes closed. Through an analysis of the directed coherence, it was found that in both the left and right hemispheres, alpha rhythms with relatively low frequency had a significantly higher correlation in the frontal-occipital direction than in the opposite direction. In the upper alpha frequency band, a significantly higher DC was observed in the occipital-frontal direction, and the right-left DC in the occipital area was consistently higher. The activity of rhythms near 10 Hz was widespread. These results suggest that there is a difference in the genesis and the structure of information transmission in the lower and upper band, and for 10-Hz alpha waves", "keywords": ["eeg", "alpha rhythm", "coherence", "autoregressive model"]}
{"id": "kp20k_training_220", "title": "Learning finite binary sequences from half-space data", "abstract": "The problem of inferring a finite binary sequence w* is an element of (-1, 1)(n) is considered. It is supposed that at epochs t = 1, 2,..., the learner is provided with random half-space data in the form of finite binary sequences u((t)is an element of) {-1, 1}(n) which have positive inner-product with w*. The goal of the learner is to determine the underlying sequence w* in an efficient, on-line fashion from the data {u((t)), t greater than or equal to 1}. In this context, it is shown that the randomized, on-line directed drift algorithm produces a sequence of hypotheses {w((t)) is an element of {-1, 1}(n), t greater than or equal to 1} which converges to w* in finite time with probability 1. It is shown that while the algorithm has a minimal space complexity of 2n bits of scratch memory, it has exponential time complexity with an expected mistake bound of order Ohm(e(0.139n)). Batch incarnations of the algorithm are introduced which allow for massive improvements in running time with a relatively small cost in space (batch size). In particular, using a batch of O(n log n) examples at each update epoch reduces the expected mistake bound of the (batch) algorithm to O(n) (in an asynchronous bit update mode) and O(1) (in a synchronous bit update mode). The problem considered here is related to binary integer programming and to learning in a mathematical model of a neuron. ", "keywords": ["on-line learning", "batch learning", "binary perceptron", "neuron", "directed drift"]}
{"id": "kp20k_training_221", "title": "projection-based statistical analysis of full-chip leakage power with non-log-normal distributions", "abstract": "In this paper we propose a novel projection-based algorithm to estimate the full-chip leakage power with consideration of both inter-die and intra-die process variations. Unlike many traditional approaches that rely on log-Normal approximations, the proposed algorithm applies a novel projection method to extract a low-rank quadratic model of the logarithm of the full-chip leakage current and, therefore, is not limited to log-Normal distributions. By exploring the underlying sparse structure of the problem, an efficient algorithm is developed to extract the non-log-Normal leakage distribution with linear computational complexity in circuit size. In addition, an incremental analysis algorithm is proposed to quickly update the leakage distribution after changes to a circuit are made. Our numerical examples in a commercial 90nm CMOS process demonstrate that the proposed algorithm provides 4x error reduction compared with the previously proposed log-Normal approximations, while achieving orders of magnitude more efficiency than a Monte Carlo analysis with 104 samples", "keywords": ["leakage power", "statistical analysis"]}
{"id": "kp20k_training_222", "title": "Hybrid numerical methods for convection-diffusion problems in arbitrary geometries", "abstract": "The hybrid nodal-integral/finite element method (NI-FEM) and the hybrid nodal-integral/finite analytic method (NI-FAM) are developed to solve the steady-state, two-dimensional convection-diffusion equation (CDE). The hybrid NI-FAM for the steady-state problem is then extended to solve the more general time-dependent, two-dimensional, CDE. These hybrid coarse mesh methods, unlike the conventional nodal-integral approach, are applicable in arbitrary geometries and maintain the high efficiency of the conventional nodal-integral method (NIM). In steady-state problems, the computational domain for both hybrid methods is discretized using rectangular nodes in the interior of the domain and along vertical and horizontal boundaries, while triangular nodes are used along the boundaries that are not parallel to the x or y axes. In time-dependent problems, the rectangular and triangular nodes become space time parallelepiped and wedge-shaped nodes, respectively. The difference schemes for the variables on the interfaces of adjacent rectangular/parallelepiped nodes are developed using the conventional NIM. For the triangular nodes in the hybrid NI-FEM, a trial function is written in terms of the edge-averaged concentration of the three edges and made to satisfy the CDE in an integral sense. In the hybrid NI-FAM, the concentration over the triangular/wedge-shaped nodes is represented using a finite analytic approximation, which is based on the analytic solution of the one-dimensional CDE. The difference schemes for both hybrid methods are then developed for the interfaces between the rectangular/parallelepiped and triangular/wedge-shaped nodes by imposing continuity of the flux across the interfaces. A formal derivation of these hybrid methods and numerical results for several test problems are presented and discussed.  ", "keywords": ["nodal-integral method", "finite analytic method", "finite element method", "convection-diffusion equation", "arbitrary geometries"]}
{"id": "kp20k_training_223", "title": "Constrained Ellipse Fitting with Center on a Line", "abstract": "Fitting an ellipse to given data points is a common optimization task in computer vision problems. However, the possibility of incorporating the prior constraint the ellipses center is located on a given line into the optimization algorithm has not been examined so far. This problem arises, for example, by fitting an ellipse to data points representing the path of the image positions of an adhesion inside a rotating vessel whose position of the rotational axis in the image is known. Our new method makes use of a constrained algebraic cost function with the incorporated ellipse center on given line-prior condition in a global convergent one-dimensional optimization approach. Further advantages of the algorithm are computational efficiency and numerical stability", "keywords": ["ellipse fitting", "constrained cost function", "eigenvalue problem"]}
{"id": "kp20k_training_224", "title": "Trellis: Portability across architectures with a high-level framework", "abstract": "Trellis shows programmability benefits of a common and portable set of directives. We illustrate descriptive capability of directives that can support portable codes. We enhance the OpenACC model with more efficient mapping and synchronization. We implement prototype source translation of Trellis to OpenMP, OpenACC and CUDA", "keywords": ["parallel computation", "parallel frameworks", "parallel architectures", "loop mapping"]}
{"id": "kp20k_training_225", "title": "pedagogical content knowledge in programming education for secondary school", "abstract": "Dissertation overview, addressing the concept of Pedadogical Content Knowledge for the teaching and learning of programming for secondary education", "keywords": ["teaching", "learning", "programming", "pedagogical content knowledge", "secondary school"]}
{"id": "kp20k_training_226", "title": "Interference Analysis for Highly Directional 60-GHz Mesh Networks: The Case for Rethinking Medium Access Control", "abstract": "We investigate spatial interference statistics for multigigabit outdoor mesh networks operating in the unlicensed 60-GHz \"millimeter (mm) wave\" band. The links in such networks are highly directional: Because of the small carrier wavelength (an order of magnitude smaller than those for existing cellular and wireless local area networks), narrow beams are essential for overcoming higher path loss and can be implemented using compact electronically steerable antenna arrays. Directionality drastically reduces interference, but it also leads to \"deafness,\" making implicit coordination using carrier sense infeasible. In this paper, we make a quantitative case for rethinking medium access control (MAC) design in such settings. Unlike existing MAC protocols for omnidirectional networks, where the focus is on interference management, we contend that MAC design for 60-GHz mesh networks can essentially ignore interference and must focus instead on the challenge of scheduling half-duplex transmissions with deaf neighbors. Our main contribution is an analytical framework for estimating the collision probability in such networks as a function of the antenna patterns and the density of simultaneously transmitting nodes. The numerical results from our interference analysis show that highly directional links can indeed be modeled as pseudowired, in that the collision probability is small even with a significant density of transmitters. Furthermore, simulation of a rudimentary directional slotted Aloha protocol shows that packet losses due to failed coordination are an order of magnitude higher than those due to collisions, confirming our analytical results and highlighting the need for more sophisticated coordination mechanisms", "keywords": ["60-ghz networks", "interference analysis", "medium access control ", "millimeter  wave networks", "wireless mesh networks"]}
{"id": "kp20k_training_227", "title": "The influence of skeletal muscle anisotropy on electroporation: in vivo study and numerical modeling", "abstract": "The aim of this study was to theoretically and experimentally investigate electroporation of mouse tibialis cranialis and to determine the reversible electroporation threshold values needed for parallel and perpendicular orientation of the applied electric field with respect to the muscle fibers. Our study was based on local electric field calculated with three-dimensional realistic numerical models, that we built, and in vivo visualization of electroporated muscle tissue. We established that electroporation of muscle cells in tissue depends on the orientation of the applied electric field; the local electric field threshold values were determined (pulse parameters: 8100?s, 1Hz) to be 80V/cm and 200V/cm for parallel and perpendicular orientation, respectively. Our results could be useful electric field parameters in the control of skeletal muscle electroporation, which can be used in treatment planning of electroporation based therapies such as gene therapy, genetic vaccination, and electrochemotherapy", "keywords": ["in vivo electroporation", "skeletal muscle", "tissue anisotropy", "magnetic resonance imaging", "local electric field distribution"]}
{"id": "kp20k_training_229", "title": "SmartRank: a smart scheduling tool for mobile cloud computing", "abstract": "Resource scarcity is a major obstacle for many mobile applications, since devices have limited energy power and processing potential. As an example, there are applications that seamlessly augment human cognition and typically require resources that far outstrip mobile hardwares capabilities, such as language translation, speech recognition, and face recognition. A new trend has been explored to tackle this problem, the use of cloud computing. This study presents SmartRank, a scheduling framework to perform load partitioning and offloading for mobile applications using cloud computing to increase performance in terms of response time. We first explore a benchmarking of face recognition application using mobile cloud and confirm its suitability to be used as case study with SmartRank. We have applied the approach to a face recognition process based on two strategies: cloudlet federation and resource ranking through balanced metrics (level of CPU utilization and round-trip time). Second, using a full factorial experimental design we tuned the SmartRank with the most suitable partitioning decision calibrating scheduling parameters. Nevertheless, SmartRank uses an equation that is extensible to include new parameters and make it applicable to other scenarios", "keywords": ["mobile cloud computing", "offloading", "partitioning", "performance evaluation"]}
{"id": "kp20k_training_230", "title": "New global exponential stability conditions for inertial CohenGrossberg neural networks with time delays", "abstract": "In this paper, global exponential stability of inertial CohenGrossberg neural networks with time delays is investigated. By using Homeomorphism theorem and inequality technique, a LMI-based global exponential stability condition and inequality form global exponential stability condition are obtained for the above neural networks. In our result, the assumptions for the differentiability and monotonicity on the behaved functions in Ke and Miao (2013) [23] are removed. Thus our results are less conservative than those obtained in Ke and Miao (2013) [23]. Hence, we obtain new global exponential stability for this neural network", "keywords": ["inertial cohengrossberg neural networks", "homeomorphism", "lyapunov functional", "lmi", "inequality technique"]}
{"id": "kp20k_training_231", "title": "Extracting the fetal heart rate variability using a frequency tracking algorithm", "abstract": "In this work, we propose an algorithm to extract the fetal heart rate variability from an ECG measured from the mother abdomen. The algorithm consists of two methods: a separation algorithm based on second-order statistics that extracts the desired signal in one shot through the data, and a heart instantaneous frequency (HIF) estimator. The HIF algorithm is used to extract the mother heart rate which serves as reference to extract the fetal heart rate. We carried out simulations where the signals overlap in frequency and time, and showed that it worked efficiently", "keywords": ["source separation", "independent component analysis", "analytic signal", "a priori information", "second-order statistics", "auto-correlation"]}
{"id": "kp20k_training_232", "title": "Education and training in health informatics: the IT-EDUCTRA project", "abstract": "In this contribution both the EDUCTRA project of the European Advanced Informatics in Medicine Programme and the IT-EDUCTRA project of the Telematics Applications Programme (Health Sector) are described. EDUCTRA had as aim to investigate which gaps in knowledge health professionals have with respect to health informatics and to suggest ways to remedy this. It was assumed that health professionals had a basic understanding of health informatics and that additional educational material only had to cover the knowledge necessary for appreciating the new products coming from the AIM programme. A state-of-the-art survey revealed that the knowledge of health professionals with respect to health informatics was deplorable. Guidelines for curricula were therefore proposed to enable potential teachers to design courses. IT-EDUCTRA is a continuation of the EDUCTRA project. It has as aim to create learning materials covering a broad area of health informatics", "keywords": ["it-eductra", "education and training", "health informatics"]}
{"id": "kp20k_training_233", "title": "Methods for reasoning from geometry about anatomic structures injured by penetrating trauma", "abstract": "This paper presents the methods used for three-dimensional (3D) reasoning about anatomic structures affected by penetrating trauma in TraumaSCAN-Web, a platform-independent decision support system for evaluating the effects of penetrating trauma to the chest and abdomen. In assessing outcomes for an injured patient, TraumaSCAN-Web utilizes 3D models of anatomic structures and 3D models of the regions of damage associated with stab and gunshot wounds to determine the probability of injury to anatomic structures. Probabilities estimated from 3D reasoning about affected anatomic structures serve as input to a Bayesian network which calculates posterior probabilities of injury based on these initial probabilities together with available information about patient signs, symptoms and test results. In addition to displaying textual descriptions of conditions arising from penetrating trauma to a patient, TraumaSCAN-Web allows users to visualize the anatomy suspected of being injured in 3D, in this way providing a guide to its reasoning process", "keywords": ["decision support", "3d modeling", "expert systems"]}
{"id": "kp20k_training_234", "title": "Use of effective stiffness matrix for the free vibration analyses of a non-uniform cantilever beam carrying multiple two degree-of-freedom springdampermass systems", "abstract": "This paper investigates the free vibration characteristics of a non-uniform cantilever beam carrying multiple two degree-of-freedom (dof) springdampermass systems by means of two finite element methods, FEM1 and FEM2. Where the FEM1 is the conventional finite element method (FEM) with each two-dof springdampermass system being considered as a finite element possessing stiffness, damping and mass matrices, while the FEM2 is an alternative approach with each two-dof springdampermass system being replaced by an effective stiffness matrix composed of four massless effective springs. Instead of using both the real part ( ) and the imaginary part ( ) of a complex eigenvalue ( ) to derive the mathematical expressions, this paper directly employs the implicit-form of the complex eigenvalue ( ) to formulate the problem. In the FEM1, since each springdampermass system has two dof, the total dof of the entire system increases two if the beam carries one more two-dof springdampermass system. However, in the FEM2, the total dof of the entire system remains unchanged, because all dof of each springdampermass system are suppressed by the effective stiffness matrix. Good agreement between the natural frequencies obtained from FEM2 and the corresponding ones from FEM1 confirms the reliability of the presented theory", "keywords": ["finite element method", "effective stiffness matrix", "natural frequency", "complex eigenvalue", "degree of freedom "]}
{"id": "kp20k_training_235", "title": "A DTC strategy dedicated to three-switch three-phase inverter-fed induction motor drives", "abstract": "Purpose - The put-pose of this paper is to describe the implementation of a direct torque control strategy dedicated to three-switch three-phase delta-shaped inverter (TSTPI) fed induction motor drives as well as the comparison of its performance with those yielded by six-switch three-phase inverter (SSTPI) fed induction motor drives under the Takahashi DTC strategy. Design/methodology/approach - Referring to the asymmetrical stator voltage vectors and in order to reach high dynamic with low ripple of the electromagnetic torque response, the design of the vector selection table should include virtual voltage vectors by the subdivision of each sector into two equal sub-sectors. Findings - It has been shown that the implementation of the proposed DTC strategy in TSTPI-fed induction motor drives leads to higher transient behaviour and better steady-state features than those exhibited by the Takahashi DTC strategy implemented in SSTPI-fed induction motor drives. Research limitations/implications - The research should be extended to a comparison of the obtained simulation results with experimental measurements. Practical implications - A 50 per cent reduction of cost and compactness associated with a 50 per cent increase of reliability makes the TSTPI an interesting candidate, especially in large-scale production applications such as the automotive industry. Originality/value - The paper proposes an approach to improve the cost-effectiveness, the compactness and the reliability of TSTPI-fed induction motor drives, which represents a crucial benefit in electric and hybrid propulsion systems", "keywords": ["ellectric motors", "torque", "vectors", "simulation"]}
{"id": "kp20k_training_236", "title": "Hybrid generative/discriminative classifier for unconstrained character recognition", "abstract": "Handwriting recognition for hand-held devices like PDAs requires very accurate and adaptive classifiers. It is such a complex classification problem that it is quite usual now to make co-operate several classification methods. In this paper, we present an original two stages recognizer. The first stage is a model-based classifier which store an exhaustive set of character models. The second stage is a pairwise classifier which separate the most ambiguous pairs of classes. This hybrid architecture is based on the idea that the correct class almost systematically belongs to the two more relevant classes found by the first classifier. Experiments on a 80,000 examples database show a 30% improvement on a 62 classes recognition problem. Moreover, we show experimentally that such an architecture suits perfectly for incremental classification", "keywords": ["handwriting recognition", "multiple classifier system", "pairwise neural networks", "confusion matrix", "adaptive classifier"]}
{"id": "kp20k_training_237", "title": "optimal sleep patterns for serving delay-tolerant jobs", "abstract": "Sleeping is an important method to reduce energy consumption in many information and communication systems. In this paper we focus on a typical server under dynamic load, where entering and leaving sleeping mode incurs an energy and a response time penalty. We seek to understand under what kind of system configuration and control method will sleep mode obtain a Pareto Optimal tradeoff between energy saving and average response time. We prove that the optimal \"sleeping\" policy has a simple hysteretic structure. Simulation results then show that this policy results in significant energy savings, especially for relatively delay insensitive applications and under low traffic load. However, we demonstrate that seeking the maximum energy saving presents another tradeoff: it drives up the peak temperature in the server, with potential reliability consequences", "keywords": ["switching cost", "sleep state", "pareto tradeoff", "markov decision process", "energy efficiency"]}
{"id": "kp20k_training_238", "title": "On channel-discontinuity-constraint routing in wireless networks", "abstract": "Multi-channel wireless networks are increasingly deployed as infrastructure networks, e.g. in metro areas. Network nodes frequently employ directional antennas to improve spatial throughput. In such networks, between two nodes, it is of interest to compute a path with a channel assignment for the links Such that the path and link bandwidths are the same. This is achieved when any two consecutive links are assigned different channels, termed as \"Channel-Discontinuity-Constraint\" (CDC). CDC-paths are also useful in TDMA systems, where, preferably, consecutive links are assigned different time-slots. In the first part of this paper, we develop a t-spanner for CDC-paths using spatial properties; a sub-network containing 0(n10) links, for any 0 > 0, such that CDC-paths increase in cost by at most a factor t = (1 2 sin(0/2))(-2). We propose a novel distributed algorithm to compute the spanner using an expected number of 0(n log n) fixed-size messages. In the second part, we present a distributed algorithm to find minimum-cost CDC-paths between two nodes using 0(n(2)) fixed-size messages, by developing an extension of Edmonds' algorithm for minimum-cost perfect matching. In a centralized implementation, our algorithm runs in 0(n(2)) time improving the previous best algorithm which requires 0(n(3)) running time. Moreover, this running time improves to 0(n/0) when used in conjunction with the spanner developed.  ", "keywords": ["algorithms", "spanners", "routing", "directional antennas"]}
{"id": "kp20k_training_239", "title": "Composition of aspects based on a relation model: Synergy of multiple paradigms", "abstract": "Software composition for timely and affordable software development and evolution is one of the oldest pursuits of software engineering. In current software composition techniques, Component- Based Software Development (CBSD) and Aspect-Oriented Software Development (AOSD) have attracted academic and industrial attention. Blackbox composition used in CBSD provides simple and safe modularization for its strong information hiding, which is, however, the main obstacle for a black box composite to evolve later. This implies that an application developed through black box composition cannot take advantage of Aspect-Oriented Programming (AOP) used in AOSD. On the contrary, AOP enhances maintainability and comprehensibility by modularizing concerns crosscutting multiple components but lacks the support for the hierarchical and external composition of aspects themselves and compromises the important software engineering principles such as encapsulation, which is almost perfectly supported in black box composition. The role and role model have been recognized to have many similarities with CBSD and AOP but have significant differences with those composition techniques as well. Although each composition paradigm has its own advantages and disadvantages, there is no substantial support to realize the synergy of these composition paradigms; the black box composition, AOP, and role model. In this paper, a new composition technique based on representational abstraction of the relationship between component instances is introduced. The model supports the simple, elegant, and dynamic composition of components with its declarative form and provides the hooks through which an aspect can evolve and a parallel developed aspect can be merged at the instance level", "keywords": ["software composition", "aspect-oriented programming", "black box composition", "component-based software development", "role", "relation model", "logic"]}
{"id": "kp20k_training_240", "title": "Generalization performance of magnitude-preserving semi-supervised ranking with graph-based regularization", "abstract": "Semi-supervised ranking is a relatively new and important learning problem inspired by many applications. We propose a novel graph-based regularized algorithm which learns the ranking function in the semi-supervised learning framework. It can exploit geometry of the data while preserving the magnitude of the preferences. The least squares ranking loss is adopted and the optimal solution of our model has an explicit form. We establish error analysis of our proposed algorithm and demonstrate the relationship between predictive performance and intrinsic properties of the graph. The experiments on three datasets for recommendation task and two quantitative structureactivity relationship datasets show that our method is effective and comparable to some other state-of-the-art algorithms for ranking", "keywords": ["ranking", "semi-supervised learning", "generalization performance", "graph laplacian", "reproducing kernel hilbert space"]}
{"id": "kp20k_training_241", "title": "Accessible haptic user interface design approach for users with visual impairments", "abstract": "With the number of people with visual impairments (e.g., low vision and blind) continuing to increase, vision loss has become one of the most challenging disabilities. Today, haptic technology, using an alternative sense to vision, is deemed an important component for effectively accessing information systems. The most appropriately designed assistive technology is critical for those with visual impairments to adopt assistive technology and to access information, which will facilitate their tasks in personal and professional life. However, most of the existing design approaches are inapplicable and inappropriate to such design contexts as users with visual impairments interacting with non-graphical user interfaces (i.e., haptic technology). To resolve such design challenges, the present study modified a participatory design approach (i.e., PICTIVE, Plastic Interface for Collaborative Technology Initiatives Video Exploration) to be applicable to haptic technologies, by considering the brain plasticity theory. The sense of touch is integrated into the design activity of PICTIVE. Participants with visual impairments were able to effectively engage in designing non-visual interfaces (e.g., haptic interfaces) through non-visual communication methods (e.g., touch modality", "keywords": ["human factors", "design method", "visual impairments", "non-visual interfaces", "accessibility", "usability"]}
{"id": "kp20k_training_242", "title": "effect of probabilistic task allocation based on statistical analysis of bid values", "abstract": "This paper presents the effect of adaptively introducing appropriate strategies into the award phase of the contract net protocol (CNP) in a massively multi-agent system (MMAS", "keywords": ["contract net protocol", "task allocation", "massively multi-agent systems", "coordination"]}
{"id": "kp20k_training_243", "title": "higher-order concurrent programs with finite communication topology (extended abstract", "abstract": "Concurrent ML (CML) is an extension of the functional language Standard ML(SML) with primitives for the dynamic creation of processes and channels and for the communication of values over channels. Because of the powerful abstraction mechanisms the communication topology of a given program may be very complex and therefore an efficient implementation may be facilitated by knowledge of the topology. This paper presents an analysis for determining when a bounded number of processes and channels will be generated. The analysis proceeds in two stages. First we extend a polymorphic type system for SML to deduce not only the type of CML programs but also their communication behaviour expressed as terms in a new process algebra. Next we develop an analysis that given the communication behaviour predicts the number of processes and channels required during the execution of the CML program. The correctness of the analysis is proved using a subject reduction property for the type system", "keywords": ["type system", "communication", " ml ", "program", "order", "concurrent program", "efficiency", "abstraction", "values", "analysis", "dynamic", "correctness", "process algebra", "polymorphic", "reduction", "implementation", "standardization", "topologies", "process", "extensibility", "complexity", "paper", "knowledge", "functional languages"]}
{"id": "kp20k_training_244", "title": "On the polyhedral structure of a multi-item production planning model with setup times", "abstract": "We present and study a mixed integer programming model that arises as a substructure in many industrial applications. This model generalizes a number of structured MIP models previously studied, and it provides a relaxation of various capacitated production planning problems and other fixed charge network flow problems. We analyze the polyhedral structure of the convex hull of this model, as well as of a strengthened LP relaxation. Among other results, we present valid inequalities that induce facets of the convex hull under certain conditions. We also discuss how to strengthen these inequalities by using known results for lifting valid inequalities for 0-1 continuous knapsack problems", "keywords": ["mixed integer programming", "production planning", "polyhedral combinatorics", "capacitated lot-sizing", "fixed charge network flow"]}
{"id": "kp20k_training_246", "title": "Coherence between one random and one periodic signal for measuring the strength of responses in the electro-encephalogram during sensory stimulation", "abstract": "Coherence between a pulse train representing periodic stimuli and the EEG has been used in the objective detection of steady-state evoked potentials. This work aimed to quantify the strength of the stimulus responses based on the statistics of coherence estimate between one random and one periodic signal focusing on the confidence limits and power of significance tests in detecting responses. To detect the responses in 95% of cases, a signal-to-noise ratio of about -7.9 dB was required when using 48 windows (M) in the coherence estimation. The ratio, however, increased to -1.2 dB when M was 12. The results were tested in Monte Carlo simulations and applied to EEGs obtained from 14 subjects during visual stimulation. The method showed differences in the strength of responses at the stimulus frequency and its harmonics, as well as variations between individuals and over cortical regions. In contrast to those from the parietal and temporal regions, results for the occipital region gave confidence limits (with M = 12) that were above zero for all subjects, indicating statistically significant responses. The proposed technique extends the usefulness of coherence as a measure of stimulus responses and allows statistical analysis that could also be applied usefully in a range of other biological signals", "keywords": ["coherence", "eeg", "statistics", "rhythmic stimulation", "synchrony measure"]}
{"id": "kp20k_training_247", "title": "Exploring the dynamics of adaptation with evolutionary activity plots", "abstract": "Evolutionary activity statistics and their visualization are introduced, and their motivation is explained. Examples of their use are described, and their strengths and limitations are discussed. References to more extensive or general accounts of these techniques are provided", "keywords": ["evolutionary activity", "evolutionary adaptation", "visualization"]}
{"id": "kp20k_training_248", "title": "Repeated Exposure to the Abused Inhalant Toluene Alters Levels of Neurotransmitters and Generates Peroxynitrite in Nigrostriatal and Mesolimbic Nuclei in Rat", "abstract": "Toluene, a volatile hydrocarbon found in a variety of chemical compounds, is misused and abused by inhalation for its euphorigenic effects. Toluene's reinforcing properties may share a common characteristic with other drugs of abuse, namely, activation of the mesolimbic dopamine system. Prior studies in our laboratory found that acutely inhaled toluene activated midbrain dopamine neurons in the rat. Moreover, single systemic injections of toluene in rats produced a dose-dependent increase in locomotor activity which was blocked by depletion of nucleus accumbens dopamine or by pretreatment with a D2 dopamine receptor antagonist. Here we examined the effects of seven daily intraperitoneal injections of 600 mg/kg toluene on the content of serotonin and dopamine in the caudate nucleus (CN) and nucleus accumbens (NAC), substantia nigra, and ventral tegmental area at 2, 4, and 24 h after the last injection. Also, the roles of nitric oxide, peroxynitrite, and the production of 3-nitrosotyrosine (3-NT), in the CN and NAC were assessed at the same time points. Toluene treatments increased dopamine levels in the CN and NAC, and serotonin levels in CN, NAC, and ventral tegmental area. Measurements of the dopamine metabolite dihydroxyphenylacetic acid (DOPAC) further suggested a change in transmitter utilization in CN and NAC. Lastly, 3-NT levels also showed a differential change between CN and NAC, but at different time points post-toluene injection. These results point out the complexity of action of toluene on neurotransmitter function following a course of chronic exposure. Changes in the production of 3-NT also suggest that toluene-induced neurotoxicity may mediate via generation of peroxynitrite", "keywords": ["inhalant", "toluene", "neurotransmitter", "dopamine", "serotonin", "oxidative stress", "peroxynitrite", "3-nitrosotyrosine", "nigrostriatal and mesolimbic nuclei", "neurotoxicity"]}
{"id": "kp20k_training_249", "title": "supporting ad-hoc ranking aggregates", "abstract": "This paper presents a principled framework for efficient processing of ad-hoc top-k (ranking) aggregate queries, which provide the k groups with the highest aggregates as results. Essential support of such queries is lacking in current systems, which process the queries in a nave materialize-group-sort scheme that can be prohibitively inefficient. Our framework is based on three fundamental principles. The Upper-Bound Principle dictates the requirements of early pruning, and the Group-Ranking and Tuple-Ranking Principles dictate group-ordering and tuple-ordering requirements. They together guide the query processor toward a provably optimal tuple schedule for aggregate query processing. We propose a new execution framework to apply the principles and requirements. We address the challenges in realizing the framework and implementing new query operators, enabling efficient group-aware and rank-aware query plans. The experimental study validates our framework by demonstrating orders of magnitude performance improvement in the new query plans, compared with the traditional plans", "keywords": ["top-k query processing", "ranking", "decision support", "aggregate query", "olap"]}
{"id": "kp20k_training_250", "title": "Meaningful and meaningless solutions for cooperative n-person games", "abstract": "Game values often represent data that can be measured in more than one acceptable way (e.g., monetary amounts). We point out that in such a case a statement about cooperative n-person game models might be meaningless in the sense that its truth or falsity depends on the choice of an acceptable way to measure game values. In particular, we analyze statements about solution concepts such as the core, stable sets, the nucleolus, the Shapley value (and some of its generalizations", "keywords": ["robustness and sensitivity analysis", "game theory"]}
{"id": "kp20k_training_251", "title": "Numerical optimization algorithm for rotationally invariant multi-orbital slave-boson method", "abstract": "We develop a generalized numerical optimization algorithm for the rotationally invariant multi-orbital slave boson approach, which is applicable for arbitrary boundary constraints of high-dimensional objective function by combining several classical optimization techniques. After constructing the calculation architecture of rotationally invariant multi-orbital slave boson model, we apply this optimization algorithm to find the stable ground state and magnetic configuration of two-orbital Hubbard models. The numerical results are consistent with available solutions, confirming the correctness and accuracy of our present algorithm. Furthermore, we utilize it to explore the effects of the transverse Hunds coupling terms on metalinsulator transition, orbital selective Mott phase and magnetism. These results show the quick convergency and robust stable character of our algorithm in searching the optimized solution of strongly correlated electron systems", "keywords": ["slave boson", "numerical optimization algorithm", "hubbard model", "metalinsulator transition"]}
{"id": "kp20k_training_252", "title": "molecular dynamics simulation of large-scale carbon nanotubes on a shared-memory architecture", "abstract": "Carbon nanotubes are expected to play a significant role in the design and manufacture of many nano-mechanical and nano-electronic devices of future. It is important, therefore, that atomic level elastomechanical response properties of both single and multiwall nanotubes be investigated in detail. Classical molecular dynamics simulations employing Brenner's reactive potential with long range van der Waals interactions have been used in mechanistic response studies of carbon nanotubes to external strains. The studies of single and multiwalled carbon nanotubes under compressive strains show the instabilities beyond elastic response. Due to inclusion of non-bonded long range interactions, the simulations also show the redistribution of strain and strain energy from sideways bucklng to the formation of highly localized strained kink sites. Bond rearrangements occur at the kink sites, leading to formation of topological defects, preventing the tube from relaxing fully back to it's original configuration. Elastomechanic response behavior of single and multiwall carbon nanotubes to externally applied compressive strains is simulated and studied in detail. We will describe the results and discuss their implication towards the stability of any molecular mechanical structure made of carbon nanotubes", "keywords": ["interaction", "role", "stability", "shared memory", "mechanical properties", "play", "structure", "atom", "simulation", "inclusion", "carbon nanotubes", "large-scale", "behavior", "device", "origin2000", "architecture", "manufacturability", "design", "energy", "configurability", "response", "parallel", "future", "molecular dynamics"]}
{"id": "kp20k_training_253", "title": "Nonprimitive recursive complexity and undecidability for Petri net equivalences", "abstract": "The aim of this note is twofold. Firstly, it shows that the undecidability result for bisimilarity in [Theor. Comput. Sci. 148 (1995) 281-301] can be immediately extended for the whole range of equivalences land preorders) on labelled Petri nets. Secondly, it shows that restricting our attention to nets with finite reachable space, the respective (decidable) problems are nonprimitive recursive; this approach also applies to Mayr and Meyer's result [J. ACM 28 (1981) 561-576] for the reachability set equality, yielding a more direct proof.  ", "keywords": ["petri-nets", "decidability", "complexity"]}
{"id": "kp20k_training_254", "title": "time-decaying aggregates in out-of-order streams", "abstract": "Processing large data streams is now a major topic in data management. The data involved can be truly massive, and the required analyses complex. In a stream of sequential events such as stock feeds, sensor readings, or IP traffic measurements, data tuples pertaining to recent events are typically more important than older ones. This can be formalized via time-decay functions, which assign weights to data based on the age of data. Decay functions such as sliding windows and exponential decay have been studied under the assumption of well-ordered arrivals, i.e., data arrives in non-decreasing order of time stamps. However, data quality issues are prevalent in massive streams (due to network asynchrony and delays etc.), and correct arrival order is not guaranteed. We focus on the computation of decayed aggregates such as range queries, quantiles, and heavy hitters on out-of-order streams, where elements do not necessarily arrive in increasing order of timestamps. Existing techniques such as Exponential Histograms and Waves are unable to handle out-of-order streams. We give the first deterministic algorithms for approximating these aggregates under popular decay functions such as sliding window and polynomial decay. We study the overhead of allowing out-of-order arrivals when compared to well-ordered arrivals, both analytically and experimentally. Our experiments confirm that these algorithms can be applied in practice, and compare the relative performance of different approaches for handling out-of-order arrivals", "keywords": ["network", "sensor", "histogram", "order", "out-of-order arrivals", "streams", "asynchronous data streams", "data streaming", "range queries", "polynomial", "event", "approximation", "performance", "computation", "delay", "experience", "timing", "data quality", "traffic measurement", "data management", "practical", "timestamp", "sliding window", "aggregate", "data", "relation", "complexity", "algorithm", "age"]}
{"id": "kp20k_training_255", "title": "The Village Telco project: a reliable and practical wireless mesh telephony infrastructure", "abstract": "VoIP (Voice over IP) over mesh networks could be a potential solution to the high cost of making phone calls in most parts of Africa. The Village Telco (VT) is an easy to use and scalable VoIP over meshed WLAN (Wireless Local Area Network) telephone infrastructure. It uses a mesh network of mesh potatoes to form a peer-to-peer network to relay telephone calls without landlines or cell phone towers. This paper discusses the Village Telco infrastructure, how it addresses the numerous difficulties associated with wireless mesh networks, and its efficient deployment for VoIP services in some communities around the globe. The paper also presents the architecture and functions of a mesh potato and a novel combined analog telephone adapter (ATA) and WiFi access point that routes calls. Lastly, the paper presents the results of preliminary tests that have been conducted on a mesh potato. The preliminary results indicate very good performance and user acceptance of the mesh potatoes. The results proved that the infrastructure is deployable in severe and under-resourced environments as a means to make cheap phone calls and render Internet and IP-based services. As a result, the VT project contributes to bridging the digital divide in developing areas", "keywords": ["wlan", "wireless mesh networks", "voip", "mesh potato", "village telco", "rural telephony"]}
{"id": "kp20k_training_256", "title": "General Subspace Learning With Corrupted Training Data Via Graph Embedding", "abstract": "We address the following subspace learning problem: supposing we are given a set of labeled, corrupted training data points, how to learn the underlying subspace, which contains three components: an intrinsic subspace that captures certain desired properties of a data set, a penalty subspace that fits the undesired properties of the data, and an error container that models the gross corruptions possibly existing in the data. Given a set of data points, these three components can be learned by solving a nuclear norm regularized optimization problem, which is convex and can be efficiently solved in polynomial time. Using the method as a tool, we propose a new discriminant analysis (i.e., supervised subspace learning) algorithm called Corruptions Tolerant Discriminant Analysis (CTDA), in which the intrinsic subspace is used to capture the features with high within-class similarity, the penalty subspace takes the role of modeling the undesired features with high between-class similarity, and the error container takes charge of fitting the possible corruptions in the data. We show that CTDA can well handle the gross corruptions possibly existing in the training data, whereas previous linear discriminant analysis algorithms arguably fail in such a setting. Extensive experiments conducted on two benchmark human face data sets and one object recognition data set show that CTDA outperforms the related algorithms", "keywords": ["subspace learning", "corrupted training data", "discriminant analysis", "graph embedding"]}
{"id": "kp20k_training_257", "title": "CLOSURE PROPERTIES OF HYPER-MINIMIZED AUTOMATA", "abstract": "Two deterministic finite automata are almost equivalent if they disagree in acceptance only for finitely many inputs. An automaton A is hyper-minimized if no automaton with fewer states is almost equivalent to A. A regular language L is canonical if the minimal automaton accepting L is hyper-minimized. The asymptotic state complexity s*(L) of a regular language L is the number of states of a hyper-minimized automaton for a language finitely different from L. In this paper we show that: (1) the class of canonical regular languages is not closed under: intersection, union, concatenation, Kleene closure, difference, symmetric difference, reversal, homomorphism, and inverse homomorphism; (2) for any regular languages L(1) and L(2) the asymptotic state complexity of their sum L(1) boolean OR L(2), intersection L(1) boolean AND L(2), difference L(1) - L(2), and symmetric difference L(1) circle plus L(2) can be bounded by s*(L(1)) . s*(L(2)). This bound is tight in binary case and in unary case can be met in infinitely many cases. (3) For any regular language L the asymptotic state complexity of its reversal L(R) can be bounded by 2(s)* (L). This bound is tight in binary case. (4) The asymptotic state complexity of Kleene closure and concatenation cannot be bounded. Namely, for every k >= 3, there exist languages K, L, and M such that s*(K) = s*(L) = s*(M) = 1 and s*(K*) = s*(L . M) = k. These are answers to open problems formulated by Back et al. [RAIRO-Theor. Inf. Appl. 43 (2009) 69-94", "keywords": ["finite state automata", "regular languages", "hyper-minimized automata"]}
{"id": "kp20k_training_258", "title": "On equivalent parameter learning in simplified feature space based on Bayesian asymptotic analysis", "abstract": "Parametric models for sequential data, such as hidden Markov models, stochastic context-free grammars, and linear dynamical systems, are widely used in time-series analysis and structural data analysis. Computation of the likelihood function is one of primary considerations in many learning methods. Iterative calculation of the likelihood such as the model selection is still time-consuming though there are effective algorithms based on dynamic programming. The present paper studies parameter learning in a simplified feature space to reduce the computational cost. Simplifying data is a common technique seen in feature selection and dimension reduction though an oversimplified space causes adverse learning results. Therefore, we mathematically investigate a condition of the feature map to have an asymptotically equivalent convergence point of estimated parameters, referred to as the vicarious map. As a demonstration to find vicarious maps, we consider the feature space, which limits the length of data, and derive a necessary length for parameter learning in hidden Markov models", "keywords": ["sequential data model", "feature selection", "bayes learning", "algebraic geometry"]}
{"id": "kp20k_training_259", "title": "A dual-scale lattice gas automata model for gas-solid two-phase flow in bubbling fluidized beds", "abstract": "Modelling the hydrodynamics of gas/solid flow is important for the design and scale-up of fluidized bed reactors. A novel gas/solid dual-scale model based on lattice gas cellular automata (LGCA) is proposed to describe the macroscopic behaviour through microscopic gas-solid interactions. Solid particles and gas pseudo-particles are aligned in lattices with different scales for solid and gas. In addition to basic LGCA rules, additional rules for collision and propagation are specifically designed for gas-solid systems. The solid's evolution is then motivated by the temporal and spatial average momentum gained through solid-solid and gas-solid interactions. A statistical method, based on the similarity principle, is derived for the conversion between model parameters and hydrodynamic properties. Simulations for bubbles generated from a vertical jet in a bubbling fluidized bed based on this model agree well with experimental results, as well as with the results of two-fluid approaches and discrete particle simulations.  ", "keywords": ["lattice gas cellular automata", "gas/solid flow", "bubbling fluidized beds", "model"]}
{"id": "kp20k_training_260", "title": "scalable algorithms for global snapshots in distributed systems", "abstract": "Existing algorithms for global snapshots in distributed systems are not scalable when the underlying topology is complete. In a network with N processors, these algorithms require O ( N ) space and O ( N ) messages per processor. As a result, these algorithms are not efficient in large systems when the logical topology of the communication layer such as MPI is complete. In this paper, we propose three algorithms for global snapshot: a grid-based, a tree-based and a centralized algorithm. The grid-based algorithm uses O ( N ) space but only O (? N ) messages per processor. The tree-based algorithm requires only O (1) space and O (log N log w ) messages per processor where w is the average number of messages in transit per processor. The centralized algorithm requires only O (1) space and O (log w ) messages per processor. We also have a matching lower bound for this problem. Our algorithms have applications in checkpointing, detecting stable predicates and implementing synchronizers. We have implemented our algorithms on top of the MPI library on the Blue Gene/L supercomputer. Our experiments confirm that the proposed algorithms significantly reduce the message and space complexity of a global snapshot", "keywords": ["fault tolerance", "stable predicates", "global snapshot algorithms", "blue gene/l", "checkpointing"]}
{"id": "kp20k_training_261", "title": "A smart TCP acknowledgment approach for multihop wireless networks", "abstract": "Reliable data transfer is one of the most difficult tasks to be accomplished in multihop wireless networks. Traditional transport protocols like TCP face severe performance degradation over multihop networks given the noisy nature of wireless media as well as unstable connectivity conditions in place. The success of TCP in wired networks motivates its extension to wireless networks. A crucial challenge faced by TCP over these networks is how to operate smoothly with the 802.11 wireless MAC protocol which also implements a retransmission mechanism at link level in addition to short RTS/CTS control frames for avoiding collisions. These features render TCP acknowledgments (ACK) transmission quite costly. Data and ACK packets cause similar medium access overheads despite the much smaller size of the ACKs. In this paper, we further evaluate our dynamic adaptive strategy for reducing ACK-induced overhead and consequent collisions. Our approach resembles the sender side's congestion control. The receiver is self-adaptive by delaying more ACKs under nonconstrained channels and less otherwise. This improves not only throughput but also power consumption. Simulation evaluations exhibit significant improvement in several scenarios", "keywords": ["wireless multihop networks", "transport control protocol", "delayed acknowledgments"]}
{"id": "kp20k_training_262", "title": "Vasopressin and social odor processing in the olfactory bulb and anterior olfactory nucleus", "abstract": "Central vasopressin facilitates social recognition and modulates numerous complex social behaviors in mammals, including parental behavior, aggression, affiliation, and pair-bonding. In rodents, social interactions are primarily mediated by the exchange of olfactory information, and there is evidence that vasopressin signaling is important in brain areas where olfactory information is processed. We recently discovered populations of vasopressin neurons in the main and accessory olfactory bulbs and anterior olfactory nucleus that are involved in the processing of social odor cues. In this review, we propose a model of how vasopressin release in these regions, potentially from the dendrites, may act to filter social odor information to facilitate odor-based social recognition. Finally, we discuss recent human research linked to vasopressin signaling and suggest that our model of priming-facilitated vasopressin signaling would be a rewarding target for further studies, as a failure of priming may underlie pathological changes in complex behaviors", "keywords": ["olfaction", "social memory", "social recognition"]}
{"id": "kp20k_training_263", "title": "Ticks, Tick-Borne Rickettsiae, and Coxiella burnetii in the Greek Island of Cephalonia", "abstract": "Domestic animals are the hosts of several tick species and the reservoirs of some tick-borne pathogens; hence, they play an important role in the circulation of these arthropods and their pathogens in nature. They may act as vectors, but, also, as reservoirs of spotted fever group (SFG) rickettsiae, which are the causative agents of SFG rickettsioses. Q fever is a worldwide zoonosis caused by Coxiella burnetii (C. burnetii), which can be isolated from ticks. A total of 1,848 ticks (954 female, 853 male, and 41 nymph) were collected from dogs, goats, sheep, cattle, and horses in 32 different localities of the Greek island of Cephalonia. Rhipicephalus (Rh.) bursa, Rh. turanicus, Rh. sanguineus, Dermacentor marginatus (D. marginatus), Ixodes gibbosus (I. gibbosus), Haemaphysalis (Ha.) punctata, Ha. sulcata, Hyalomma (Hy.) anatolicum excavatum and Hy. marginatum marginatum were the species identified. C. burnetii and four different SFG rickettsiae, including Rickettsia (R.) conorii, R. massiliae, R. rhipicephali, and R. aeschlimannii were detected using molecular methods. Double infection with R. massiliae and C. burnetii was found in one of the positive ticks", "keywords": ["ticks", "rickettsia conorii", "rickettsia massiliae", "rickettsia rhipicephali", "rickettsia aeschlimannii", "coxiella burnetii", "greece"]}
{"id": "kp20k_training_264", "title": "Quiver polynomials in iterated residue form", "abstract": "Degeneracy loci polynomials for quiver representations generalize several important polynomials in algebraic combinatorics. In this paper we give a nonconventional generating sequence description of these polynomials when the quiver is of Dynkin type", "keywords": ["quiver", "degeneracy loci", "equivariant cohomology", "iterated residues"]}
{"id": "kp20k_training_265", "title": "The relationship among soft sets, soft rough sets and topologies", "abstract": "Molodtsovs soft set theory is a newly emerging tool to deal with uncertain problems. Based on the novel granulation structures called soft approximation spaces, Feng et al. initiated soft rough approximations and soft rough sets. Fengs soft rough sets can be seen as a generalized rough set model based on soft sets, which could provide better approximations than Pawlaks rough sets in some cases. This paper is devoted to establishing the relationship among soft sets, soft rough sets and topologies. We introduce the concept of topological soft sets by combining soft sets with topologies and give their properties. New types of soft sets such as keeping intersection soft sets and keeping union soft sets are defined and supported by some illustrative examples. We describe the relationship between rough sets and soft rough sets. We obtain the structure of soft rough sets and the topological structure of soft sets, and reveal that every topological space on the initial universe is a soft approximating space", "keywords": ["soft sets", "topological soft sets", "soft rough approximations", "soft rough sets", "rough sets", "topologies"]}
{"id": "kp20k_training_266", "title": "A highly efficient VLSI architecture for H.264/AVC CAVLC decoder", "abstract": "In this paper, an efficient algorithm is proposed to improve the decoding efficiency of the context-based adaptive variable length coding (CAVLC) procedure. Due to the data dependency among symbols in the decoding How, the CAVLC decoder requires large computation time, which dominates the overall decoder system performance. To expedite its decoding speed, the critical path in the CAVLC decoder is first analyzed and then reduced by forwarding the adaptive detection for succeeding symbols. With a shortened critical path, the CAVLC architecture is further divided into two segments, which can be easily implemented by a pipeline structure. Consequently, the overall performance is effectively improved. In the hardware implementation, a low power combined LUT and single output buffer have been adopted to reduce the area as well as power consumption without affecting the decoding performance. Experimental results show that the proposed architecture surpassing other recent designs can approximately reduce power consumption by 40% and achieve three times decoding speed in comparison to the original decoding procedure suggested in the H.264 standard. The maximum frequency can be larger than 210 MHz, which can easily support the real-time requirement for resolutions higher than the HD1080 format", "keywords": ["context-based adaptive variable length coding ", "h.264/avc", "variable length coding"]}
{"id": "kp20k_training_267", "title": "building database applications of virtual reality with x-vrml", "abstract": "A new method of building active database-driven virtual reality applications is presented. The term \"active\" is used to describe applications that allow server-side user interaction, dynamic composition of virtual scenes, access to on-line data, continuous visualization, and implementation of persistency.The use the X-VRML language for building active applications of virtual reality is proposed. X-VRML is a high-level XML-based language that overcomes the main limitations of the current virtual reality systems by providing convenient access to databases, object-orientation, parameterization, and imperative programming techniques. Applications of X-VRML include on-line data visualization, geographical information systems, scientific visualization, virtual games, and e-commerce applications such as virtual shops. In this paper, methods of accessing databases from X-VRML are described, architectures of X-VRML systems for different application domains are discussed, and examples of database applications of virtual reality implemented in X-VRML are presented", "keywords": ["data visualization", "activation", "applications", "use", "examples", "databases", "domain", "parameterization", "games", "java", " virtual reality ", "web3d", "paper", "access", "information system", "object-oriented", "compositing", "program", "mpeg-4", "method", "multimedia", "visualization", "systems", "architecture", "user interaction", "dynamic", "language", "xml", "implementation", "data", "vrml", "virtualization", "scientific visualiztion", "continuation", "server"]}
{"id": "kp20k_training_268", "title": "Distributed H infinity filtering for sensor networks with switching topology", "abstract": "In this article, the distributed H filtering problem is investigated for a class of sensor networks under topology switching. The main purpose is to design the distributed H filter that allows one to regulate the sensor's working modes. Firstly, a switched system model is proposed to reflect the working mode change of the sensors. Then, a stochastic sequence is adopted to model the packet dropout phenomenon occurring in the channels from the plant to the networked sensors. By utilising the Lyapunov functional method and stochastic analysis, some sufficient conditions are established to ensure that the filtering error system is mean-square exponentially stable with a prescribed H performance level. Furthermore, the filter parameters are determined by solving a set of linear matrix inequalities (LMIs). Our results relates the decay rate of the filtering error system to the switching frequency of the topology directly and shows the existence of such a distributed filter when the topology is not varying very frequently, which is helpful for the sensor state regulation. Finally, the effectiveness of the proposed design method is demonstrated by two numerical examples", "keywords": ["distributed filtering", "sensor networks", "energy efficient", "switching topology", "exponentially stable", "lmis"]}
{"id": "kp20k_training_269", "title": "The evolution of goal-based information modelling: literature review", "abstract": "Purpose - The first in a series on goal-based information modelling, this paper presents a literature review of two goal-based measurement methods. The second article in the series will build on this background to present an overview of some recent case-based research that shows the applicability of the goal-based methods for information modelling (as opposed to measurement). The third and concluding article in the series will present a new goal-based information model - the goal-based information framework (GbIF) - that is well suited to the task of documenting and evaluating organisational information flow. Design/methodology/approach - Following a literature review of the goal-question-metric (GQM) and goal-question-indicator-measure (GQIM) methods, the paper presents the strengths and weaknesses of goal-based approaches. Findings - The literature indicates that the goal-based methods are both rigorous and adaptable. With over 20 years of use, goal-based methods have achieved demonstrable and quantifiable results in both practitioner and academic studies. The down side of the methods are the potential expense and the \"expansiveness\" of goal-based models. The overheads of managing the goal-based process, from early negotiations on objectives and goals to maintaining the model (adding new goals, questions and indicators), could make the method unwieldy and expensive for organisations with limited resources. An additional challenge identified in the literature is the narrow focus of \"top-down\" (i.e. goal-based) methods. Since the methods limit the focus to a pre-defined set of goals and questions, the opportunity for discovery of new information is limited. Research limitations/implications - Much of the previous work on goal-based methodologies has been confined to software measurement contexts in larger organisations with well-established information gathering processes. Although the next part of the series presents goal-based methods outside of this native context, and within low maturity organisations, further work needs to be done to understand the applicability of these methods in the information science discipline. Originality/value - This paper presents ail overview of goal-based methods. The next article in the series will present the method outside the native context of software measurement. With the universality of the method established, information scientists will have a new tool to evaluate and document organisational information flow", "keywords": ["information", "modelling"]}
{"id": "kp20k_training_270", "title": "A communication reduction approach to iteratively solve large sparse linear systems on a GPGPU cluster", "abstract": "Finite Element Methods (FEM) are widely used in academia and industry, especially in the fields of mechanical engineering, civil engineering, aerospace, and electrical engineering. These methods usually convert partial difference equations into large sparse linear systems. For complex problems, solving these large sparse linear systems is a time consuming process. This paper presents a parallelized iterative solver for large sparse linear systems implemented on a GPGPU cluster. Traditionally, these problems do not scale well on GPGPU clusters. This paper presents an approach to reduce the communications between cluster compute nodes for these solvers. Additionally, computation and communication are overlapped to reduce the impact of data exchange. The parallelized system achieved a speedup of up to 15.3 times on 16 NVIDIA Tesla GPUs, compared to a single GPU. An analytical evaluation of the algorithm is conducted in this paper, and the analytical equations for predicting the performance are presented and validated", "keywords": ["iterative solver", "gpgpu cluster", "communication reduction", "sparse linear systems"]}
{"id": "kp20k_training_271", "title": "transductive inference using multiple experts for brushwork annotation in paintings domain", "abstract": "Many recent studies perform annotation of paintings based on brushwork. In these studies the brushwork is modeled indirectly as part of the annotation of high-level artistic concepts such as the artist name using low-level texture. In this paper, we develop a serial multi-expert framework for explicit annotation of paintings with brushwork classes. In the proposed framework, each individual expert implements transductive inference by exploiting both labeled and unlabelled data. To minimize the problem of noise in the feature space, the experts select appropriate features based on their relevance to the brushwork classes. The selected features are utilized to generate several models to annotate the unlabelled patterns. The experts select the best performing model based on Vapnik combined bound. The transductive annotation using multiple experts out-performs the conventional baseline method in annotating patterns with brushwork classes", "keywords": ["select", " framework ", "method", "brushwork", "inference", "space", "transductive inference", "painting", "annotation", "concept", "data", "relevance", "model", "paper", "feature", "noise", "feature selection", "class", "pattern"]}
{"id": "kp20k_training_272", "title": "On the integration of equations of motion for particle-in-cell codes", "abstract": "An area-preserving implementation of the 2nd order Runge-Kutta integration method for equations of motion is presented. For forces independent of velocity the scheme possesses the same numerical simplicity and stability as the leapfrog method, and is not implicit for forces which do depend on velocity. It can be therefore easily applied where the leapfrog method in general cannot. We discuss the stability of the new scheme and test its performance in calculations of particle motion in three cases of interest. First, in the ubiquitous and numerically demanding example of nonlinear interaction of particles with a propagating plane wave, second, in the case of particle motion in a static magnetic field and, third, in a nonlinear dissipative case leading to a limit cycle. We compare computed orbits with exact orbits and with results from the leapfrog and other low-order integration schemes. Of special interest is the role of intrinsic stochasticity introduced by time diferencing, which can destroy orbits of an otherwise exactly integrable system and therefore constitutes a restriction on the applicability of an integration scheme in such a context [A. Friedman, S.P. Auerbach, J. Comput. Phys. 93 (1991) 171]. In particular, we show that for a plane wave the new scheme proposed herein can be reduced to a symmetric standard map. This leads to the nonlinear stability condition Delta t omega(B) <= 1, where Delta t is the time step and omega(B) the particle bounce frequency.  ", "keywords": ["equations of motion", "2nd order integration methods", "nonlinear oscillations"]}
{"id": "kp20k_training_273", "title": "system support for mobile augmented reality services", "abstract": "Developing and deploying augmented reality (AR) services in pervasive computing environments is quite difficult because almost of all current systems require heavy and bulky head-mounted displays (HMDs) and are based on inflexible centralized architectures for detecting service locations and superimposing AR images. We propose a light-weight mobile AR service framework that combines personal mobile devices most of people own nowadays, visual tags as inexpensive AR techniques, and mobile code that enables easy-to-deploy environments. Our framework enables developers to easily deploy mobile AR services in pervasive computing environments and users to interact them in a both of practical and intuitive way", "keywords": ["vidgets framework", "mobile augmented reality"]}
{"id": "kp20k_training_274", "title": "Fabrication of the wireless systems for controlling movements of the electrical stimulus capsule in the small intestines", "abstract": "Diseases of the gastro-intestinal tract are becoming more prevalent. New techniques and devices, such as the wireless capsule endoscope and the telemetry capsule, that are able to measure the various signals of the digestive organs (temperature, pH, and pressure), have been developed for the observation of the digestive organs. In these capsule devices, there are no methods of moving and grasping them. In order to make a swift diagnosis and to give proper medication, it is necessary to control the moving speed of the capsule. This paper presents a wireless system for the control of movements of an electrical stimulus capsule. This includes an electrical stimulus capsule which can be swallowed and an external transmitting control system. A receiver, a receiving antenna (small multi-loop), a transmitter, and a transmitting antenna (monopole) were designed and fabricated taking into consideration the MPE, power consumption, system size, signal-to-noise ratio and the modulation method. The wireless system, which was designed and implemented for the control of movements of the electrical stimulus capsule, was verified by in-vitro experiments which were performed on the small intestines of a pig. As a result, we found that when the small intestines are contracted by electrical stimuli, the capsule can move to the opposite direction, which means that the capsule can go up or down in the small intestines", "keywords": ["wireless capsule endoscope", "electrical stimulus capsule", "moving speed", "wireless system", "receiver", "transmitter", "small multi-loop", "in-vitro experiments"]}
{"id": "kp20k_training_275", "title": "A CONTINUOUS WAVELET-BASED APPROACH TO DETECT ANISOTROPIC PROPERTIES IN SPATIAL POINT PROCESSES", "abstract": "A two-dimensional stochastic point process can be regarded as a random measure and thus represented as a (countable) sum of Delta Dirac measures concentrated at some points. Integration with respect to the point process itself leads to the concept of the continuous wavelet transform of a point process. Applying then suitable translation, rotation and dilation operations through a non unitary operator, we obtain a transformed point process which highlights main properties of the original point process. The choice of the mother wavelet is relevant and we thus conduct a detailed analysis proposing three two-dimensional mother wavelets. We use this approach to detect main directions present in the point process, and to test for anisotropy", "keywords": ["anisotropic point processes", "continuous wavelet transform", "curvature", "end-stopped mother wavelet", "mexican hat mother wavelet", "morlet mother wavelet", "energy density position representation", "random measure", "transformed point processes"]}
{"id": "kp20k_training_276", "title": "ROBUST OBJECT TRACKING USING JOINT COLOR-TEXTURE HISTOGRAM", "abstract": "A novel object tracking algorithm is presented in this paper by using the joint color-texture histogram to represent a target and then applying it to the mean shift framework. Apart from the conventional color histogram features, the texture features of the object are also extracted by using the local binary pattern (LBP) technique to represent the object. The major uniform LBP patterns are exploited to form a mask for joint color-texture feature selection. Compared with the traditional color histogram based algorithms that use the whole target region for tracking, the proposed algorithm extracts effectively the edge and corner features in the target region, which characterize better and represent more robustly the target. The experimental results validate that the proposed method improves greatly the tracking accuracy and efficiency with fewer mean shift iterations than standard mean shift tracking. It can robustly track the target under complex scenes, such as similar target and background appearance, on which the traditional color based schemes may fail to track", "keywords": ["object tracking", "mean shift", "local binary pattern", "color histogram"]}
{"id": "kp20k_training_277", "title": "Quasi-Resonant Interconnects: A Low Power, Low Latency Design Methodology", "abstract": "Design and analysis guidelines for quasi-resonant interconnect networks (QRN) are presented in this paper. The methodology focuses on developing an accurate analytic distributed model of the on-chip interconnect and inductor to obtain both low power and low latency. Excellent agreement is shown between the proposed model and SpectraS simulations. The analysis and design of the inductor, insertion point, and driver resistance for minimum power-delay product is described. A case study demonstrates the design of a quasi-resonant interconnect, transmitting a 5 Gb/s data signal along a 5 mm line in a TSMC 0.18-mu m CMOS technology. As compared to classical repeater insertion, an average reduction of 91.1% and 37.8% is obtained in power consumption and delay, respectively. As compared to optical links, a reduction of 97.1% and 35.6% is observed in power consumption and delay, respectively", "keywords": ["latency", "on-chip inductors", "on-chip interconnects", "power dissipation", "resonance"]}
{"id": "kp20k_training_278", "title": "Combining Hashing and Enciphering Algorithms for Epidemiological Analysis of Gathered Data", "abstract": "Objectives: Compiling individual records coming from different sources is necessary for multi-center studies. Legal aspects can be satisfied by implementing anonymization procedures. When using these procedures with a different key for each study it becomes almost impossible to link records from separate data collections. Methods: The originality of the method relies on the way the combination of hashing and enciphering techniques is performed: like in asymmetric encryption, two keys are used but the private key depends on the patient's identity. Results:The combination of hashing and enciphering techniques provides a great improvement in the overall security of the proposed scheme. Conclusion: This methodology makes stored data available for use in the field of public health, while respecting legal security requirements", "keywords": ["security", "patient identification", "encryption", "hashing"]}
{"id": "kp20k_training_279", "title": "A personalized English learning recommender system for ESL students", "abstract": "This paper has developed an online personalized English learning recommender system capable of providing ESL students with reading lessons that suit their different interests and therefore increase the motivation to learn. The system, using content-based analysis, collaborative filtering, and data mining techniques, analyzes real students reading data and generates recommender scores, based on which to help select appropriate lessons for respective students. Its performance having been tracked over a period of one year, this recommender system has proved to be very useful in heightening ESL learners motivation and interest in reading", "keywords": ["online learning", "learning system", "esl", "data mining", "association rules", "clustering", "recommender system"]}
{"id": "kp20k_training_280", "title": "Graph-based hierarchical conceptual clustering", "abstract": "Hierarchical conceptual clustering has proven to be a useful, although under-explored, data mining technique. A graph-based representation of structural information combined with a substructure discovery technique has been shown to be successful in knowledge discovery. The SUBDUE substructure discovery system provides one such combination of approaches. This work presents SUBDUE and the development of its clustering functionalities. Several examples are used to illustrate the validity of the approach both in structured and unstructured domains, as well as to compare SUBDUE to the Cobweb clustering algorithm. We also develop a new metric for comparing structurally-defined clusterings. Results show that SUBDUE successfully discovers hierarchical clusterings in both structured and unstructured data", "keywords": ["clustering", "cluster analysis", "concept formation", "structural data", "graph match"]}
{"id": "kp20k_training_281", "title": "An intelligent system employing an enhanced fuzzy c-means clustering model: Application in the case of forest fires", "abstract": "Fuzzy c-means is a well-established clustering algorithm. According to this approach instead of having each data point Dpi=(X,Y) belonging only to a specific cluster in a crisp manner, each Dpi belongs to all of the determined clusters with a different degree of membership. In this way cluster overlapping is allowed. This research effort enhances the fuzzy c-means model in an intelligent manner, employing a flexible fuzzy termination criterion. The enhanced fuzzy c-means clustering algorithm performs several iterations before the proper centers of the clusters more or less stabilize, which means that their coordinates remain almost equal to the previous ones. In this way the algorithm is expanded to perform in a more flexible and human like intelligent way, avoiding the chance of infinite loops and the performance of unnecessary iterations. A corresponding software system has been developed in C++ programming language applying the extended model. The system has been applied for the clustering of the Greek forest departments according to their forest fire risk. Two risk factors were taken into consideration, namely the number of forest fires and the annual burned forested areas. The design and the development of the innovative model-system and the results of its application are presented and discussed in this research paper", "keywords": ["extended fuzzy c-means clustering", "innovative fuzzy termination criterion", "forest fires", "forest fire risk clustering"]}
{"id": "kp20k_training_282", "title": "Miniaturization of UWB Antennas and its Influence on Antenna-Transceiver Performance in Impulse-UWB Communication", "abstract": "In this paper, a co-design methodology and the effect of antenna miniaturization in an impulse UWB system/transceiver is presented. Modified small-size printed tapered monopole antennas (PTMA) are designed in different scaling sizes. In order to evaluate the performance and functionality of these antennas, the effect of each antenna is studied in a given impulse UWB system. The UWB system includes an impulse UWB transmitter and two kinds of UWB receivers are considered, one based on correlation detection and one on energy detection schemes. A tunable low-power Impulse UWB transmitter is designed and the benefit of co-designing it with the PTMA antenna is investigated for the 3.110.6GHz band. A comparison is given between a 50(Omega ) design and a co-designed version. Our antenna/transceiver co-design methodology shows improvement in both transmitter efficiency and whole system performance. The simulation results show that the PTMA antenna and its miniaturized geometries are suitable for UWB applications", "keywords": ["uwb antennas", "design methodology", "impulse radio", "transceiver", "ultra-wideband"]}
{"id": "kp20k_training_283", "title": "INDUCED QUASI-ARITHMETIC UNCERTAIN LINGUISTIC AGGREGATION OPERATOR", "abstract": "Induced quasi-arithmetic aggregation operators are considered to aggregate uncertain linguistic information by using order inducing variables. We introduce the induced correlative uncertain linguistic aggregation operator with Choquet integral and we also present the induced uncertain linguistic aggregation operator by using the Dempster-Shafer theory of evidence. The special cases of the new proposed operators are investigated. Many existing linguistic aggregation operators are special cases of our new operators and more new uncertain linguistic aggregation operators can be derived from them. Decision making methods based on the new aggregation operators are proposed and architecture material supplier selection problems are presented to illustrate the feasibility and efficiency of the new methods", "keywords": ["choquet integral", "dempster-shafer theory", "uncertain linguistic variable", "aggregation operator", "decision making"]}
{"id": "kp20k_training_284", "title": "On fuzzy congruence of a near-ring module", "abstract": "The aim of this paper is to introduce fuzzy submodule and fuzzy congruence of an R-module (Near-ring module), to obtain the correspondence between fuzzy congruences and fuzzy submodules of an R-module, to define quotient R-module of an R-module over a fuzzy submodule and to obtain correspondence between fuzzy congruences of an R-module and fuzzy congruences of quotient R-module over a fuzzy submodule of an R-module.  ", "keywords": ["algebra", "r-module", "fuzzy submodule", "quotient module", "fuzzy congruence"]}
{"id": "kp20k_training_285", "title": "Self-bounded controlled invariant subspaces in measurable signal decoupling with stability: Minimal-order feedforward solution", "abstract": "The structural properties of self-bounded controlled invariant subspaces are fundamental to the synthesis of a dynamic feedforward compensator achieving insensitivity of the controlled output to a disturbance input accessible for measurement, on the assumption that the system is stable or pre-stabilized by an inner feedback. The control system herein devised has several important features: i) minimum order of the feedforward compensator; ii) minimum number of unassignable dynamics internal to the feedforward compensator; iii) maximum number of dynamics, external to the feedforward compensator, arbitrarily assignable by a possible inner feedback. From the numerical point of view, the design method herein detailed does not involve any computation of eigenspaces, which may be critical for systems of high order. The procedure is first presented for left-invertible systems. Then, it is extended to non-left-invertible systems by means of a simple, original, squaring-down technique", "keywords": ["geometric approach", "linear systems", "self-bounded controlled invariant subspaces", "measurable signal decoupling", "non-left-invertible systems"]}
{"id": "kp20k_training_286", "title": "hypergraph-based multilevel matrix approximation for text information retrieval", "abstract": "In Latent Semantic Indexing (LSI), a collection of documents is often pre-processed to form a sparse term-document matrix, followed by a computation of a low-rank approximation to the data matrix. A multilevel framework based on hypergraph coarsening is presented which exploits the hypergraph that is canonically associated with the sparse term-document matrix representing the data. The main goal is to reduce the cost of the matrix approximation without sacrificing accuracy. Because coarsening by multilevel hypergraph techniques is a form of clustering, the proposed approach can be regarded as a hybrid of factorization-based LSI and clustering-based LSI. Experimental results indicate that our method achieves good improvement of the retrieval performance at a reduced cost", "keywords": ["multilevel hypergraph partitioning", "text information retrieval", "latent semantic indexing", "low-rank matrix approximation"]}
{"id": "kp20k_training_287", "title": "Balanced paths in acyclic networks: Tractable cases and related approaches", "abstract": "Given a weighted acyclic network G and two nodes s and t in G, we consider the problem of computing k balanced paths from s to t, that is, k paths such that the difference in cost between the longest and the shortest path is minimized. The problem has several variants. We show that, whereas the general problem is solvable in pseudopolynomial time, both the arc-disjoint and the node-disjoint variants (i.e., the variants where the k paths are required to be arc-disjoint and node-disjoint, respectively) are strongly NP-Hard. We then address some significant special cases of such variants, and propose exact as well as approximate algorithms for their solution. The proposed approaches are also able to solve versions of the problem in which k origin-destination pairs are provided, and a set of k paths linking the origin-destination pairs has to be computed in such a way to minimize the difference in cost between the longest and the shortest path in the set. ", "keywords": ["layered networks", "balanced paths", "cost difference", "pseudopolynomial approaches"]}
{"id": "kp20k_training_288", "title": "The ?-connected assignment problem", "abstract": "Given a graph and costs of assigning to each vertex one of k different colors, we want to find a minimum cost assignment such that no color q induces a subgraph with more than a given number (?q) of connected components. This problem arose in the context of contiguity-constrained clustering, but also has a number of other possible applications. We show the problem to be NP-hard. Nevertheless, we derive a dynamic programming algorithm that proves the case where the underlying graph is a tree to be solvable in polynomial time. Next, we propose mixed-integer programming formulations for this problem that lead to branch-and-cut and branch-and-price algorithms. Finally, we introduce a new class of valid inequalities to obtain an enhanced branch-and-cut. Extensive computational experiments are reported", "keywords": ["assignment", "clustering", "cutting", "pricing", "integer programming"]}
{"id": "kp20k_training_289", "title": "Stable Spaces for Real-time Clothing", "abstract": "We present a technique for learning clothing models that enables the simultaneous animation of thousands of detailed garments in real-time. This surprisingly simple conditional model learns and preserves the key dynamic properties of a cloth motion along with folding details. Our approach requires no a priori physical model, but rather treats training data as a \"black box.\" We show that the models learned with our method are stable over large time-steps and can approximately resolve cloth-body collisions. We also show that within a class of methods, no simpler model covers the full range of cloth dynamics captured by ours. Our method bridges the current gap between skinning and physical simulation, combining benefits of speed from the former with dynamic effects from the latter. We demonstrate our approach on a variety of apparel worn by male and female human characters performing a varied set of motions typically used in video games (e.g., walking, running, jumping, etc", "keywords": ["cloth animation", "character animation", "virtual reality", "cloth simulation", "video games"]}
{"id": "kp20k_training_290", "title": "using topes to validate and reformat data in end-user programming tools", "abstract": "End-user programming tools offer no data types except \"string\" for many categories of data, such as person names and street addresses. Consequently, these tools cannot automatically validate or reformat these data. To address this problem, we have developed a user-extensible model for string-like data. Each \"tope\" in this model is a user-defined abstraction that guides the interpretation of strings as a particular kind of data. Specifically, each tope implementation contains software functions for recognizing and reformatting instances of that tope's kind of data. This makes it possible at runtime to distinguish between invalid data, valid data, and questionable data that could be valid or invalid. Once identified, questionable and/or invalid data can be double-checked and possibly corrected, thereby increasing the overall reliability of the data. Valid data can be automatically reformatted to any of the formats appropriate for that kind of data. To show the general applicability of topes, we describe new features that topes have enabled us to provide in four tools", "keywords": ["web macros", "data", "abstraction", "web applications", "spreadsheets", "end-user programming", "validation", "end-user software engineering"]}
{"id": "kp20k_training_291", "title": "Rough Sets and the role of the monetary policy in financial stability (macroeconomic problem) and the prediction of insolvency in insurance sector (microeconomic problem", "abstract": "This paper faces two questions related with financial stability. The first one is a macroeconomic problem in which we try to further investigate the role of monetary policy in explaining banking sector fragility and, ultimately, systemic banking crisis. It analyses a large sample of countries in the period 19811999. We find that the degree of central bank independence is one of the key variables to explain financial crisis. However, the effects of the degree of independence are not linear. Surprisingly, either a high degree of independence or a high degree of dependence are compatible with a situation of financial stability, while intermediate levels of independence are more likely associated with financial crisis. It seems that it is the uncertainty related with a non-clear allocation of monetary policy responsibilities that contributes to financial crisis episodes. The second one is a microeconomic problem: the prediction of insolvency in insurance companies. This question has been a concern of several parties stemmed from the perceived need to protect general public and to minimize the costs associated such as the effects on state insurance guaranty funds or the responsibilities for management and auditors. We have developed a bankruptcy prediction model for Spanish non-life insurance companies and the results obtained are very encouraging in comparison with previous analysis. This model could be used as an early warning system for supervisors in charge of the soundness of these entities and/or in charge of the financial system stability. Most methods applied in the past to tackle these two problems are techniques of statistical nature and, variables employed in these models do not usually satisfy statistical assumptions what complicates the analysis. We propose an approach to undertake these questions based on Rough Set Theory", "keywords": ["rough sets", "financial stability", "central bank independence", "insolvency", "insurance companies"]}
{"id": "kp20k_training_292", "title": "CLASSIFICATION OF SELF-DUAL CODES OF LENGTH 36", "abstract": "A complete classification of binary self-dual codes of length 36 is given", "keywords": ["self-dual code", "weight enumerator", "mass formula"]}
{"id": "kp20k_training_293", "title": "Supporting pervasive computing applications with active context fusion and semantic context delivery", "abstract": "Future pervasive computing applications are envisioned to adapt the applications behaviors by utilizing various contexts of an environment and its users. Such context information may often be ambiguous and also heterogeneous, which make the delivery of unambiguous context information to real applications extremely challenging. Thus, a significant challenge facing the development of realistic and deployable context-aware services for pervasive computing applications is the ability to deal with these ambiguous contexts. In this paper, we propose a resource optimized quality assured context mediation framework based on efficient context-aware data fusion and semantic-based context delivery. In this framework, contexts are first fused by an active fusion technique based on Dynamic Bayesian Networks and ontology, and further mediated using a composable ontological rule-based model with the involvement of users or application developers. The fused context data are then organized into an ontology-based semantic network together with the associated ontologies in order to facilitate efficient context delivery. Experimental results using SunSPOT and other sensors demonstrate the promise of this approach", "keywords": ["pervasive computing", "context awareness", "context fusion", "bayesian networks", "ontology", "sunspot"]}
{"id": "kp20k_training_294", "title": "On computing the minimum 3-path vertex cover and dissociation number of graphs", "abstract": "The dissociation number of a graph G is the number of vertices in a maximum size induced subgraph of G with vertex degree at most 1. A k-path vertex cover of a graph G is a subset S of vertices of G such that every path of order k in G contains at least one vertex from S. The minimum 3-path vertex cover is a dual problem to the dissociation number. For this problem, we present an exact algorithm with a running time of O*(1.5171(n)) on a graph with n vertices. We also provide a polynomial time randomized approximation algorithm with an expected approximation ratio of 23/11 for the minimum 3-path vertex cover.  ", "keywords": ["path vertex cover", "dissociation number", "approximation"]}
{"id": "kp20k_training_295", "title": "Interval multiplicative transitivity for consistency, missing values and priority weights of interval fuzzy preference relations", "abstract": "In this paper, the concept of multiplicative transitivity of a fuzzy preference relation, as defined by Tanino [T. Tanino, Fuzzy preference orderings in group decision-making, Fuzzy Sets and Systems 12 (1984) 117131], is extended to discover whether an interval fuzzy preference relation is consistent or not, and to derive the priority vector of a consistent interval fuzzy preference relation. We achieve this by introducing the concept of interval multiplicative transitivity of an interval fuzzy preference relation and show that, by solving numerical examples, the test of consistency and the weights derived by the simple formulas based on the interval multiplicative transitivity produce the same results as those of linear programming models proposed by Xu and Chen [Z.S. Xu, J. Chen, Some models for deriving the priority weights from interval fuzzy preference relations, European Journal of Operational Research 184 (2008) 266280]. In addition, by taking advantage of interval multiplicative transitivity of an interval fuzzy preference relation, we put forward two approaches to estimate missing value(s) of an incomplete interval fuzzy preference relation, and present numerical examples to illustrate these two approaches", "keywords": ["interval multiplicative transitivity", "interval fuzzy preference relation", "consistency", "missing values", "priority vector"]}
{"id": "kp20k_training_296", "title": "An O(n log n) algorithm for finding a shortest central link segment", "abstract": "A central link segment of a simple n-vertex polygon P is a segment s inside P that minimizes the quantity max(x epsilon P) min(y epsilon s) d(L)(x, y), where d(L)(x, y) is the link distance between points a: and y of P. In this paper we present an O(n log n) algorithm for finding a central link segment of P. This generalizes previous results for finding an edge or a segment of P from which P is visible. Moreover, in the same time bound, our algorithm finds a central link segment of minimum length. Constructing a central link segment has applications to the problems of finding an optimal robot placement in a simply connected polygonal region and determining the minimum value k for which a given polygon is k-visible from some segment", "keywords": ["algorithm design and analysis", "computational geometry", "link distance", "simple polygon", "shortest segment"]}
{"id": "kp20k_training_297", "title": "Deconstructing switch-reference", "abstract": "This paper develops a new view on switch-reference, a phenomenon commonly taken to involve a morphological marker on a verb indicating whether the subject of this verb is coreferent with or disjoint from the subject of another verb. Ipropose a new structural source of switch-reference marking, which centers around coordination at different heights of the clausal structure, coupled with distinct morphological realizations of the syntactic coordination head. Conjunction of two VPs has two independent consequences: First, only a single external argument is projected; second, the coordinator head is realized by some marker A (the same subject marker). Conjunction of two vPs, by contrast, leads to projection of two independent external arguments and a different realization of the coordination by a marker B (the different subject marker). The hallmark properties of this analysis are that (i)subject identity or disjointness is only indirectly tied to the switch-reference markers, furnishing a straightforward account of cases where this correlation breaks down; (ii)switch-reference does not operate across fully developed clauses, which accounts for the widely observed featural defectiveness of switch-reference clauses; (iii)same subject and different subject constructions differ in their syntactic structure, thus accommodating cases where the choice of the switch-reference markers has an impact on event structure. The analysis is mainly developed on the basis of evidence from the Mexican language Seri, the Papuan language Amele, and the North-American language Kiowa", "keywords": ["coordination", "clause linkage", "reference tracking", "distributed morphology", "event semantics", "verbal projections"]}
{"id": "kp20k_training_298", "title": "An optimized parallel LSQR algorithm for seismic tomography", "abstract": "The LSQR algorithm developed by Paige and Saunders (1982) is considered one of the most efficient and stable methods for solving large, sparse, and ill-posed linear (or linearized) systems. In seismic tomography, the LSQR method has been widely used in solving linearized inversion problems. As the amount of seismic observations increase and tomographic techniques advance, the size of inversion problems can grow accordingly. Currently, a few parallel LSQR solvers are presented or available for solving large problems on supercomputers, but the scalabilities are generally weak because of the significant communication cost among processors. In this paper, we present the details of our optimizations on the LSQR code for, but not limited to, seismic tomographic inversions. The optimizations we have implemented to our LSQR code include: reordering the damping matrix to reduce its band-width for simplifying the communication pattern and reducing the amount of communication during calculations; adopting sparse matrix storage formats for efficiently storing and partitioning matrices; using the MPI I/O functions to parallelize the date reading and result writing processes; providing different data partition strategies for efficiently using computational resources. A large seismic tomographic inversion problem, the full-3D waveform tomography for Southern California, is used to explain the details of our optimizations and examine the performance on Yellowstone supercomputer at the NCAR-Wyoming Supercomputing Center (NWSC). The results showed that the required wall time of our code for the same inversion problem is much less than that of the LSQR solver from the PETSc library (Balay et al., 1997", "keywords": ["lsqr algorithm", "tomographic inversion", "mpi", "computational seismology", "inverse problems", "parallel scientific computing"]}
{"id": "kp20k_training_299", "title": "on computer-assisted classification of coupled integrable equations", "abstract": "We show how the triangularization method of the second author can be successfully applied to the problem of classification of homogeneous coupled integrable equations. The classifications rely on the recent algorithm developed by the first author that requires solving 17 systems of polynomial equations. We show that these systems can be completely resolved in the case of coupled Korteweg-de Vries, Sawada-Kotera and Kaup-Kupershmidttype equations", "keywords": ["generalized symmetries", "integrable pdes", "polynomial systems", "triangular decompositions", "mathematical physics"]}
{"id": "kp20k_training_300", "title": "A novel method for fingerprint verification that approaches the problem as a two-class pattern recognition problem", "abstract": "We present a system for fingerprint verification that approaches the problem as a two-class pattern recognition problem. The distances of the test fingerprint to the reference fingerprints are normalized by the corresponding mean values obtained from the reference set, to form a five-dimensional feature vector. This feature vector is then projected onto a one-dimensional Karhunen-Loeve space and then classified into one of the two classes (genuine or impostor", "keywords": ["fingerprint verification", "support vector machine"]}
{"id": "kp20k_training_301", "title": "The uncovering of hidden structures by Latent Semantic Analysis", "abstract": "Latent Semantic Analysis (LSA) is a well-known method for information retrieval. It has also been applied as a model of cognitive processing and word-meaning acquisition. This dual importance of LSA derives from its capacity to modulate the meaning of words by contexts, dealing successfully with polysemy and synonymy. The underlying reasons that make the method work are not clear enough. We propose that the method works because it detects an underlying block structure (the blocks corresponding to topics) in the term-by-document matrix. In real cases this block structure is hidden because of perturbations. We propose that the correct explanation for LSA must be searched in the structure of singular vectors rather than in the profile of singular values. Using the PerronFrobenius theory we show that the presence of disjoint blocks of documents is marked by sign-homogeneous entries in the vectors corresponding to the documents of one block and zeros elsewhere. In the case of nearly disjoint blocks, perturbation theory shows that if the perturbations are small, the zeros in the leading vectors are replaced by small numbers (pseudo-zeros). Since the singular values of each block might be very different in magnitude, their order does not mirror the order of blocks. When the norms of the blocks are similar, LSA works fine, but we propose that when the topics have different sizes, the usual procedure of selecting the first k singular triplets (k being the number of blocks) should be replaced by a method that selects the perturbed Perron vectors for each block", "keywords": ["perronfrobenius theory", "perturbation theory", "lsa", "information search and retrieval"]}
{"id": "kp20k_training_302", "title": "computing monodromy groups defined by plane algebraic curves", "abstract": "We present a symbolic-numeric method to compute the monodromy group of a plane algebraic curve viewed as a ramified covering space of the complex plane. Following the definition, our algorithm is based on analytic continuation of algebraic functions above paths in the complex plane. Our contribution is three-fold : first of all, we show how to use a minimum spanning tree to minimize the length of paths ; then, we propose a strategy that gives a good compromise between the number of steps and the truncation orders of Puiseux expansions, obtaining for the first time a complexity result about the number of steps; finally, we present an efficient numerical-modular algorithm to compute Puiseux expansions above critical points,which is a non trivial task", "keywords": ["algebraic curves", "riemann surfaces", "symbolic-numeric computation", "monodromy"]}
{"id": "kp20k_training_303", "title": "Stone-like representation theorems and three-valued filters in R-0- algebras (nilpotent minimum algebras", "abstract": "Nilpotent minimum algebras (NM-algebras) are algebraic counterpart of a formal deductive system where conjunction is modeled by the nilpotent minimum t-norm, a logic also independently introduced by Guo-Jun Wang in the mid 1990s. Such algebras are to this logic just what Boolean algebras are to the classical propositional logic. In this paper, by introducing respectively the Stone topology and a three-valued fuzzy Stone topology on the set of all maximal filters in an NM-algebra, we first establish two analogues for an NM-algebra of the well-known Stone representation theorem for a Boolean algebra, which state that the Boolean skeleton of an NM-algebra is isomorphic to the algebra of all clopen subsets of its Stone space and the three-valued skeleton is isomorphic to the algebra of all clopen fuzzy subsets of its three-valued fuzzy Stone space, respectively. Then we introduce the notions of Boolean filter and of three-valued filter in an NM-algebra, and finally we prove that three-valued filters and closed subsets of the Stone space of an NM-algebra are in one-to-one correspondence and Boolean filters uniquely correspond to closed subsets of the subspace consisting of all ultrafilters.  ", "keywords": ["non-classical logics", "nilpotent minimum", "finite square intersection property", "prime ideal theorem", "maximal filter", "stone representation theorem"]}
{"id": "kp20k_training_304", "title": "An adaptive learning scheme for load balancing with zone partition in multi-sink wireless sensor network", "abstract": "In many researches on load balancing in multi-sink WSN, sensors usually choose the nearest sink as destination for sending data. However, in WSN, events often occur in specific area. If all sensors in this area all follow the nearest-sink strategy, sensors around nearest sink called hotspot will exhaust energy early. It means that this sink is isolated from network early and numbers of routing paths are broken. In this paper, we propose an adaptive learning scheme for load balancing scheme in multi-sink WSN. The agent in a centralized mobile anchor with directional antenna is introduced to adaptively partition the network into several zones according to the residual energy of hotspots around sink nodes. In addition, machine learning is applied to the mobile anchor to make it adaptable to any traffic pattern. Through interactions with the environment, the agent can discovery a near-optimal control policy for movement of mobile anchor. The policy can achieve minimization of residual energys variance among sinks, which prevent the early isolation of sink and prolong the network lifetime", "keywords": ["adaptive learning", "reinforcement learning problem", "load balancing", "multi-sink wireless sensor network", "q-learning based adaptive zone partition scheme"]}
{"id": "kp20k_training_305", "title": "interactive visual tools to explore spatio-temporal variation", "abstract": "CommonGIS is a developing software system for exploratory analysis of spatial data. It includes a multitude of tools applicable to different data types and helping an analyst to find answers to a variety of questions. CommonGIS has been recently extended to support exploration of spatio-temporal data, i.e. temporally variant data referring to spatial locations. The set of new tools includes animated thematic maps, map series, value flow maps, time graphs, and dynamic transformations of the data. We demonstrate the use of the new tools by considering different analytical questions arising in the course of analysis of thematic spatio-temporal data", "keywords": ["animated maps", "temporal variation", "time-series spatial data", "information visualisation", "time-series analysis", "exploratory data analysis"]}
{"id": "kp20k_training_306", "title": "Multiprocessor system-on-chip (MPSoC) technology", "abstract": "The multiprocessor system-on-chip (MPSoC) uses multiple CPUs along with other hardware subsystems to implement a system. A wide range of MPSoC architectures have been developed over the past decade. This paper surveys the history of MPSoCs to argue that they represent an important and distinct category of computer architecture. We consider some of the technological trends that have driven the design of MPSoCs. We also survey computer-aided design problems relevant to the design of MPSoCs", "keywords": ["configurable processors", "encoding", "hardware/software codesign", "multiprocessor", "multiprocessor system-on-chip "]}
{"id": "kp20k_training_307", "title": "Statistical behavior of joint least-square estimation in the phase diversity context", "abstract": "The images recorded by optical telescopes are often degraded by aberrations that induce phase variations in the pupil plane. Several wavefront sensing techniques have been proposed to estimate aberrated phases. One of them is phase diversity, for which the joint least-square approach introduced by Gonsalves et al. is a reference method to estimate phase coefficients from the recorded images. In this paper, we rely on the asymptotic theory of Toeplitz matrices to show that Gonsalves' technique provides a consistent phase estimator as the size of the images grows. No comparable result is yielded by the classical joint maximum likelihood interpretation (e.g., as found in the work by Paxman et al.). Finally, our theoretical analysis is illustrated through simulated problems", "keywords": ["error analysis", "least-squares methods", "optical image processing", "parameter estimation", "phase diversity", "statistics", "toeplitz matrices"]}
{"id": "kp20k_training_308", "title": "Integrated in silico approaches for the prediction of Ames test mutagenicity", "abstract": "The bacterial reverse mutation assay (Ames test) is a biological assay used to assess the mutagenic potential of chemical compounds. In this paper approaches for the development of an in silico mutagenicity screening tool are described. Three individual in silico models, which cover both structure activity relationship methods (SARs) and quantitative structure activity relationship methods (QSARs), were built using three different modelling techniques: (1) an in-house alert model: which uses SAR approach where alerts are generated based on experts judgements; (2) a kNN approach (k-Nearest Neighbours), which is a QSAR model where a prediction is given based on outcomes of its k chemical neighbours; (3) a naive Bayesian model (NB), which is another QSAR model, where a prediction is derived using a Bayesian formula through preselected identified informative chemical features (e.g., physico-chemical, structural descriptors). These in silico models, were compared against two well-known alert models (DEREK and ToxTree) and also against three different consensus approaches (Categorical Bayesian Integration Approach (CBI), Partial Least Squares Discriminate Analysis (PLS-DA) and simple majority vote approach). By applying these integration methods on the validation sets it was shown that both integration models (PLS-DA and CBI) achieved better performance than any of the individual models or consensus obtained by simple majority rule. In conclusion, the recommendation of this paper is that when obtaining consensus predictions for Ames mutagenicity, approaches like PLS-DA or CBI should be the first choice for the integration as compared to a simple majority vote approach", "keywords": ["ames", "qsar", "sar", "admet", "in silico models"]}
{"id": "kp20k_training_309", "title": "Visualization and clustering of categorical data with probabilistic self-organizing map", "abstract": "This paper introduces a self-organizing map dedicated to clustering, analysis and visualization of categorical data. Usually, when dealing with categorical data, topological maps use an encoding stage: categorical data are changed into numerical vectors and traditional numerical algorithms (SOM) are run. In the present paper, we propose a novel probabilistic formalism of Kohonen map dedicated to categorical data where neurons are represented by probability tables. We do not need to use any coding to encode variables. We evaluate the effectiveness of our model in four examples using real data. Our experiments show that our model provides a good quality of results when dealing with categorical data", "keywords": ["probabilistic self-organizing map", "categorical variables", "visualization", "em algorithm"]}
{"id": "kp20k_training_310", "title": "Stiffness analysis of parallelogram-type parallel manipulators using a strain energy method", "abstract": "Stiffness analysis of a general PTPM using an algebraic method. Result comparison between the proposed method and a finite element analysis method. A new stiffness index relating the stiffness property to the wrench experienced in a task", "keywords": ["stiffness analysis", "parallelogram-type parallel manipulator", "strain energy method", "algebraic method", "stiffness index"]}
{"id": "kp20k_training_311", "title": "Simulation of natural and social process interactions - An example from Bronze Age Mesopotamia", "abstract": "New multimodel simulations of Bronze Age Mesopotamian settlement system dynamics, using advanced object-based simulation frameworks, are addressing fine-scale interaction of natural processes (crop growth, hydrology, etc.) and social processes (kinship-driven behaviors, farming and herding practices, etc.) on a daily basis across multi-enerational model runs. Key components of these simulations are representations of initial settlement populations that are demographically and socially plausible, and detailed models of social mechanisms that can produce and maintain realistic textures of social structure and dynamics over time. The simulation engine has broad applicability and is also being used to address modern problems such as agroeconomic sustainability in Southeast Asia. This article describes the simulation framework and presents results of initial studies, highlighting some social system representations", "keywords": ["multimodel", "simulations", "agent-based", "holistic", "environment", "social", "interaction"]}
{"id": "kp20k_training_312", "title": "Newton-Like Dynamics and Forward-Backward Methods for Structured Monotone Inclusions in Hilbert Spaces", "abstract": "In a Hilbert space setting we introduce dynamical systems, which are linked to Newton and LevenbergMarquardt methods. They are intended to solve, by splitting methods, inclusions governed by structured monotone operators M=A+B, where A is a general maximal monotone operator, and B is monotone and locally Lipschitz continuous. Based on the Minty representation of A as a Lipschitz manifold, we show that these dynamics can be formulated as differential systems, which are relevant to the CauchyLipschitz theorem, and involve separately B and the resolvents of A. In the convex subdifferential case, by using Lyapunov asymptotic analysis, we prove a descent minimizing property and weak convergence to equilibria of the trajectories. Time discretization of these dynamics gives algorithms combining Newtons method and forward-backward methods for solving structured monotone inclusions", "keywords": ["monotone inclusions", "newton method", "levenbergmarquardt regularization", "dissipative dynamical systems", "lyapunov analysis", "weak asymptotic convergence", "forward-backward algorithms", "gradient-projection methods"]}
{"id": "kp20k_training_313", "title": "Damage identification of a target substructure with moving load excitation", "abstract": "This paper presents a substructural damage identification approach under moving vehicular loads based on a dynamic response reconstruction technique. The relationship between two sets of time response vectors from the substructure subject to moving loads is formulated with the transmissibility matrix based on impulse response function in the wavelet domain. Only the finite element model of the intact target substructure and the measured dynamic acceleration responses from the target substructure in the damaged state are required. The time-histories of moving loads and interface forces on the substructure are not required in the proposed algorithm. The dynamic response sensitivity-based method is adopted for the substructural damage identification with the local damage modeled as a reduction in the elemental stiffness factor. The adaptive Tikhonov regularization technique is employed to have an improved identification result when noise effect is included in the measurements. Numerical studies on a three-dimensional box-section girder bridge deck subject to a single moving force or a two-axle three-dimensional moving vehicle are conducted to investigate the performance of the proposed substructural damage identification approach. The simulated local damage can be identified with 5% noise in the measured data", "keywords": ["substructure", "damage identification", "response reconstruction", "transmissibility", "wavelet", "moving loads"]}
{"id": "kp20k_training_314", "title": "randomized parallel communication (preliminary version", "abstract": "Using a simple finite degree interconnection network among n processors and a straightforward randomized algorithm for packet delivery, it is possible to deliver a set of n packets travelling to unique targets from unique sources in 0( log n ) expected time. The expected delivery time is in other words the depth of the interconnection graph. The b-way shufile networks are examples of such. This represents a crude analysis of the transient response to a sudden but very uniform request load on the network. Variations in the uniformity of the load are also considered. Consider s i packets with randomly chosen targets beginning at a source labelled i . The expected overall delay is then [equation] where the labelling is chosen so that s 1 ?s 2 ?. These ideas can be used to guage the asymptotic efficiency of various synchronous parallel algorithms which use such a randomized communications system. The only important assumption is that variations in the physical transmission time along any connection link are negligible in comparison to the amount of work done at a processor", "keywords": ["communication", "network", "use", "examples", "efficiency", "synchronization", "analysis", "parallel algorithm", "delay", "timing", "response", "worst case", "randomization", "comparisons", "linking", "randomized algorithm", "parallel communication", "average response time", "processor", "label", "systems", "parallel", "variation", "interconnect", "graph", "physical", "version", "connection", "interconnection network"]}
{"id": "kp20k_training_315", "title": "feature selection for fast speech emotion recognition", "abstract": "In speech based emotion recognition, both acoustic features extraction and features classification are usually time consuming,which obstruct the system to be real time. In this paper, we proposea novel feature selection (FSalgorithm to filter out the low efficiency features towards fast speech emotion recognition.Firstly, each acoustic feature's discriminative ability, time consumption and redundancy are calculated. Then, we map the original feature space into a nonlinear one to select nonlinear features,which can exploit the underlying relationship among the original features. Thirdly, high discriminative nonlinear feature with low time consumption is initially preserved. Finally, a further selection is followed to obtain low redundant features based on these preserved features. The final selected nonlinear features are used in features' extraction and features' classification in our approach, we call them qualified features. The experimental results demonstrate that recognition time consumption can be dramatically reduced in not only the extraction phase but also the classification phase. Moreover, a competitive of recognition accuracy has been observed in the speech emotion recognition", "keywords": ["emotion recognition", "time consumption", "qualified features", "feature selection", "nonlinear space"]}
{"id": "kp20k_training_316", "title": "Automated inspection planning of free-form shape parts by laser scanning", "abstract": "The inspection operation accounts for a large portion of manufacturing lead time, and its importance in quality control cannot be overemphasized. In recent years, due to the development of laser technology, the accuracy of laser scanners has been improved significantly so that they can be used in a production environment. They are noncontact-type-measuring devices and usually have the scanning speed that is 50100 times faster than that of coordinate measuring machines. This laser-scanning technology provides us a platform that enables us to perform a 100% inspection of complicated shape parts. This research proposes algorithms that lead to the automation of laser scanner-based inspection operations. The proposed algorithms consist of three steps: firstly, all possible accessible directions at each sampled point on a part surface are generated considering constraints existing in a laser scanning operation. The constraints include satisfying the view angle, the depth of view, checking interference with a part, and avoiding collision with the probe. Secondly, the number of scans and the most desired direction for each scan are calculated. Finally, the scan path that gives the minimum scan time is generated. The proposed algorithms are applied to sample parts and the results are discussed", "keywords": ["automated inspection", "reverse engineering", "laser scanner"]}
{"id": "kp20k_training_317", "title": "GBF: a grammar based filter for Internet applications", "abstract": "Observing network traffic is necessary for achieving different purposes such as system performance, network debugging and/or information security. Observations, as such, are obtained from low-level monitors that may record a large volume of relevant and irrelevant events. Thus adequate filters are needed to pass interesting information only. This work presents a multilayer system, GBF that integrates both packet (low-level) and document (high-level) filters. Actually, the design of GBF is grammar-based so that it relies upon a set of context-free grammars to carry out various processes, specially the document reconstruction process. GBF consists of three layers, acquisition layer, packet filter layer, and reconstruction layer. The performance of the reconstruction process is evaluated in terms of the time consumed during service separation and session separation tasks", "keywords": ["packet monitoring", "event filterng", "sniffing", "context free grammar", "document reconstruction"]}
{"id": "kp20k_training_318", "title": "Enhanced particle swarm optimizer incorporating a weighted particle", "abstract": "This study proposes an enhanced particle swarm optimizer incorporating a weighted particle (EPSOWP) to improve the evolutionary performance for a set of benchmark functions. In conventional particle swarm optimizer (PSO), there are two principal forces to guide the moving direction of each particle. However, if the current particle lies too close to either the personal best particle or the global best particle, the velocity is mainly updated by only one term. As a result, search step becomes smaller and the optimization of the swarm is likely to be trapped into a local optimum. To address this problem, we define a weighted particle for incorporation into the particle swarm optimization. Because the weighted particle has a better opportunity getting closer to the optimal solution than the global best particle during the evolution, the EPSOWP is capable of guiding the swarm to a better direction to search the optimal solution. Simulation results show the effectiveness of the EPSOWP, which outperforms various evolutionary algorithms on a selected set of benchmark functions. Furthermore, the proposed EPSOWP is applied to controller design and parameter identification for an inverted pendulum system as well as parameter learning of neural network for function approximation to show its viability to solve practical design problems", "keywords": ["particle swarm optimization ", "weighted particle", "convergence", "pid controller design", "inverted pendulum system", "neural network"]}
{"id": "kp20k_training_319", "title": "Media access protocol for a coexisting cognitive femtocell network", "abstract": "Femtocell networks are widely deployed to extend cellular network coverage into indoor environments such as large office spaces and homes. Cognitive radio functionality can be implemented in femtocell networks based on an overlay mechanism under the assumption of a hierarchical access scenario. This study introduces a novel femtocell network architecture, that is characterized by a completely autonomous femtocell bandwidth access and a distributed media access control protocol for supporting data and real-time traffic. The detailed description of the architecture and media access protocol is presented. Furthermore, in-depth theoretical analysis is performed on the proposed media access protocol using discrete-time Markov chain modeling to validate the effectiveness of the proposed protocol and architecture", "keywords": ["cognitive radio network", "dynamic spectrum access", "femtocell network", "media access control"]}
{"id": "kp20k_training_320", "title": "Integrating computer animation and multimedia", "abstract": "Multimedia provides an immensely powerful tool for the dissemination of both information and entertainment. Current multimedia presentations consist of synchronised excerpts of media (such as sound, video gi text) which are coordinated by an author to ensure a clear narrative is presented to the audience. However each of the segments of the presentation consist of previously recorded footage, only the timing and synchronisation are dynamically constructed. The next logical advance for such systems is therefore to include the capability of generating material 'on-the-fly' in response to the actions of the audience. This paper describes a mechanism for using computer animation to generate this interactive material. Unlike previous animation techniques the approach presented here is suitable for use in constructing a storyline which the author can control, but the user can influence. In order to allow such techniques to be used we also present a multimedia authoring gr playback system which incorporates interactive animation with existing media", "keywords": ["multimedia", "computer animation", "keyframing"]}
{"id": "kp20k_training_321", "title": "an ontology for supporting communities of practice", "abstract": "In the context of the Palette project aimed at enhancingallindividual and organizational learning in Communities of Practice (CoPs), we are developing Knowledge Management (KM) services. Our approach is based on an ontology dedicated to CoPs and built from analysis of information sources about eleven CoPs available in Palette project. This ontology aims both at modeling the members of the CoP and at annotating the CoP knowledge resources. The paper describes our method for building this ontology, its structure and contents and it analyses our experience feedback from the cooperative building of this ontology", "keywords": ["community of practice", "knowledge management", "ontology"]}
{"id": "kp20k_training_322", "title": "A set of neural tools for human-computer interactions: Application to the handwritten character recognition, and visual speech recognition problems", "abstract": "This paper presents a new technique of data coding and an associated set of homogenous processing tools for the development of Human Computer Interactions (HCI). The proposed technique facilitates the fusion of different sensorial modalities and simplifies the implementations. The coding takes into account the spatio-temporal nature of the signals to be processed in the framework of a sparse representation of data. Neural networks adapted to such a representation of data are proposed to perform the recognition tasks. Their development is illustrated by two examples: one of on-line handwritten character recognition; and the other of visual speech recognition", "keywords": ["human-machine interaction", "lipreading", "on-line handwritten character recognition", "spatio-temporal coding", "spatio-temporal neural networks", "spatio-temporal patterns", "spiking neurons", "visual speech recognition"]}
{"id": "kp20k_training_323", "title": "impact of sub-optimal checkpoint intervals on application efficiency in computational clusters", "abstract": "As computational clusters rapidly grow in both size and complexity, system reliability and, in particular, application resilience have become increasingly important factors to consider in maintaining efficiency and providing improved computational performance over predecessor systems. One commonly used mechanism for providing application fault tolerance in parallel systems is the use of checkpointing. By making use of a multi-cluster simulator, we study the impact of sub-optimal checkpoint intervals on overall application efficiency. By using a model of a 1926 node cluster and workload statistics from Los Alamos National Laboratory to parameterize the simulator, we find that dramatically overestimating the AMTTI has a fairly minor impact on application efficiency while potentially having a much more severe impact on user-centric performance metrics such a queueing delay. We compare and contrast these results with the trends predicted by an analytical model", "keywords": ["prediction", "simulation", "checkpointing", "resilience"]}
{"id": "kp20k_training_324", "title": "An approach to a content-based retrieval of multimedia data", "abstract": "This paper presents a data model tailored for multimedia data representation, along with the main characteristics of a Multimedia Query Language that exploits the features of the proposed model. The model addresses data presentation, manipulation and content-based retrieval. It consists of three parts: a Multimedia Description Model, which provides a structural view of raw multimedia data, a Multimedia Presentation Model, and a Multimedia Interpretation Model which allows semantic information to be associated with multimedia data. The paper focuses on the structuring of a multimedia data model which provides support for content-based retrieval of multimedia data. The Query Language is an extension of a traditional query language which allows restrictions to be expressed on features, concepts, and the structural aspects of the objects of multimedia data and the formulation of queries with imprecise conditions. The result of a query is an approximate set of database objects which partially match such a query", "keywords": ["multimedia information systems", "information storage and retrieval", "data modeling"]}
{"id": "kp20k_training_325", "title": "Monte Carlo EM with importance reweighting and its applications in random effects models1", "abstract": "In this paper we propose a new Monte Carlo EM algorithm to compute maximum likelihood estimates in the context of random effects models. The algorithm involves the construction of efficient sampling distributions for the Monte Carlo implementation of the E-step, together with a reweighting procedure that allows repeatedly using a same sample of random effects. In addition, we explore the use of stochastic approximations to speed up convergence once stability has been reached. Our algorithm is compared with that of McCulloch (1997). Extensions to more general problems are discussed", "keywords": ["importance sampling", "metropolishastings algorithm", "stochastic approximations"]}
{"id": "kp20k_training_326", "title": "A perceptual approach for stereoscopic rendering optimization", "abstract": "The traditional way of stereoscopic rendering requires rendering the scene for left and right eyes separately; which doubles the rendering complexity. In this study, we propose a perceptually-based approach for accelerating stereoscopic rendering. This optimization approach is based on the Binocular Suppression Theory, which claims that the overall percept of a stereo pair in a region is determined by the dominant image on the corresponding region. We investigate how binocular suppression mechanism of human visual system can be utilized for rendering optimization. Our aim is to identify the graphics rendering and modeling features that do not affect the overall quality of a stereo pair when simplified in one view. By combining the results of this investigation with the principles of visual attention, we infer that this optimization approach is feasible if the high quality view has more intensity contrast. For this reason, we performed a subjective experiment, in which various representative graphical methods were analyzed. The experimental results verified our hypothesis that a modification, applied on a single view, is not perceptible if it decreases the intensity contrast, and thus can be used for stereoscopic rendering", "keywords": ["stereoscopic rendering", "binocular vision", "binocular suppression", "perception"]}
{"id": "kp20k_training_327", "title": "using traditional loop unrolling to fit application on a new hybrid reconfigurable architecture", "abstract": "This paper presents a strategy to modify a sequential implementation of an H.264/AVC motion estimation to run on a new reconfigurable architecture called RoSA. The modifications aim to provide more parallelism that will be exploited by the architecture. In the strategy presented in this paper we used traditional loop unrolling and profile information as techniques to modify the application and to generate a best fit solution to RoSA architecture", "keywords": ["stream-based", "reconfigurable architecture", "optimization", "performance"]}
{"id": "kp20k_training_328", "title": "Evaluating fluid semantics for passive stochastic process algebra cooperation", "abstract": "Fluid modelling is a next-generation technique for analysing massive performance models. Passive cooperation is a popular cooperation mechanism frequently used by performance engineers. Therefore having an accurate translation of passive cooperation into a fluid model is of direct practical application. We compare different existing styles of fluid model translations of passive cooperation in a stochastic process algebra and show how the previous model can be improved upon significantly. We evaluate the new passive cooperation fluid semantics and show that the first-order fluid model is a good approximation to the dynamics of the underlying continuous-time Markov chain. We show that in a family of possible translations to the fluid model, there is an optimal translation which can be expected to introduce least error. Finally, we use these new techniques to show how the scalability of a passively-cooperating distributed software architecture could be assessed", "keywords": ["stochastic process algebra", "fluid approximation", "passive cooperation"]}
{"id": "kp20k_training_329", "title": "using new media to improve self-help for clients and staff", "abstract": "One of the most common frustrations for any person looking for technical support is actually finding effective technical support. Even if a solution seems clear, it can be misunderstood if the vernacular is not just right. A large part of a successful support call involves being able to determine the actual problem based on the information the client provides. Help desk analysts must have the ability to translate \"non-tech\" descriptions to identify a problem in technical terms and then communicate a solution using vernacular the client can understand. This process is always a little different. If we aim to be successful analysts, we must speak different \"languages\" in order to help our clients. Based on this logic, it stands to reason that our self-help documentation must do the same. Providing a variety of methods to get self-help ensures a message will be received by a wider audience. In the world of modern media, audiences are presented with many ways to consume information. This ensures the message is heard by the most people in a manner that is the most appealing and the most clear. New methods of consuming information have become possible as the face of mainstream media has become democratized over the last few years. This is thanks largely to the fact that the tools needed to create and distribute content have become affordable and readily available to anyone with a bit of technical skill. Anyone with a laptop, a webcam and a little imagination can and do create content. Considering all of this, we asked ourselves, \"Why shouldn't we?.\" We have found that creating content in new media is relatively easy and fun. Finding and creating new methods to deliver content positively engages and challenges our help desk team. Thinking about how to best use new media requires help desk analysts to rethink otherwise standardized and mundane processes and create fresh perspectives. The creation and production of new media establishes stronger ownership of procedures and process. We would like to share the following from our ongoing experiences with new media at our help desk: General issues we see with clients finding help How creating new media creates stronger ownership and morale with staff Expanding the technical skills of help desk staff How using new media improves our client experience Casting a wider net (ensuring a message gets to the most people) How we use new media and what we have done with it How to make your own video podcast in 1,345 easy steps", "keywords": ["self-help", "video podcast", "team building", "client support", "new media"]}
{"id": "kp20k_training_330", "title": "A framework for preservation of cloud users data privacy using dynamic reconstruction of metadata", "abstract": "In the rising paradigm of cloud computing, attainment of sustainable levels of cloud users trust in using cloud services is directly dependent on effective mitigation of its associated impending risks and resultant security threats. Among the various indispensible security services required to ensure effective cloud functionality leading to enhancement of users confidence in using cloud offerings, those related to the preservation of cloud users data privacy are significantly important and must be matured enough to withstand the imminent security threats, as emphasized in this research paper. This paper highlights the possibility of exploiting the metadata stored in cloud's database in order to compromise the privacy of users data items stored using a cloud provider's simple storage service. It, then, proposes a framework based on database schema redesign and dynamic reconstruction of metadata for the preservation of cloud users data privacy. Using the sensitivity parameterization parent class membership of cloud database attributes, the database schema is modified using cryptographic as well as relational privacy preservation operations. At the same time, unaltered access to database files is ensured for the cloud provider using dynamic reconstruction of metadata for the restoration of original database schema, when required. The suitability of the proposed technique with respect to private cloud environments is ensured by keeping the formulation of its constituent steps well aligned with the recommendations proposed by various Standards Development Organizations working in this domain", "keywords": ["cloud computing", "private cloud", "ubuntu enterprise cloud eucalyptus", "privacy", "metadata"]}
{"id": "kp20k_training_331", "title": "A systematic literature review on SOA migration", "abstract": "When Service Orientation was introduced as the solution for retaining and rehabilitating legacy assets, both researchers and practitioners proposed techniques, methods, and guidelines for SOA migration. With so much hype surrounding SOA, it is not surprising that the concept was interpreted in many different ways, and consequently, different approaches to SOA migration were proposed. Accordingly, soon there was an abundance of methods that were hard to compare and eventually adopt. Against this backdrop, this paper reports on a systematic literature review that was conducted to extract the categories of SOA migration proposed by the research community. We provide the state-of-the-art in SOA migration approaches, and discuss categories of activities carried out and knowledge elements used or produced in those approaches. From such categorization, we derive a reference model, called SOA migration frame of reference, that can be used for selecting and defining SOA migration approaches. As a co-product of the analysis, we shed light on how SOA migration is perceived in the field, which further points to promising future research directions. ", "keywords": ["migration", "service orientation", "systematic literature review", "knowledge management"]}
{"id": "kp20k_training_332", "title": "Application of projection pursuit learning to boundary detection and deblurring in images", "abstract": "Projection pursuit learning networks (PPLNs) have been used in many fields of research but have not been widely used in image processing. In this paper we demonstrate how this highly promising technique may be used to connect edges and produce continuous boundaries. We also propose the application of PPLN to deblurring a degraded image when little or no a priori information about the blur is available. The PPLN was successful at developing an inverse blur filter to enhance blurry images. Theory and background information on projection pursuit regression (PPR) and PPLN are also presented", "keywords": ["boundary detection", "image deblurring", "projection pursuit regression", "projection pursuit learning networks"]}
{"id": "kp20k_training_333", "title": "Learning to transform time series with a few examples", "abstract": "We describe a semisupervised regression algorithm that learns to transform one time series into another time series given examples of the transformation. This algorithm is applied to tracking, where a time series of observations from sensors is transformed to a time series describing the pose of a target. Instead of defining and implementing such transformations for each tracking task separately, our algorithm learns a memoryless transformation of time series from a few example input-output mappings. The algorithm searches for a smooth function that fits the training examples and, when applied to the input time series, produces a time series that evolves according to assumed dynamics. The learning procedure is fast and lends itself to a closed-form solution. It is closely related to nonlinear system identification and manifold learning techniques. We demonstrate our algorithm on the tasks of tracking RFID tags from signal strength measurements, recovering the pose of rigid objects, deformable bodies, and articulated bodies from video sequences. For these tasks, this algorithm requires significantly fewer examples compared to fully supervised regression algorithms or semisupervised learning algorithms that do not take the dynamics of the output time series into account", "keywords": ["semisupervised learning", "example-based tracking", "manifold learning", "nonlinear system identification"]}
{"id": "kp20k_training_334", "title": "Almost periodic solutions to abstract semilinear evolution equations with Stepanov almost periodic coefficients", "abstract": "In this paper, almost periodicity of the abstract semilinear evolution equation u'(t) = A(t)u(t) f(t, u(t)) with Stepanov almost periodic coefficients is discussed. We establish a new composition theorem of Stepanov almost periodic functions; and, with its help, we study the existence and uniqueness of almost periodic solutions to the above semilinear evolution equation. Our results are even new for the case of A(t) equivalent to A", "keywords": ["almost periodic", "stepanov almost periodic", "semilinear evolution equations", "banach space"]}
{"id": "kp20k_training_335", "title": "phoenix-based clone detection using suffix trees", "abstract": "A code clone represents a sequence of statements that are duplicated in multiple locations of a program. Clones often arise in source code as a result of multiple cut/paste operations on the source, or due to the emergence of crosscutting concerns. Programs containing code clones can manifest problems during the maintenance phase. When a fault is found or an update is needed on the original copy of a code section, all similar clones must also be found so that they can be fixed or updated accordingly. The ability to detect clones becomes a necessity when performing maintenance tasks. However, if done manually, clone detection can be a slow and tedious activity that is also error prone. A tool that can automatically detect clones offers a significant advantage during software evolution. With such an automated detection tool, clones can be found and updated in less time. Moreover, restructuring or refactoring of these clones can yield better performance and modularity in the program.This paper describes an investigation into an automatic clone detection technique developed as a plug-in for Microsoft's new Phoenix framework. Our investigation finds function-level clones in a program using abstract syntax trees (ASTs) and suffix trees. An AST provides the structural representation of the code after the lexical analysis process. The AST nodes are used to generate a suffix tree, which allows analysis on the nodes to be performed rapidly. We use the same methods that have been successfully applied to find duplicate sections in biological sequences to search for matches on the suffix tree that is generated, which in turn reveal matches in the code", "keywords": ["software analysis", "suffix trees", "clone detection", "code clones"]}
{"id": "kp20k_training_336", "title": "Slimeware: Engineering Devices with Slime Mold", "abstract": "The plasmodium of the acellular slime mold Physarum polycephalum is a gigantic single cell visible to the unaided eye. The cell shows a rich spectrum of behavioral patterns in response to environmental conditions. In a series of simple experiments we demonstrate how to make computing, sensing, and actuating devices from the slime mold. We show how to program living slime mold machines by configurations of repelling and attracting gradients and demonstrate the workability of the living machines on tasks of computational geometry, logic, and arithmetic", "keywords": ["parallel biological computers", "amorphous computers", "living technology", "slime mold"]}
{"id": "kp20k_training_337", "title": "Automated aspect-oriented decomposition of process-control systems for ultra-high dependability assurance", "abstract": "This paper presents a method for decomposing process-control systems. This decomposition method is automated, meaning that a series of principles that can be evolved to support automated tools are given to help a designer decompose complex systems into a collection of simpler components. Each component resulting from the decomposition process can be designed and implemented independently of the other components. Also, these components can be tested or verified by the end-user independently of each other. Moreover, the system properties, such as safety, stability, and reliability, can be mathematically inferred from the properties of the individual components. These components are referred to as IDEAL ( Independently Developable End-user Assessable Logical) components. This decomposition method is applied to a case study specified by the High-Integrity Systems group at Sandia National Labs, which involves the control of a future version of the Bay Area Rapid Transit ( BART) system", "keywords": ["software decomposition", "dependability assurance", "process-control systems", "aspect-oriented modeling"]}
{"id": "kp20k_training_338", "title": "Diffusion-Confusion Based Light-Weight Security for Item-RFID Tag-Reader Communication", "abstract": "In this paper we propose a challenge-response protocol called: DCSTaR, which takes a novel approach to solve security issues that are specific to low-cost item-RFID tags. Our DCSTaR protocol is built upon light-weight primitives such as 16 bit: Random Number Generator, Exclusive-OR, and Cyclic Redundancy Check and utilizing these primitives it also provides a simple Diffusion-Confusion cipher to encrypt the challenge and response from the tag to the RFID reader. As a result our protocol achieves RFID tag-reader-server mutual authentication, communicating-data confidentiality and integrity, secure key-distribution and key-protection. It also provides an efficient way for consumers to verify whether tagged items are genuine or fake and to protect consumers' privacy while carrying tagged items", "keywords": ["rfid", "tag-reader communication security", "light-weight cryptography", "customer privacy", "diffusion-confusion cipher", "epcglobal class-1 gen-2"]}
{"id": "kp20k_training_339", "title": "On solutions of functional-integral equations of Urysohn type on an unbounded interval", "abstract": "In this paper we establish the existence of solutions of functional-integral and quadratic Urysohn integral on the interval R(+) = [0, infinity). The technique of proving applied in this paper is based on the concept of measure of noncompactness and the fixed point theorem. Some new results are given.  ", "keywords": ["nonlinear integral equation", "measure of noncompactness", "fixed point theorem"]}
{"id": "kp20k_training_340", "title": "a cultural probes study on video sharing and social communication on the internet", "abstract": "The focus of this article is the link between video sharing and interpersonal communication on the internet. Previous works on social television systems belong to two categories: 1) studies on how collocated groups of viewers socialize while watching TV, and 2) studies on novel Social TV applications (e.g. experimental set-ups) and devices (e.g. ambient displays) that provide technological support for TV sociability over a distance. The main shortcoming of those studies is that they have not considered the dominant contemporary method of Social TV. Early adopters of technology have been watching and sharing video online. We employed cultural probes in order to gain in-depth information about the social aspect of video sharing on the internet. Our sample consisted of six heavy users of internet video, watching an average of at least one hour of internet video a day. In particular, we explored how they are integrating video into their daily social communication practices. We found that internet video is shared and discussed with distant friends. Moreover, the results of the study indicate several opportunities and threats for the development of integrated mass and interpersonal communication applications and services", "keywords": ["cultural probes", "online communication", "user study", "internet video"]}
{"id": "kp20k_training_341", "title": "Phenotypic Modulation of Vascular Smooth Muscle Cells", "abstract": "The smooth muscle myosin heavy chain (MHC) gene and its isoforms are excellent molecular markers that reflect smooth muscle phenotypes. The SMemb/Nonmuscle Myosin Heavy Chain B (NMHC-B) is a distinct MHC gene expressed predominantly in phenotypically modulated SMCs (synthetic-type SMC). To dissect the molecular mechanisms governing phenotypic modulation of SMCs, we analyzed the transcriptional regulatory mechanisms underlying expression of the SMemb gene. We previously reported two transcription factors, BTEB2/IKLF and Hex, which transactivate the SMemb gene promoter based on the transient reporter transfection assays. BTEB2/IKLF is a zinc finger transcription factor, whereas Hex is a homeobox protein. BTEB2/IKLF expression in SMCs is downregulated with vascular development in vivo but upregulated in cultured SMCs and in neointima in response to vascular injury after balloon angioplasty. BTEB2/IKLF and Hex activate not only the SMemb gene but also other genes activated in synthetic SMCs including plasminogen activator inhibitor-1 (PAI-1), iNOS, PDGF-A, Egr-1, and VEGF receptors. Mitogenic stimulation activates BTEB2/IKLF gene expression through MEK1 and Egr-1. Elevation of intracellular cAMP is also important in phenotypic modulation of SMCs, because the SMemb promoter is activated under cooperatively by cAMP-response element binding protein (CREB) and Hex", "keywords": ["vascular smooth muscle cells", "phenotypic modulation"]}
{"id": "kp20k_training_342", "title": "Intent specifications: An approach to building human-centered specifications", "abstract": "This paper examines and proposes an approach to writing software specifications, based on research in systems theory, cognitive psychology, and human-machine interaction. The goal is to provide specifications that support human problem solving and the tasks that humans must perform in software development and evolution. A type of specification, called intent specifications, is constructed upon this underlying foundation", "keywords": ["requirements", "requirements specification", "safety-critical software", "software evolution", "human-centered specifications", "means-ends hierarchy", "cognitive engineering"]}
{"id": "kp20k_training_344", "title": "An improved evaluation of ladder logic diagrams and Petri nets for the sequence controller design in manufacturing systems", "abstract": "Sequence controller designs play a key role in advanced manufacturing systems. Traditionally, the ladder logic diagram (LLD) has been widely applied to programmable logic controllers (PLC), while recently the Petri net (PN) has emerged as an alternative tool for the sequence control of complex systems. The evaluation of both approaches has become crucial and has thus attracted attention", "keywords": ["ladder logic diagrams", "petri nets", "plc", "sequence controllers", "manufacturing systems"]}
{"id": "kp20k_training_345", "title": "Lightweight detection of node presence in MANETs", "abstract": "While mobility in the sense of node movement has been an intensively studied aspect of mobile ad hoc networks (MANETs), another aspect of mobility has not yet been subjected to systematic research: nodes may not only move around but also enter and leave the network. In fact, many proposed protocols for MANETs exhibit worst case behavior when an intended communication partner is currently not present. Therefore, knowing whether a given node is currently present in the network can often help to avoid unnecessary overhead. In this paper, we present a solution to the presence detection problem. It uses a Bloom filter-based beaconing mechanism to aggregate and distribute information about the presence of network nodes. We describe the algorithm and discuss design alternatives. We assess the algorithms properties both analytically and through simulation, and thereby underline the effectiveness and applicability of our approach", "keywords": ["presence detection", "mobile ad hoc networks", "manets", "soft state bloom filter"]}
{"id": "kp20k_training_346", "title": "An integrated toolchain for model based functional safety analysis", "abstract": "We design a complete toolchain for integrating fault tolerance analysis into modeling. The goal of this work is to bridge the gap between the different specialized tools available. Having an integrated environment will reduce errors, ensure coherence and simplify analysis", "keywords": ["bayesian networks", "safety analysis", "model-based design", "functional testing"]}
{"id": "kp20k_training_347", "title": "SCALE INVARIANT FEATURE MATCHING USING ROTATION-INVARIANT DISTANCE FOR REMOTE SENSING IMAGE REGISTRATION", "abstract": "Scale invariant feature transform (SIFT) has been widely used in image matching. But when SIFT is introduced in the registration of remote sensing images, the keypoint pairs which are expected to be matched are often assigned two different value of main orientation owing to the significant difference in the image intensity between remote sensing image pairs, and therefore a lot of incorrect matches of keypoints will appear. This paper presents a method using rotation-invariant distance instead of Euclid distance to match the scale invariant feature vectors associated with the keypoints. In the proposed method, the feature vectors are reorganized into feature matrices, and fast Fourier transform (FFT) is introduced to compute the rotation-invariant distance between the matrices. Much more correct matches are obtained by the proposed method since the rotation-invariant distance is independent of the main orientation of the keypoints. Experimental results indicate that the proposed method improves the match performance compared to other state-of-art methods in terms of correct match rate and aligning accuracy", "keywords": ["remote sensing image", "image registration", "sift", "main orientation", "feature matching", "rotation-invariance distance"]}
{"id": "kp20k_training_348", "title": "computer-related gender differences", "abstract": "Computer-related gender differences are examined using survey responses from 651 college students. Issues studied include gender differences regarding interest and enjoyment of both using a computer and computer programming. Interesting gender differences with implications for teaching are examined for the groups (family, teachers, friends, others) that have the most influence on students' interest in computers. Traditional areas such as confidence, career understanding and social bias are also discussed. Preliminary results for a small sample of technology majors indicate that computer majors have unique interests and attitudes compared to other science majors", "keywords": ["gender issues"]}
{"id": "kp20k_training_349", "title": "Analysis of EEG signals by combining eigenvector methods and multiclass support vector machines", "abstract": "A new approach based on the implementation of multiclass support vector machine (SVM) with the error correcting output codes (ECOC) is presented for classification of electroencephalogram (EEG) signals. In practical applications of pattern recognition, there are often diverse features extracted from raw data which needs recognizing. Decision making was performed in two stages: feature extraction by eigenvector methods and classification using the classifiers trained on the extracted features. The aim of the study is classification of the EEG signals by the combination of eigenvector methods and multiclass SVM. The purpose is to determine an optimum classification scheme for this problem and also to infer clues about the extracted features. The present research demonstrated that the eigenvector methods are the features which well represent the EEG signals and the multiclass SVM trained on these features achieved high classification accuracies", "keywords": ["multiclass support vector machine ", "eigenvector methods", "electroencephalogram  signals"]}
{"id": "kp20k_training_350", "title": "lambda-RBAC: PROGRAMMING WITH ROLE-BASED ACCESS CONTROL", "abstract": "We study mechanisms that permit program components to express role constraints on clients, focusing on programmatic security mechanism, which permit access controls to be expressed, in situ, as part of the code realizing basic functionality. In this setting, two questions immediately arise. (1) The user of a component faces the issue of safety: is a particular role sufficient to use the component? (2) The component designer faces the dual issue of protection: is a particular role demanded in all execution paths of the component? We provide a formal calculus and static analysis to answer both questions", "keywords": ["role-based access control", "lambda-calculus", "static analysis"]}
{"id": "kp20k_training_351", "title": "Output-only Modal Analysis using Continuous-Scan Laser Doppler Vibrometry and application to a 20kW wind turbine", "abstract": "Continuous-Scan Laser Doppler Vibrometry (CSLDV) is a technique where the measurement point continuously sweeps over a structure while measuring, capturing both spatial and temporal information. The continuous-scan approach can greatly accelerate measurements, allowing one to capture spatially detailed mode shapes in the same amount of time that conventional methods require to measure the response at a single point. The method is especially beneficial when testing large structures, such as wind turbines, that have low natural frequencies and hence may require very long time records at each measurement point. Several CSLDV methods have been presented that use sinusoidal excitation or impulse excitation, but CSLDV has not previously been employed with an unmeasured, broadband random input. This work extends CSLDV to that class of input, developing an Output-only Modal Analysis method (OMA-CSLDV). A recently developed algorithm for linear time-periodic system identification, which makes use of harmonic power spectra and the harmonic transfer function concept developed by Wereley [17], is used in conjunction with CSLDV measurements. One key consideration, the choice of the scan frequency, is explored. The proposed method is validated on a randomly excited free-free beam, where one-dimensional mode shapes are captured by scanning the laser along the length of the beam. The first seven natural frequencies and mode shapes are extracted from the harmonic power spectrum of the vibrometer signal and show good agreement with the analytically-derived modes of the beam. The method is then applied to identify the mode shapes of a parked 20kW wind turbine using a ground based laser and with only a light breeze providing excitation", "keywords": ["modal identification", "output-only modal analysis", "operational modal analysis", "laser doppler vibrometry", "periodically time varying"]}
{"id": "kp20k_training_352", "title": "Preferences in Wikipedia abstracts: Empirical findings and implications for automatic entity summarization", "abstract": "We empirically study how Wikipedians summarize entity descriptions in practice. We compare entity descriptions in DBpedia with their Wikipedia abstracts. We analyze the length of a summary and the priorities of property values. We analyze the priorities of, diversity of, and correlation between properties. Implications for automatic entity summarization are drawn from the findings", "keywords": ["dbpedia", "entity summarization", "feature selection", "property ranking", "wikipedia"]}
{"id": "kp20k_training_353", "title": "Multi-organ localization with cascaded global-to-local regression and shape prior", "abstract": "We propose a fast and robust method for multiple organs localization. Our method provides organ-dedicated confidence maps for each organ. It extends the cascade of random forest with additional shape prior. The values of the testing and learning parameters can be explained physically. We evaluate our method on 130 CT volumes and show its good accuracy", "keywords": ["multi-organ localization", "regression", "random forest", "3d ct", "abdominal organs"]}
{"id": "kp20k_training_354", "title": "Ordered interval routing schemes", "abstract": "An Interval Routing Scheme (IRS) represents the routing tables in a network in a space-efficient way by labeling each vertex with an unique integer address, and the outgoing edges at each vertex with disjoint subintervals of these addresses. An IRS that has at most k intervals per edge label is called a k-IRS. In this paper, we propose a new type of interval routing scheme, called an Ordered Interval Routing Scheme (OIRS), that uses an ordering of the outgoing edges at each vertex and allows non-disjoint intervals in the labels of those edges. We show for a number of graph classes that using an OIRS instead of an IRS reduces the size of the routing tables in the case of optimal routing, i.e., routing along shortest paths. We show that optimal routing in any k-tree is possible using an OIRS with at most 2k?1 2 k ? 1 intervals per edge label, although the best known result for an IRS is 2k+1 2 k + 1 intervals per edge label. Any torus has an optimal 1-OIRS, although it may not have an optimal 1-IRS. We present similar results for the Petersen graph, k-garland graphs and a few other graphs", "keywords": ["oirs", "interval routing", "routing table"]}
{"id": "kp20k_training_355", "title": "Performance analysis in non-Rayleigh and non-Rician communications channels", "abstract": "This paper investigates the probability of erasure for mobile communication channels containing limited number of scatterers. Two kinds of channels with and without line of sight are examined. The resultant data is depicted by graphs to express the differences in existing theoretical models more clearly. The results indicate that the probability of erasure is different from that of predicted by both Rayleigh and Rician models for small number of scatterers", "keywords": ["fading", "mobile communications", "non-rayleigh and non-rician channel"]}
{"id": "kp20k_training_356", "title": "Computational geometry column 41", "abstract": "The recent result that n congruent balls in R(d) have at most 4 distinct geometric permutations is described", "keywords": ["line transversal", "geometric permutation", "stabbing"]}
{"id": "kp20k_training_357", "title": "Trends of environmental information systems in the context of the European Water Framework Directive", "abstract": "In Europe, the development of Environmental Information Systems for the water domain is heavily influenced by the need to support the processes of the European Water Framework Directive (WFD). The aim of the WFD is to ensure that all European waters, these being groundwater, surface or coastal waters, are protected according to a common standard. While the WFD itself does only include concrete information technology (IT) recommendations on a very high-level of data exchange, regional and/or national environmental agencies build or adapt their information systems according to their specific requirements in order to deliver the results for the first WFD reporting phase on time. Moreover, as the WFD requires a water management policy centered on natural river basin districts instead of administrative and political regions, the agencies have to co-ordinate their work, possibly across national borders. On this background, the present article analyses existing IT recommendations for the WFD implementation strategy and motivates the need to develop an IT Framework Architecture that comprises different views such as an organisational, a process, a data and a functional view. After having presented representative functions of operational water body information systems for the thematic and the co-operation layer, the article concludes with a summary of future IT developments that are required to efficiently support the WFD implementation", "keywords": ["environmental information systems", "water framework directive", "eis", "wfd", "gml", "inspire", "gmes", "java", "ogc"]}
{"id": "kp20k_training_358", "title": "A finite volume method for viscous incompressible flows using a consistent flux reconstruction scheme", "abstract": "An incompressible Navier-Stokes solver using curvilinear body-fitted collocated grid has been developed to solve unconfined flow past arbitrary two-dimensional body geometries. In this solver, the full Navier-Stokes equations have been solved numerically in the physical plane itself without using any transformation to the computational plane. For the proper coupling of pressure and velocity field on collocated grid, a new scheme, designated 'consistent flux reconstruction' (CFR) scheme, has been developed. In this scheme, the cell face centre velocities are obtained explicitly by solving the momentum equations at the centre of the cell faces. The velocities at the cell centres are also updated explicitly by solving the momentum equations at the cell centres. By resorting to such a fully explicit treatment considerable simplification has been achieved compared to earlier approaches. In the present investigation the solver has been applied to unconfined flow past a square cylinder at zero and non-zero incidence at low and moderate Reynolds numbers and reasonably good agreement has been obtained with results available from literature. ", "keywords": ["curvilinear collocated grid", "incompressible navier-stokes solver", "finite volume method", "physical plane", "explicit-explicit scheme", "consistent flux reconstruction"]}
{"id": "kp20k_training_359", "title": "practical online retrieval evaluation", "abstract": "Online evaluation is amongst the few evaluation techniques available to the information retrieval community that is guaranteed to reflect how users actually respond to improvements developed by the community. Broadly speaking, online evaluation refers to any evaluation of retrieval quality conducted while observing user behavior in a natural context. However, it is rarely employed outside of large commercial search engines due primarily to a perception that it is impractical at small scales. The goal of this tutorial is to familiarize information retrieval researchers with state-of-the-art techniques in evaluating information retrieval systems based on natural user clicking behavior, as well as to show how such methods can be practically deployed. In particular, our focus will be on demonstrating how the Interleaving approach and other click based techniques contrast with traditional offline evaluation, and how these online methods can be effectively used in academic-scale research. In addition to lecture notes, we will also provide sample software and code walk-throughs to showcase the ease with which Interleaving and other click-based methods can be employed by students, academics and other researchers", "keywords": ["interleaving", "preference judgments", "web search", "clickthrough data", "online evaluation"]}
{"id": "kp20k_training_360", "title": "A privacy-preserving clustering approach toward secure and effective data analysis for business collaboration", "abstract": "The sharing of data has been proven beneficial in data mining applications. However, privacy regulations and other privacy concerns may prevent data owners from sharing information for data analysis. To resolve this challenging problem, data owners must design a solution that meets privacy requirements and guarantees valid data clustering results. To achieve this dual goal, we introduce a new method for privacy-preserving clustering called Dimensionality Reduction-Based Transformation (DRBT). This method relies on the intuition behind random projection to protect the underlying attribute values subjected to cluster analysis. The major features of this method are: (a) it is independent of distance-based clustering algorithms; (b) it has a sound mathematical foundation; and (c) it does not require CPU-intensive operations. We show analytically and empirically that transforming a data set using DRBT, a data owner can achieve privacy preservation and get accurate clustering with a little overhead of communication cost", "keywords": ["privacy-preserving data mining", "privacy-preserving clustering", "dimensionality reduction", "random projection", "privacy-preserving clustering over centralized data", "privacy-preserving clustering over vertically partitioned data"]}
{"id": "kp20k_training_361", "title": "Unified read requests", "abstract": "Most work on multimedia storage systems has assumed that clients will be serviced using a round-robin strategy. The server services the clients in rounds and each client is allocated a time slice within that round. Furthermore, most such algorithms are evaluated on the basis of a tightly specified cost function. This is the basis for well known algorithms such as FCFS, SCAN, SCAN-EDF, etc. In this paper, we describe a Request Merging (RM) module that takes as input, a set of client requests, and a set of constraints on the desired performance such as client waiting time or maximum disk bandwidth, and a cost function. It produces as output, a Unified Read Request (URR), telling the storage server which data items to read, and when the clients would like these data items to be delivered to them. Given a cost function cf, a URR is optimal if there is no other URR satisfying the constraints with a lower cost. We present three algorithms in this paper, each of which accomplishes this kind of request merging. The first algorithm OptURR is guaranteed to produce minimal cost URRs with respect to arbitrary cost functions. In general, the problem of computing an optimal URR is NP-complete, even when only two data objects are considered. To alleviate this problem, we develop two other algorithms, called GreedyURR and FastURR that may produce sub-optimal URRs, but which have some nicer computational properties. We will report on the pros and cons of these algorithms through an experimental evaluation", "keywords": ["multimedia storage server", "request merging", "optimality", "cost function"]}
{"id": "kp20k_training_362", "title": "Brain-Computer Evolutionary Multiobjective Optimization: A Genetic Algorithm Adapting to the Decision Maker", "abstract": "The centrality of the decision maker (DM) is widely recognized in the multiple criteria decision-making community. This translates into emphasis on seamless human-computer interaction, and adaptation of the solution technique to the knowledge which is progressively acquired from the DM. This paper adopts the methodology of reactive search optimization (RSO) for evolutionary interactive multiobjective optimization. RSO follows to the paradigm of \"learning while optimizing,\" through the use of online machine learning techniques as an integral part of a self-tuning optimization scheme. User judgments of couples of solutions are used to build robust incremental models of the user utility function, with the objective to reduce the cognitive burden required from the DM to identify a satisficing solution. The technique of support vector ranking is used together with a k-fold cross-validation procedure to select the best kernel for the problem at hand, during the utility function training procedure. Experimental results are presented for a series of benchmark problems", "keywords": ["interactive decision making", "machine learning", "reactive search optimization", "support vector ranking"]}
{"id": "kp20k_training_363", "title": "Linear Separability of Gene Expression Data Sets", "abstract": "We study simple geometric properties of gene expression data sets, where samples are taken from two distinct classes (e.g., two types of cancer). Specifically, the problem of linear separability for pairs of genes is investigated. If a pair of genes exhibits linear separation with respect to the two classes, then the joint expression level of the two genes is strongly correlated to the phenomena of the sample being taken from one class or the other. This may indicate an underlying molecular mechanism relating the two genes and the phenomena (e. g., a specific cancer). We developed and implemented novel efficient algorithmic tools for finding all pairs of genes that induce a linear separation of the two sample classes. These tools are based on computational geometric properties and were applied to 10 publicly available cancer data sets. For each data set, we computed the number of actual separating pairs and compared it to an upper bound on the number expected by chance and to the numbers resulting from shuffling the labels of the data at random empirically. Seven out of these 10 data sets are highly separable. Statistically, this phenomenon is highly significant, very unlikely to occur at random. It is therefore reasonable to expect that it manifests a functional association between separating genes and the underlying phenotypic classes", "keywords": ["gene expression analysis", "dna microarrays", "diagnosis", "linear separation"]}
{"id": "kp20k_training_364", "title": "A language for representing and extracting 3D geometry semantics from paper-based sketches", "abstract": "The key contribution is a visual language to formally represent form geometry semantics on paper. Parsing the language allows for the automatic generation of 3D virtual models. A proof-of-concept prototype tool was implemented. The language is capable to roughly model forms with linear topological ordering. Evaluation results show that practising designers would use the language", "keywords": ["humancomputer interaction", "computer-aided sketching", "3d modelling"]}
{"id": "kp20k_training_365", "title": "Decentralized list scheduling", "abstract": "Classical list scheduling is a very popular and efficient technique for scheduling jobs for parallel and distributed platforms. It is inherently centralized. However, with the increasing number of processors, the cost for managing a single centralized list becomes too prohibitive. A suitable approach to reduce the contention is to distribute the list among the computational units: each processor only has a local view of the work to execute. Thus, the scheduler is no longer greedy and standard performance guarantees are lost", "keywords": ["scheduling", "list algorithms", "work stealing"]}
{"id": "kp20k_training_366", "title": "Antenna impedance matching with neural networks", "abstract": "Impedance matching between transmission lines and antennas is an important and fundamental concept in electromagnetic theory. One definition of antenna impedance is the resistance and reactance seen at the antenna terminals or the ratio of electric to magnetic fields at the input. The primary intent of this paper is real-time compensation for changes in the driving point impedance of an antenna due to frequency deviations. In general, the driving point impedance of an antenna or antenna array is computed by numerical methods such as the method of moments or similar techniques. Some configurations do lend themselves to analytical solutions, which will be the primary focus of this work. This paper employs a neural control system to match antenna feed lines to two common antennas during frequency sweeps. In practice, impedance matching is performed off-line with Smith charts or relatively complex formulas but they rarely perform optimally over a large bandwidth. There have been very few attempts to compensate for matching errors while the transmission system is in operation and most techniques have been targeted to a relatively small range of frequencies. The approach proposed here employs three small neural networks to perform real-time impedance matching over a broad range of frequencies during transmitter operation. Double stub tuners are being explored in this paper but the approach can certainly be applied to other methodologies. The ultimate purpose of this work is the development of an inexpensive microcontroller-based system", "keywords": ["impedance matching", "control system", "vswr"]}
{"id": "kp20k_training_367", "title": "Cellular Automata over Group Alphabets: Undergraduate Education and the PascGalois Project", "abstract": "This purpose of this note is to report efforts underway in the PascGalois Project (www.pascgalois.org) to provide connections between standard courses in the undergraduate mathematics curriculum (e.g. abstract algebra, number theory, discrete mathematics) and cellular automata. The value of these connections to the mathematical education of undergraduates will be described. Project course supplements, supporting software, and areas of student research will also be summarized", "keywords": ["pascgalois project", "pascgalois je", "group alphabets", "fractal dimensions", "growth rate dimensions", "abstract algebra", "undergraduate research"]}
{"id": "kp20k_training_368", "title": "from structured documents to novel query facilities", "abstract": "Structured documents (e.g., SGML) can benefit a lot from database support and more specifically from object-oriented database (OODB) management systems. This paper describes a natural mapping from SGML documents into OODB's and a formal extension of two OODB query languages (one SQL-like and the other calculus) in order to deal with SGML document retrieval. Although motivated by structured documents, the extensions of query languages that we present are general and useful for a variety of other OODB applications. A key element is the introduction of paths as first class citizens. The new features allow to query data (and to some extent schema) without exact knowledge of the schema in a simple and homogeneous fashion", "keywords": ["applications", "order", "document retrieval", "structure", "systems", "formalism", "object-oriented database", "sql", "query languages", "data", "support", "map", "schema", "extensibility", "general", "knowledge", "paper", "feature", "documentation", "database", "management", "class", "query"]}
{"id": "kp20k_training_369", "title": "Determination of Oxidized Low-Density Lipoproteins (ox-LDL) versus ox-LDL/?2GPI Complexes for the Assessment of Autoimmune-Mediated Atherosclerosis", "abstract": "The immunolocalization of oxidized low-density lipoproteins (ox-LDL), ?2-glycoprotein I (?2GPI), CD4+/CD8+ immunoreactive lymphocytes, and immunoglobulins in atherosclerotic lesions strongly suggested an active participation of the immune system in atherogenesis. Oxidative stress leading to ox-LDL production is thought to play a central role in both the initiation and progression of atherosclerosis. ox-LDL is highly proinflammatory and chemotactic for macrophage/monocyte and immune cells. Enzyme-linked immunosorbent assays (ELISAs) to measure circulating ox-LDL have been developed and are being currently used to assess oxidative stress as risk factor or marker of atherosclerotic disease. ox-LDL interacts with ?2GPI and circulating ox-LDL/?2GPI complexes have been demonstrated in patients with systemic lupus erythematosus (SLE) and antiphospholipid syndrome (APS). It has been postulated that ?2GPI binds ox-LDL to neutralize its proinflammatory and proatherosclerotic effects. Because ?2GPI is ubiquitous in plasma, its interaction with ox-LDL may mask oxidized epitopes recognized by capture antibodies potentially interfering with immunoassays results. The measurement of ox-LDL/?2GPI complexes may circumvent this interference representing a more physiological and accurate way of measuring ox-LDL", "keywords": ["oxidized low-density lipoprotein ", "2-glycoprotein i (?2gpi", "oxidative stress", "elisa", "atherosclerosis", "autoimmunity"]}
{"id": "kp20k_training_370", "title": "Text document clustering based on frequent word meaning sequences", "abstract": "Most of existing text clustering algorithms use the vector space model, which treats documents as bags of words. Thus, word sequences in the documents are ignored, while the meaning of natural languages strongly depends on them. In this paper, we propose two new text clustering algorithms, named Clustering based on Frequent Word Sequences (CFWS) and Clustering based on Frequent Word Meaning Sequences (CFWMS). A word is the word form showing in the document, and a word meaning is the concept expressed by synonymous word forms. A word (meaning) sequence is frequent if it occurs in more than certain percentage of the documents in the text database. The frequent word (meaning) sequences can provide compact and valuable information about those text documents. For experiments, we used the Reuters-21578 text collection, CISI documents of the Classic data set [Classic data set, ftp://ftp.cs.cornell.edu/pub/smart/], and a corpus of the Text Retrieval Conference (TREC) [High Accuracy Retrieval from Documents (HARD) Track of Text Retrieval Conference, 2004]. Our experimental results show that CFWS and CFWMS have much better clustering accuracy than Bisecting k-means (BKM) [M. Steinbach, G. Karypis, V. Kumar, A Comparison of Document Clustering Techniques, KDD-2000 Workshop on Text Mining, 2000], a modified bisecting k-means using background knowledge (BBK) [A. Hotho, S. Staab, G. Stumme, Ontologies improve text document clustering, in: Proceedings of the 3rd IEEE International Conference on Data Mining, 2003, pp. 541544] and Frequent Itemset-based Hierarchical Clustering (FIHC) [B.C.M. Fung, K. Wang, M. Ester, Hierarchical document clustering using frequent itemsets, in: Proceedings of SIAM International Conference on Data Mining, 2003] algorithms", "keywords": ["text documents", "clustering", "frequent word sequences", "frequent word meaning sequences", "web search", "wordnet"]}
{"id": "kp20k_training_371", "title": "an online approach based on locally weighted learning for short-term traffic flow prediction", "abstract": "Traffic flow prediction is a basic function of Intelligent Transportation System. Due to the complexity of traffic phenomenon, most existing methods build complex models such as neural networks for traffic flow prediction. As a model may lose effect with time lapse, it is important to update the model on line. However, the high computational cost of maintaining a complex model puts great challenge for model updating. The high computation cost lies in two aspects: computation of complex model coefficients and huge amount training data for it. In this paper, we propose to use a nonparametric approach based on locally weighted learning to predict traffic flow. Our approach incrementally incorporates new data to the model and is computationally efficient, which makes it suitable for online model updating and predicting. In addition, we adopt wavelet analysis to extract the periodic characteristic of the traffic data, which is then used for the input of the prediction model instead of the raw traffic flow data. The primary experiments on real data demonstrate the effectiveness and efficiency of our approach", "keywords": ["prediction", "online locally weighted learning", "traffic", "real time"]}
{"id": "kp20k_training_372", "title": "usable computing on open distributed systems", "abstract": "An open distributed system provides a best-effort guarantee on the quality of service provided to applications. This has worked well for throughput-based applications of the kind typically executed in Condor or BOINCstyle environments. For other applications, the absence of timeliness of correctness guarantees limit the utility or appeal of this environment. Computational results that are too late or erroneous are not usable to the application. We present techniques designed to efficiently promote usable computing in open distributed systems", "keywords": ["autonomic computing", "grid computing", "computing paradigm"]}
{"id": "kp20k_training_373", "title": "A note on the not 3-choosability of some families of planar graphs", "abstract": "A graph G is L-list colorable if for a given list assignment L = {L(v): v epsilon V}, there exists a proper coloring c of G such that c(v) epsilon L(v) for all v epsilon V. If G is L-list colorable for any list assignment with vertical bar L(v)vertical bar >= k for all v epsilon V, then G is said k-choosable. In [M. Voigt, A not 3-choosable planar graph without 3-cycles, Discrete Math. 146 (1995) 325-328] and [M. Voigt, A non-3-choosable planar graph without cycles of length 4 and 5, 2003, Manuscript], Voigt gave a planar graph without 3-cycles and a planar graph without 4-cycles and 5-cycles which are not 3-choosable. In this note, we give smaller and easier graphs than those proposed by Voigt and suggest an extension of Erdos' relaxation of Steinberg's conjecture to 3-choosability.  ", "keywords": ["combinatorial problems", "coloring", "list-coloring", "choosability"]}
{"id": "kp20k_training_374", "title": "Impedance spectroscopy studies of moisture uptake in low-k dielectrics and its relation to reliability", "abstract": "Water incursion into low-k BEOL capacitors was monitored via impedance spectroscopy. It is a non-destructive, zero DC field, low AC field probe (<0.5V). Samples are tested at device operation conditions and are re-testable. Thermal activation energies related to water bonding with dielectric are measured. The increase in AC loss is correlated with poorer reliability, i.e. early failure", "keywords": ["low-k", "impedance spectroscopy", "dielectric relaxation", "ac losses", "time dependent dielectric breakdown", "reliability"]}
{"id": "kp20k_training_375", "title": "A multi-level depiction method for painterly rendering based on visual perception cue", "abstract": "Increasing the level of detail (LOD) in brushstrokes within areas of interest improved the realism of painterly rendering. Using a modified quad-tree, we segmented an image into areas with similar levels of saliency; each of these segments was then used to control the brush strokes during rendering. We could also simulate real oil painting steps based on saliency information. Our method runs in a reasonable fine and produces results that are visually appealing and competitive with previous techniques", "keywords": ["non-photorealistic rendering", "painting technique", "image saliency"]}
{"id": "kp20k_training_376", "title": "Preventive replacement for systems with condition monitoring and additional manual inspections", "abstract": "Researched a problem of both condition monitoring and inspection. Defined two types of preventive replacements. Utilized the delay time concept to model the failure process. Formulated a decision problem of two decision variables simultaneously", "keywords": ["maintenance", "condition monitoring", "inspection", "delay-time", "two-stage failure process"]}
{"id": "kp20k_training_377", "title": "Redundant and force-differentiated systems in engineering and nature", "abstract": "Sophisticated load-carrying structures, in nature as well as man-made, share some common properties. A clear differentiation of tension, compression and shear is in nature primarily manifested in the properties of materials adapted to the efforts, whereas they in engineering are distributed on different components. For stability and failure safety, redundancy on different levels is also commonly used. The paper aims at collecting and expanding previous methods for the computational treatment of redundant and force-differentiated systems. A common notation is sought, giving and developing criteria for describing the diverse problems from a common structural mechanical viewpoint. From this, new criteria for the existence of solutions, and a method for treatment of targeted dynamic solutions are developed. Added aspects to previously described examples aim at emphasizing similarities and differences between engineering and nature, in the forms of a tension truss structure and the human musculoskeletal system", "keywords": ["structures", "equilibrium", "statics", "dynamics", "mechanisms", "redundancy", "target control"]}
{"id": "kp20k_training_378", "title": "Simultaneous optimization of the material properties and the topology of functionally graded structures", "abstract": "A level set based method is proposed for the simultaneous optimization of the material properties and the topology of functionally graded structures. The objective of the present study is to determine the optimal material properties (via the material volume fractions) and the structural topology to maximize the performance of the structure in a given application. In the proposed method, the volume fraction and the structural boundary are considered as the design variables, with the former being discretized as a scalar field and the latter being implicitly represented by the level set method. To perform simultaneous optimization, the two design variables are integrated into a common objective functional. Sensitivity analysis is conducted to obtain the descent directions. The optimization process is then expressed as the solution to a coupled HamiltonJacobi equation and diffusion partial differential equation. Numerical results are provided for the problem of mean compliance optimization in two dimensions", "keywords": ["topology optimization", "level set method", "dynamic implicit boundary", "functionally graded materials", "heterogeneous objects"]}
{"id": "kp20k_training_379", "title": "The impact of head movements on user involvement in mediated interaction", "abstract": "We examine engagement within conversational behaviours of the subject when interacting with a socially expressive system. We found real-time communication requires more than verbal communication, and head nodding. Head nodding effects depend on precise on-screen movement by synchronize the on-screen movement with the head movement", "keywords": ["engagement", "nonverbal behaviours", "head movements", "face-to-face interaction", "telepresence robot"]}
{"id": "kp20k_training_380", "title": "Parsing images into regions, curves, and curve groups", "abstract": "In this paper, we present an algorithm for parsing natural images into middle level vision representations-regions, curves, and curve groups (parallel curves and trees). This algorithm is targeted for an integrated solution to image segmentation and curve grouping through Bayesian inference. The paper makes the following contributions. (1) It adopts a layered (or 2. 1 D-sketch) representation integrating both region and curve models which compete to explain an input image. The curve layer occludes the region layer and curves observe a partial order occlusion relation. (2) A Markov chain search scheme Metropolized Gibbs Samplers (MGS) is studied. It consists of several pairs of reversible jumps to traverse the complex solution space. An MGS proposes the next state within the jump scope of the current state according to a conditional probability like a Gibbs sampler and then accepts the proposal with a Metropolis-Hastings step. This paper discusses systematic design strategies of devising reversible jumps for a complex inference task. (3) The proposal probability ratios in jumps are factorized into ratios of discriminative probabilities. The latter are computed in a bottom-up process, and they drive the Markov chain dynamics in a data-driven Markov chain Monte Carlo framework. We demonstrate the performance of the algorithm in experiments with a number of natural images", "keywords": ["image segmentation", "perceptual organization", "curve grouping", "graph partition", "data-driven markov chain monte carlo", "metropolized gibbs sampler"]}
{"id": "kp20k_training_381", "title": "Laparoscopic Management of Adnexal Masses", "abstract": "Suspected ovarian neoplasm is a common clinical problem affecting women of all ages. Although the majority of adnexal masses are benign, the primary goal of diagnostic evaluation is the exclusion of malignancy. It has been estimated that approximately 510% of women in the United States will undergo a surgical procedure for a suspected ovarian neoplasm during their lifetime. Despite the magnitude of the problem, there is still considerable disagreement regarding the optimal surgical management of these lesions. Traditional management has relied on laparotomy to avoid undertreatment of a potentially malignant process. Advances in detection, diagnosis, and minimally invasive surgical techniques make it necessary now to review this practice in an effort to avoid unnecessary morbidity among patients. Here, we review the literature on the laparosopic approach to the treatment of the adnexal mass without sacrificing the principles of oncologic surgery. We highlight potentials of minimally invasive surgery and address the risks associated with the laparoscopic approach", "keywords": ["adnexal masses", "ovarian neoplasm", "laparotomy"]}
{"id": "kp20k_training_382", "title": "Dealing with plagiarism in the information systems research community: A look at factors that drive plagiarism and ways to address them", "abstract": "Imagine yourself spending years conducting a research project and having it published as an article in a refereed journal, only to see a plagiarized copy of the article later published in another journal. Then imagine yourself being left to fight for your rights alone, and eventually finding out that it would be very difficult to hold the plagiarist accountable for what he or she did. The recent decision by the Association of Information Systems to create a standing committee on member misconduct suggests that while this type of situation may sound outrageous, it is likely to become uncomfortably frequent in the information systems research community if proper measures are not taken by a community-backed organization. In this article, we discuss factors that can drive plagiarism, as well as potential measures to prevent it. Our goal is to discuss alternative ways in which plagiarism can be prevented and dealt with when it arises. We hope to start a debate that provides the basis on which broader mechanisms to deal with plagiarism can be established, which we envision as being associated with and complementary to the committee created by the Association for Information Systems", "keywords": ["ethics", "committees", "community", "plagiarism", "information systems research"]}
{"id": "kp20k_training_383", "title": "Percolation in the secrecy graph", "abstract": "The secrecy graph is a random geometric graph which is intended to model the connectivity of wireless networks under secrecy constraints. Directed edges in the graph are present whenever a node can talk to another node securely in the presence of eavesdroppers, which, in the model, is determined solely by the locations of the nodes and eavesdroppers. In the case of infinite networks, a critical parameter is the maximum density of eavesdroppers that can be accommodated while still guaranteeing an infinite component in the network, i.e., the percolation threshold. We focus on the case where the locations of the nodes and eavesdroppers are given by Poisson point processes, and present bounds for different types of percolation, including in-, out- and undirected percolation", "keywords": ["percolation", "branching process", "secrecy graph"]}
{"id": "kp20k_training_384", "title": "Evaluation of Region-of-Interest coders using perceptual image quality assessments", "abstract": "Perceptual image assessment is proposed for coder performance evaluation. Proposed assessment uses a linear combination of perceptual measures just based on features. Region-of-Interest coder perceptual evaluation aims at identifying coder behavior. Some perceptual assessments are adequate to evaluate test coders", "keywords": ["region-of-interest", "image coding", "wavelet", "distortion measure", "quality assessment", "perceptual evaluation", "human visual system", "mean-observed scores", "rate-distortion function"]}
{"id": "kp20k_training_385", "title": "achieving anycast in dtns by enhancing existing unicast protocols", "abstract": "Many DTN environments, such as emergency response networks and pocket-switched networks, are based on human mobility and communication patterns, which naturally lead to groups. In these scenarios, group-based communication is central, and hence a natural and useful routing paradigm is anycast, where a node attempts to communicate with at least one member of a particular group. Unfortunately, most existing anycast solutions assume connectivity, and the few specifically for DTNs are single-copy in nature and have only been evaluated in highly limited mobility models. In this paper, we propose a protocol-independent method of enhancing a large number of existing DTN unicast protocols, giving them the ability to perform anycast communication. This method requires no change to the unicast protocols themselves and instead changes their world view by adding a thin layer beneath the routing layer. Through a thorough set of simulations, we also evaluate how different parameters and network conditions affect the performance of these newly transformed anycast protocols", "keywords": ["routing", "anycast", "dtn"]}
{"id": "kp20k_training_386", "title": "a framework for supporting data integration using the materialized and virtual approaches", "abstract": "This paper presents a framework for data integration currently under development in the Squirrel project. The framework is based on a special class of mediators, called Squirrel integration mediators. These mediators can support the traditional virtual and materialized approaches, and also hybrids of them.In the Squirrel mediators, a relation in the integrated view can be supported as (a) fully materialized, (b) fully virtual, or (c) partially materialized (i.e., with some attributes materialized and other attributes virtual). In general, (partially) materialized relations of the integrated view are maintained by incremental updates from the source databases. Squirrel mediators provide two approaches for doing this: (1) materialize all needed auxiliary data, so that data sources do not have to be queried when processing the incremental updates; or (2) leave some or all of the auxiliary data virtual, and query selected source databases when processing incremental updates.The paper presents formal notions of consistency and \"freshness\" for integrated views defined over multiple autonomous source databases. It is shown that Squirrel mediators satisfy these properties", "keywords": [" framework ", "views", "formalism", "developer", "process", "data", "project", "mediator", "support", "general", "attributes", "relation", "consistency", "paper", "virtualization", "database", "autonomic", "hybrid", "update", "query", "data integrity", "class", "integrability", "incremental"]}
{"id": "kp20k_training_387", "title": "Validation and verification of intelligent systems - what are they and how are they different", "abstract": "Researchers and practitioners in the field of expert systems all generally agree that to be useful, any fielded intelligent system must be adequately verified and validated. But what does this mean in concrete terms? What exactly is verification? What exactly is validation? How are they different? Many authors have attempted to define these terms and, as a result, several interpretations have surfaced. It is our opinion that there is great confusion as to what these terms mean. how they are different, and how they are implemented. This paper. therefore, has two aims-to clarify the meaning of the terms validation and verification as they apply to intelligent systems, and to describe how several researchers are implementing these. The second part of the paper, therefore, details some techniques that can be used to perform the verification and validation of systems. Also discussed is the role of testing as part of the above-mentioned processes", "keywords": ["validation", "verification", "evaluation", "expert systems", "intelligent systems"]}
{"id": "kp20k_training_388", "title": "Multiple blocking sets and multisets in Desarguesian planes", "abstract": "In AG(2, q (2)), the minimum size of a minimal (q - 1)-fold blocking set is known to be q (3) - 1. Here, we construct minimal (q - 1)-fold blocking sets of size q (3) in AG(2, q (2)). As a byproduct, we also obtain new two-character multisets in PG(2, q (2)). The essential idea in this paper is to investigate q (3)-sets satisfying the opposite of Ebert's discriminant condition", "keywords": ["multiple blocking set", "multiset"]}
{"id": "kp20k_training_389", "title": "A simple weighting scheme for classification in two-group discriminant problems", "abstract": "This paper introduces a new weighted linear programming model, which is simple and has strong intuitive appeal for two-group classifications. Generally, in applying weights to solve a classification problem in discriminant analysis where the relative importance of every observation is known, larger weights (penalties) will be assigned to those more important observations. The perceived importance of an observation is measured here as the willingness of the decision-maker to misclassify this observation. For instance, a decision-maker is least willing to see a classification rule that misclassifies a top financially strong firm to the group that contains bankrupt firms. Our weighted-linear programming model provides an objective-weighting scheme whereby observations can be weighted according to their perceived importance. The more important this observation, the heavier its assigned weight. Results of a simulation experiment that uses contaminated data show that the weighted linear programming model consistently and significantly outperforms existing linear programming and standard statistical approaches in attaining higher average hit-ratios in the 100 replications for each of the 27 cases tested. Scope and purpose Generally, in applying weights to solve a discriminant problem where the relative importance of every observation is known, larger weights (penalties) will be assigned to those more important observations. However, if decision-makers do not have prior or additional information about the observations, it is very difficult to assign weights to the observations. Subjective judgements from decision-makers may be a way of obtaining those weights. An alternative way is to suggest an objective weighting scheme for obtaining classification weights of observations from the data matrix of the training sample. We suggest a new approach, which provides an objective weighting scheme whereby individual observations can be weighted according to their perceived importance. The more important the observation, the heavier its assigned weight will be. The importance of individual observation is first determined in one of two stages of our model using more than one discriminant function. Simulation experiments are run to test this new approach", "keywords": ["classification", "discriminant analysis", "linear programming", "statistics"]}
{"id": "kp20k_training_390", "title": "HYBRID INTELLIGENT PACKING SYSTEM (HIPS) THROUGH INTEGRATION OF ARTIFICIAL NEURAL NETWORKS, ARTIFICIAL-INTELLIGENCE, AND MATHEMATICAL-PROGRAMMING", "abstract": "A successful solution to the packing problem is a major step toward material savings on the scrap that could be avoided in the cutting process and therefore money savings. Although the problem is of great interest, no satisfactory algorithm has been found that can be applied to all the possible situations. This paper models a Hybrid Intelligent Packing System (HIPS) by integrating Artificial Neural Networks (ANNs), Artificial Intelligence (AI), and Operations Research (OR) approaches for solving the packing problem. The HIPS consists of two main modules, an intelligent generator module and a tester module. The intelligent generator module has two components: (i) a rough assignment module and (ii) a packing module. The rough assignment module utilizes the expert system and rules concerning cutting restrictions and allocation goals in order to generate many possible patterns. The packing module is an ANN that packs the generated patterns and performs post-solution adjustments. The tester module, which consists of a mathematical programming model, selects the sets of patterns that will result in a minimum amount of scrap", "keywords": ["cutting and packing", "parallel processing", "data driven", "connectionist", "extensional programming"]}
{"id": "kp20k_training_391", "title": "Distributed Scheduling and Resource Allocation for Cognitive OFDMA Radios", "abstract": "Scheduling spectrum access and allocating power and rate resources are tasks affecting critically the performance of wireless cognitive radio (CR) networks. The present contribution develops a primal-dual optimization framework to schedule any-to-any CR communications based on orthogonal frequency division multiple access and allocate power so as to maximize the weighted average sum-rate of all users. Fairness is ensured among CR communicators and possible hierarchies are respected by guaranteeing minimum rate requirements for primary users while allowing secondary users to access the spectrum opportunistically. The framework leads to an iterative channel-adaptive distributed algorithm whereby nodes rely only on local information exchanges with their neighbors to attain global optimality. Simulations confirm that the distributed online algorithm does not require knowledge of the underlying fading channel distribution and converges to the optimum almost surely from any initialization", "keywords": ["cognitive radios", "resource allocation", "quality of service", "distributed online implementation"]}
{"id": "kp20k_training_392", "title": "physically based hydraulic erosion simulation on graphics processing unit", "abstract": "Visual simulation of natural erosion on terrains has always been a fascinating research topic in the field of computer graphics. While there are many algorithms already developed to improve the visual quality of terrain, the recent simulation methods revolve around physically-based hydraulic erosion because it can generate realistic natural-looking terrains. However, many of such algorithms were tested only on low resolution terrains. When simulated on a higher resolution terrain, most of the current algorithms become computationally expensive. This is why in many applications today, terrains are generated off-line and loaded during the application runtime. This method restricts the number of terrains which can be stored if there is a limitation on storage capacity. Recently, graphics hardware has evolved into an indispensable tool in improving the speed of computation. This has motivated us to develop an erosion algorithm to map to graphics hardware for faster terrain generation. In this paper, we propose a fast and efficient hydraulic erosion procedural technique that utilizes the GPUs powerful computation capability in order to generate high resolution erosion on terrains. Our method is based on the Newtonian physics approach that is implemented on a two-dimensional data structure which stores height fields, water amount, and dissolved sediment and water velocities. We also present a comprehensive comparison between the CPU and GPU implementations together with the visual results and the statistics on simulation time taken", "keywords": ["terrain", "physically based modeling", "natural phenomena", "visual simulation", "hydraulic erosion"]}
{"id": "kp20k_training_393", "title": "novel immune-based framework for securing ad hoc networks", "abstract": "One of the main security issues in mobile ad hoc networks (MANETs) is a malicious node that can falsify a route advertisement, overwhelm traffic without forwarding it, help to forward corrupted data and inject false or uncompleted information, and many other security problems. Mapping immune system mechanisms to networking security is the main objective of this paper which may significantly contribute in securing MANETs. In a step for providing secured and reliable broadband services, formal specification logic along with a novel immuneinspired security framework (I 2 MANETs) are introduced. The different immune components are synchronized with the framework through an agent that has the ability to replicate, monitor, detect, classify, and block/isolate the corrupted packets and/or nodes in a federated domain. The framework functions as the Human Immune System in first response, second response, adaptability, distributability, and survivability and other immune features and properties. Interoperability with different routing protocols is considered. The framework has been implemented in a real environment. Desired and achieved results are presented", "keywords": ["security", "manets", "specification logic", "mobile agent"]}
{"id": "kp20k_training_394", "title": "Slabpose columnsort: A new oblivious algorithm for out-of-core sorting on distributed-memory clusters", "abstract": "Our goal is to develop a robust out-of-core sorting program for a distributed-memory cluster. The literature contains two dominant paradigms for out-of-core sorting algorithms: merging-based and partitioning-based. We explore a third paradigm, that of oblivious algorithms. Unlike the two dominant paradigms, oblivious algorithms do not depend on the input keys and therefore lead to predetermined I/O and communication patterns in an out-of-core setting. Predetermined I/O and communication patterns facilitate overlapping I/O, communication, and computation for efficient implementation. We have developed several out-of-core sorting programs using the paradigm of oblivious algorithms. Our baseline implementation, 3-pass columnsort, was based on Leighton's columnsort algorithm. Though efficient in terms of I/O and communication, 3-pass columnsort has a restriction on the maximum problem size. As our first effort toward relaxing this restriction, we developed two implementations: subblock columnsort and M-columnsort. Both of these implementations incur substantial performance costs: subblock columnsort performs additional disk I/O, and M-columnsort needs substantial amounts of extra communication and computation. In this paper we present slabpose columnsort, a new oblivious algorithm that we have designed explicitly for the out-of-core setting. Slabpose columnsort relaxes the problem-size restriction at no extra I/O or communication cost. Experimental evidence on a Beowulf cluster shows that unlike subblock columnsort and M-columnsort, slabpose columnsort runs almost as fast as 3-pass columnsort. To the best of our knowledge, our implementations are the first out-of-core multiprocessor sorting algorithms that make no assumptions about the keys and produce output that is perfectly load balanced and in the striped order assumed by the Parallel Disk Model", "keywords": ["columnsort", "out-of-core", "parallel sorting", "distributed-memory cluster", "oblivious algorithms"]}
{"id": "kp20k_training_395", "title": "A real-time kinematics on the translational crawl motion of a quadruped robot", "abstract": "It is known that the kinematics of a quadruped robot is complex due to its topology and the redundant actuation in the robot. However, it is fundamental to compute the inverse and direct kinematics for the sophisticated control of the robot in real-time. In this paper, the translational crawl gait of a quadruped robot is introduced and the approach to find the solution of the kinematics for such a crawl motion is proposed. Since the resulting kinematics is simplified, the formulation can be used for the real-time control of the robot. The results of simulation and experiment shows that the present method is feasible and efficient", "keywords": ["crawl velocity", "joint position", "joint velocity", "quadruped robot", "real-time kinematics", "trajectory of center-of-gravity", "translational crawl gait"]}
{"id": "kp20k_training_396", "title": "Geographical classification of olive oils by the application of CART and SVM to their FT-IR", "abstract": "This paper reports the application of Fourier-transform infrared (FT-IR) spectroscopy to the geographical classification of extra virgin olive oils. Two chemometrical techniques, classification and regression trees (CART) and support vector machines (SVM) based on the Gaussian kernel and the recently introduced Euclidean distance-based Pearson VII Universal Kernel (PUK), were applied to discriminate between Italian and non-Italian and between Ligurian and non-Ligurian olive oils. The PUK is applied in literature with success on regression problems. In this paper the mapping power of this universal kernel for classification was investigated. In this study it was observed that SVM performed better than CART. SVM based on the PUK provide models with a high selectivity and sensitivity (thus a better accuracy) as compared to those obtained using the Gaussian kernel. The wave numbers selected in the classification trees were interpreted demonstrating that the trees were chemically justified. This study also shows that FT-IR spectroscopy associated with SVM and CART can be used to correctly discriminate between various origins of olive oils, demonstrating that the combination of techniques might be a powerful tool for supporting the claimed origin of olive oils. ", "keywords": ["ft-ir", "olive oil", "classification and regression trees", "support vector machines"]}
{"id": "kp20k_training_397", "title": "ARFNNs under Different Types SVR for Identification of Nonlinear Magneto-Rheological Damper Systems with Outliers", "abstract": "This paper demonstrates different types support vector regression (SVR) for annealing robust fuzzy neural networks (ARFNNs) to identification of nonlinear magneto-rheological (MR) damper with outliers. A SVR has the good performances to determine the number of rule in the simplified fuzzy inference system and initial weights for the fuzzy neural networks. In this paper, we independently proposed two different types SVR for the ARFNNs. Hence, a combination model that fuses simplified fuzzy inference system, SVR and radial basis function networks is used. Based on these initial structures, and then annealing robust learning algorithm (ARLA) can be used effectively to adjust the parameters of structures. Simulation results show the superiority of the proposed method with the different types SVR for the nonlinear MR damper systems with outliers", "keywords": ["magneto-rheological damper", "fuzzy neural networks", "support vector regression", "annealing robust learning algorithm"]}
{"id": "kp20k_training_398", "title": "Fuzzy linear regression model based on fuzzy scalar product", "abstract": "The new concept and method of imposing imprecise (fuzzy) input and output data upon the conventional linear regression model is proposed in this paper. We introduce the fuzzy scalar (inner) product to formulate the fuzzy linear regression model. In order to invoke the conventional approach of linear regression analysis for real-valued data, we transact the alpha-level linear regression models of the fuzzy linear regression model. We construct the membership functions of fuzzy least squares estimators via the form of \"Resolution Identity\" which is a well-known formula in fuzzy sets theory. In order to obtain the membership value of any given least squares estimate taken from the fuzzy least squares estimator, we transform the original problem into the optimization problems. We also provide two computational procedures to solve the optimization problems", "keywords": ["fuzzy number", "fuzzy linear regression model", "fuzzy scalar  product", "least squares estimator", "optimization"]}
{"id": "kp20k_training_399", "title": "Relating torque and slip in an odometric model for an autonomous agricultural vehicle", "abstract": "This paper describes a method of considering the slip that is experienced by the wheels of an agricultural autonomous guided vehicle such that the accuracy of dead-reckoning navigation may be improved. Traction models for off-road locomotion are reviewed. Using experimental data from an agricultural AGV, a simplified form suitable for vehicle navigation is derived. This simplified model relates measurements of the torques applied to the wheels with wheel slip, and is used as the basis of an observation model for odometric sensor data in the vehicle's extended Kalman filter (EKF) navigation system. The slip model parameters are included as states in the vehicle EKF so that the vehicle may adapt to changing surface properties. Results using real field data and a simulation of the vehicle EKF show that positional accuracy can be increased by a slip-aware odometric model, and that when used as part of a multi-sensor navigation system, the consistency of the EKF state estimator is improved", "keywords": ["navigation", "kalman filter", "odometry", "slip", "traction"]}
{"id": "kp20k_training_400", "title": "Evaluation of Folksonomy Induction Algorithms", "abstract": "Algorithms for constructing hierarchical structures from user-generated metadata have caught the interest of the academic community in recent years. In social tagging systems, the output of these algorithms is usually referred to as folksonomies (from folk-generated taxonomies). Evaluation of folksonomies and folksonomy induction algorithms is a challenging issue complicated by the lack of golden standards, lack of comprehensive methods and tools as well as a lack of research and empirical/simulation studies applying these methods. In this article, we report results from a broad comparative study of state-of-the-art folksonomy induction algorithms that we have applied and evaluated in the context of five social tagging systems. In addition to adopting semantic evaluation techniques, we present and adopt a new technique that can be used to evaluate the usefulness of folksonomies for navigation. Our work sheds new light on the properties and characteristics of state-of-the-art folksonomy induction algorithms and introduces a new pragmatic approach to folksonomy evaluation, while at the same time identifying some important limitations and challenges of folksonomy evaluation. Our results show that folksonomy induction algorithms specifically developed to capture intuitions of social tagging systems outperform traditional hierarchical clustering techniques. To the best of our knowledge, this work represents the largest and most comprehensive evaluation study of state-of-the-art folksonomy induction algorithms to date", "keywords": ["algorithms", "experimentation", "folksonomies", "taxonomies", "evaluation", "social tagging systems"]}
{"id": "kp20k_training_401", "title": "TELEPORTATION OF N-QUDIT STATE", "abstract": "In this paper, we study the teleportation of arbitrary N-qudit state with the tensor representation. The necessary and sufficient condition for realizing a successful or perfect teleportation is obtained, as will be shown, which is determined by the measurement matrix T-delta and the quantum channel parameter matrix X. The general expressions of the measurement matrix T-delta are written out and the quantum channel parameter matrix X are discussed. As an example, we show the details of three-ququart state teleportation", "keywords": ["qudit state", "ququart state", "channel parameter matrix ", "measurement matrix", "transformation matrix"]}
{"id": "kp20k_training_402", "title": "Bio-Interactive Healthcare Service System Using Lifelog Based Context Computing", "abstract": "Intelligent bio-sensor information processing was developed using lifelog based context aware technology to provide a flexible and dynamic range of diagnostic capabilities to satisfy healthcare requirements in ubiquitous and mobile computing environments. To accomplish this, various noise signals were grouped into six categories by context estimation and effectively reconfigured noise reduction filters by neural network and genetic algorithm. The neural network-based control module effectively selected an optimal filter block by noise context-based clustering in running mode, and filtering performance was improved by genetic algorithm in evolution mode. Due to its adaptive criteria, genetic algorithm was used to explore the action configuration for each identified bio-context to implement our concept. Our proposed Bio-interactive healthcare service system adopts the concepts of biological context-awareness with evolutionary computations in working environments modeled and identified as bio-sensors based environmental contexts. We used an unsupervised learning algorithm for lifelog based context modeling and a supervised learning algorithm for context identification", "keywords": ["biometric interaction", "context awareness", "interactive healthcare"]}
{"id": "kp20k_training_403", "title": "SMB: Collision detection based on temporal coherence", "abstract": "The paper presents a novel collision detection algorithm, termed the sort moving boxes (SMB) for large number of moving 2D/3D objects which are represented by their axis-aligned bounding boxes (AABBs). The main feature of the algorithm is the full exploitation of the temporal coherence of the objects exhibited in a dynamic environment. In the algorithm, the AABBs are first projected to each Cartesian axis. The projected intervals on the axes are separately sorted by the diminishing increment sort (DIS) and further divided into subsections. By processing all the intervals within the subsections to check if they overlap, a complete contact list can be built. The SMB is a fast and robust collision detection algorithm, particularly for systems involving a large number of moving AABBs, and also supports for the dynamic insertion and deletion of objects. Its performance in terms of both expected total detection time and memory requirements is proportional to the total number of AABBs, N, and is not influenced by size differences of AABBs, the space size and packing density over a large range up to ten times difference. The only assumption made is that the sorted list at one time step will remain an almost sorted list at the next time step, which is valid for most applications whose movement and deformation of each AABB and the dynamic change of the total number N are approximately continuous", "keywords": ["collision detection", "contact search", "sort", "axis-aligned bounding boxes ", "moving", "temporal coherence"]}
{"id": "kp20k_training_404", "title": "An international analysis of the extensions to the IEEE LOMv1.0 metadata standard", "abstract": "We analyzed 44 works using the IEEE LOMv1.0 standard and found 15 types of extensions made to it. Due to Mexico interoperability difficulties, we compared its extensions with the rest of the world. We found that local extensions do not help to increase the system's interoperability ability. We found the action most important after implementing extensions is to publish them", "keywords": ["metadata", "learning objects", "interoperability", "extensions", "ieee lomv1.0 standard", "metadata application profiles"]}
{"id": "kp20k_training_406", "title": "An architectural history of metaphors", "abstract": "This paper presents a review and an historical perspective on the architectural metaphor. It identifies common characteristics and peculiaritiesas they apply to given historical periodsand analyses the similarities and divergences. The review provides a vocabulary, which will facilitate an appreciation of existing and new metaphors", "keywords": ["metaphor", "architecture", "art", "traditional or classical art", "ancient prehistoric", "modern and contemporary architecture"]}
{"id": "kp20k_training_407", "title": "time-based query performance predictors", "abstract": "Query performance prediction is aimed at predicting the retrieval effectiveness that a query will achieve with respect to a particular ranking model. In this paper, we study query performance prediction for a ranking model that explicitly incorporates the time dimension into ranking. Different time-based predictors are proposed as analogous to existing keyword-based predictors. In order to improve predicting performance, we combine different predictors using linear regression and neural networks. Extensive experiments are conducted using queries and relevance judgments obtained by crowdsourcing", "keywords": ["time-aware ranking", "query performance prediction"]}
{"id": "kp20k_training_408", "title": "Asymptotically sufficient partitions and quantizations", "abstract": "We consider quantizations of observations represented by finite partitions of observation spaces. Partitions usually decrease the sensitivity of observations to their probability distributions. A sequence of quantizations is considered to be asymptotically sufficient for a statistical problem if the loss of sensitivity is asymptotically negligible. The sensitivity is measured by f-divergences of distributions or the closely related f-informations including the classical Shannon information. It is demonstrated that in some cases the maximization of f-divergences means the same as minimization of distortion of observations in the classical sense considered in mathematical statistics and information theory. The main result of the correspondence is a general sufficient condition ford the asymptotic sufficiency of quantizations. Selected applications of this condition are studied leading to new simple criteria of asymptotic optimality for quantizations of vector-valued observations and observations on general Poisson processes", "keywords": ["abstract observation spaces", "asymptotically sufficient partitions", "asymptotically sufficient quantizations", "euclidean observation spaces", "f-divergences", "f-informations", "general poisson processes", "optimal quantizations", "sufficient statistics"]}
{"id": "kp20k_training_409", "title": "The topology aware file distribution problem", "abstract": "We present theoretical results for large-file distribution on general networks of known topology (known link bandwidths and router locations). We show that the problem of distributing a file in minimum time is NP-hard in this model, and we give an O(log n) approximation algorithm, where n is the number of workstations that require the file. We also characterize our method as optimal amongst the class of \"no-link-sharing\" algorithms", "keywords": ["network", "file distribution", "approximation"]}
{"id": "kp20k_training_410", "title": "Achieving quality assurance functionality in the food industry using a hybrid case-based reasoning and fuzzy logic approach", "abstract": "Quality control of food inventories in the warehouse is complex as well as challenging due to the fact that food can easily deteriorate. Currently, this difficult storage problem is managed mostly by using a human dependent quality assurance and decision making process. This has however, occasionally led to unimaginative, arduous and inconsistent decisions due to the injection of subjective human intervention into the process. Therefore, it could be said that current practice is not powerful enough to support high-quality inventory management. In this paper, the development of an integrative prototype decision support system, namely, Intelligent Food Quality Assurance System (IFQAS) is described which will assist the process by automating the human based decision making process in the quality control of food storage. The system, which is composed of a Case-based Reasoning (CBR) engine and a Fuzzy rule-based Reasoning (FBR) engine, starts with the receipt of incoming food inventory. With the CBR engine, certain quality assurance operations can be suggested based on the attributes of the food received. Further of this, the FBR engine can make suggestions on the optimal storage conditions of inventory by systematically evaluating the food conditions when the food is receiving. With the assistance of the system, a holistic monitoring in quality control of the receiving operations and the storage conditions of the food in the warehouse can be performed. It provides consistent and systematic Quality Assurance Guidelines for quality control which leads to improvement in the level of customer satisfaction and minimization of the defective rate", "keywords": ["food quality", "case-based reasoning", "fuzzy logic", "decision support system", "operation guidelines", "storage conditions"]}
{"id": "kp20k_training_411", "title": "Coverage and connectivity in three-dimensional underwater sensor networks", "abstract": "Unlike a terrestrial network. an underwater sensor network call have significant height which makes it a three-dimensional network. There are many important sensor network design problems where the physical dimensionality of the network plays it significant role. One Such problem is determining how to deploy minimum number of sensor nodes so that all points inside the network is within the sensing range of at least one sensor and all sensor nodes call communicate with each other, possibly over a multi-hop path. The solution to this problem depends oil the ratio of the communication ran-e and the sensing range of each sensor. Under sphere-based communication and sensing model, placing a node at the center of each virtual cell created by truncated octahedron-based tessellation solves this problem when this ratio is greater than 1.7889. However, for smaller values of this ratio, the solution depends on how much communication redundancy the network needs. We provide Solutions for both limited and full communication redundancy requirements. ", "keywords": ["three-dimensional", "coverage", "connectivity", "polyhedron", "node placement", "sphere-based sensing and communication"]}
{"id": "kp20k_training_412", "title": "error correction of voicemail transcripts in scanmail", "abstract": "Despite its widespread use, voicemail presents numerous usability challenges: People must listen to messages in their entirety, they cannot search by keywords, and audio files do not naturally support visual skimming. SCANMail overcomes these flaws by automatically generating text transcripts of voicemail messages and presenting them in an email-like interface. Transcripts facilitate quick browsing and permanent archive. However, errors from the automatic speech recognition (ASR) hinder the usefulness of the transcripts. The work presented here specifically addresses these problems by evaluating user-initiated error correction of transcripts. User studies of two editor interfaces-a grammar-assisted menu and simple replacement by typing-reveal reduced audio playback times and an emphasis on editing important words with the menu, suggesting its value in mobile environments where limited input capabilities are the norm and user privacy is essential. The study also adds to the scarce body of work on ASR confidence shading, suggesting that shading may be more helpful than previously reported", "keywords": ["speech recognition", "voicemail", "editor interfaces", "confidence shading", "error correction"]}
{"id": "kp20k_training_413", "title": "Study of stress waves in geomedia and effect of a soil cover layer on wave attenuation using a 1-D finite-difference method", "abstract": "The propagation and attenuation of blast-induced stress waves differs between geomedia such as rock or soil mass. This paper numerically studies the propagation and attenuation of blast-induced elastoplastic waves in deep geomedia by using a one-dimensional (I-D) finite-difference code. Firstly, the elastoplastic Cap models for rock and soil masses are introduced into the governing equations of spherical wave motion and a FORTRAN code based on the finite difference method is developed. Secondly, an underground spherical blast is simulated with this code and verified by software, RENEWTO. The propagation of stress-waves in rock and soil masses is numerically investigated, respectively. Finally, the effect of a soil cover layer on the attenuation of stress waves in the rear rock mass is studied. It is determined that large plastic deformation of geomedia can effectively dissipate the energy of stress-waves inward and the developed I-D finite difference code coupled with elastoplastic Cap models is convenient and effective in the numerical simulations for underground spherical explosion.  ", "keywords": ["geomedia", "elastoplastic cap model", "stress-waves", "soil cover layer", "attenuation", "finite difference method"]}
{"id": "kp20k_training_414", "title": "Keyed hash function based on a dynamic lookup table of functions", "abstract": "In this paper, we present a novel keyed hash function based on a dynamic lookup table of functions. More specifically, we first exploit the piecewise linear chaotic map (PWLCM) with secret keys used for producing four 32-bit initial buffers and then elaborate the lookup table of functions used for selecting composite functions associated with messages. Next, we convert the divided message blocks into ASCII code values, check the equivalent indices and then find the associated composite functions in the lookup table of functions. For each message block, the four buffers are reassigned by the corresponding composite function and then the lookup table of functions is dynamically updated. After all the message blocks are processed, the final 128-bit hash value is obtained by cascading the last reassigned four buffers. Finally, we evaluate our hash function and the results demonstrate that the proposed hash algorithm has good statistical properties, strong collision resistance, high efficiency, and better statistical performance compared with existing chaotic hash functions", "keywords": ["chaos", "keyed hash function", "piecewise linear chaotic map", "lookup table of functions", "transfer function", "composite function"]}
{"id": "kp20k_training_415", "title": "Exploring hierarchical multidimensional data with unified views of distribution and correlation", "abstract": "Data analysts explore data by inspecting features such as clustering, distribution and correlation. Much existing research has focused on different visualisations for different data exploration tasks. For example, a data analyst might inspect clustering and correlation with scatterplots, but use histograms to inspect a distribution. Such visualisations allow an analyst to confirm prior expectations. For example, a scatterplot may confirm an expected correlation or may show deviations from the expected correlation. In order to better facilitate discovery of unexpected features in data, however, a combination of different perspectives may be needed. In this paper, we combine distributional and correlational views of hierarchical multidimensional data. Our unified view supports the simultaneous exploration of data distribution and correlation. By presenting a unified view, we aim to increase the chances of discovery of unexpected data features, and to provide the means to explore such features in detail. Further, our unified view is equipped with a small number of primitive interaction operators which a user composes to facilitate smooth and flexible exploration.  ", "keywords": ["data analysis", "multidimensional data", "data distribution", "correlation"]}
{"id": "kp20k_training_416", "title": "Application driven network-on-chip architecture exploration & refinement for a complex SoC", "abstract": "This article presents an overview of the design process of an interconnection network, using the technology proposed by Arteris. Section 2 summarizes the various features a NoC is required to implement to be integrated in modern SoCs. Section 3 describes the proposed top-down approach, based on the progressive refinement of the NoC description, from its functional specification (Sect. 4) to its verification (Sect. 8). The approach is illustrated by a typical use-case of a NoC embedded in a hand-held gaming device. The methodology relies on the definition of the performance behavior and expectation (Sect. 5), which can be early and efficiently simulated against various NoC architectures. The system architect is then able to identify bottle-necks and converge towards the NoC implementation fulfilling the requirements of the target application (Sect. 6", "keywords": ["multimedia system-on-chip ", "network-on-chip ", "memory-mapped transaction interconnect", "dynamic memory scheduling", "quality-of-service ", "performance verification", "architecture exploration", "systemc transaction level modeling "]}
{"id": "kp20k_training_417", "title": "Real-valued MVDR beamforming using spherical arrays with frequency invariant characteristic", "abstract": "Complex-valued minimum variance distortionless response (MVDR) beamforming for wideband signals has very high computational amount. In this paper, we design a novel real-valued MVDR beamformer for spherical arrays. The dependence of the array steering matrix on source signal directions and frequencies is decoupled using spherical harmonic decomposition. Then a compensation network is designed to solve the frequency dependence of the array response and to get a new array response only determined by the spherical harmonics of the source directions. All frequency bins of wideband signals can be used together instead of being processed independently. By exploiting the property of the conjugate spherical harmonics, a unitary transform can be found to acquire a real-valued frequency invariant steering matrix (FISM). Based on the FISM, real-valued MVDR (RV-MVDR) is developed to obtain good performance with low computational amount. Simulation results demonstrate the performance of our proposed method for beamforming and direction-of-arrival (DOA) estimation by comparing with the complex-valued and real-weighted MVDR methods", "keywords": ["spherical arrays", "spherical harmonic decomposition", "real-valued minimum variance distortionless response ", "frequency invariant beamforming", "unitary transform"]}
{"id": "kp20k_training_418", "title": "PRIVATE DATABASE QUERIES USING QUANTUM STATES WITH LIMITED COHERENCE TIMES", "abstract": "We describe a method for private database queries using exchange of quantum states with bits encoded in mutually incompatible bases. For technology with limited coherence time, the database vendor can announce the encoding after a suitable delay to allow the user to privately learn one of two items in the database without the ability to also definitely infer the second item. This quantum approach also allows the user to choose to learn other functions of the items, such as the exclusive-or of their bits, but not to gain more information than equivalent to learning one item, on average. This method is especially useful for items consisting of a few bits by avoiding the substantial overhead of conventional cryptographic approaches", "keywords": ["quantum computing", "private data access", "digital property rights"]}
{"id": "kp20k_training_419", "title": "Scheduling for information gathering on sensor network", "abstract": "We investigate a unique wireless sensor network scheduling problem in which all nodes in a cluster send exactly one packet to a designated sink node in an effort to minimize transmission time. However, node transmissions must be sufficiently isolated either in time or in space to avoid collisions. The problem is formulated and solved via graph representation. We prove that an optimal transmission schedule can be obtained efficiently through a pipeline-like schedule when the underlying topology is either line or tree. The minimum time required for a line or tree topology with n nodes is 3(n-2). We further prove that our scheduling problem is NP-hard for general graphs. We propose a heuristic algorithm for general graphs. Our heuristic tries to schedule as many independent segments as possible to increase the degree of parallel transmissions. This algorithm is compared to an RTS/CTS based distributed algorithm. Preliminary simulated results indicate that our heuristic algorithm outperforms the RTS/CTS based distributed algorithm ( up to 30%) and exhibits stable behavior", "keywords": ["sensor network", "hybrid network", "scheduling", "all-to-one information gathering"]}
{"id": "kp20k_training_420", "title": "synchronization analysis and control in chaos system based on complex network", "abstract": "For a certain kind of complex network, Lorenz chaos system is used to describe the state equation of nodes in network. By constructing a Lyapunov function, it is proved that this network model can achieve synchronization under the adaptive control scheme. The control strategy is simple, effective and easy for the engineering design in the future. The simulation results show the effectiveness of control scheme", "keywords": ["synchronization", "chaos system", "adaptive control", "complex network"]}
{"id": "kp20k_training_421", "title": "Improved property in organic light-emitting diode utilizing two Al/Alq3 layers", "abstract": "We reported on the fabrication of organic light-emitting devices (OLEDs) utilizing the two Al/Alq3 layers and two electrodes. This novel green device with structure of Al(110nm)/tris(8-hydroxyquinoline) aluminum (Alq3)(65nm)/Al(110nm)/Alq3(50nm)/N,N?-dipheny1-N, N?-bis-(3-methy1phyeny1)-1, 1?-bipheny1-4, 4?-diamine (TPD)(60nm)/ITO(60nm)/Glass. TPD were used as holes transporting layer (HTL), and Alq3 was used as electron transporting layer (ETL), at the same time, Alq3 was also used as emitting layer (EL), Al and ITO were used as cathode and anode, respectively. The results showed that the device containing the two Al/Alq3 layers and two electrodes had a higher brightness and electroluminescent efficiency than the device without this layer. At current density of 14mA/cm2, the brightness of the device with the two Al/Alq3 layers reach 3693cd/m2, which is higher than the 2537cd/m2 of the Al/Alq3/TPD:Alq3/ITO/Glass device and the 1504.0cd/m2 of the Al/Alq3/TPD/ITO/Glass. Turn-on voltage of the device with two Al/Alq3 layers was 7V, which is lower than the others", "keywords": ["oleds", "emitting layer", "transporting layer"]}
{"id": "kp20k_training_422", "title": "Concept development for kindergarten children through a health simulation", "abstract": "According to many dental professionals, the decay process resulting from the accumulation of sugar on teeth is a very difficult concept for young children to learn. Playing the dental hygiene game with ThinkingTags not only brings context into the classroom, but also allows children to work with digital manipulatives that provide rich personal experiences and instant feedback. Instead of watching a demonstration of the accumulation of sugars on a computer screen, or being told about dental health, this simulation allows pre-school children to experience improving or decaying dental health without any real adverse health effects. Small, wearable, microprocessor-driven Tags were brought into the kindergarten classroom to simulate the decay process, providing information about sugars in foods and creating a discussion about teeth. Preliminary analyses suggest that this program was effective and enthusiastically received by this age group", "keywords": ["collaboration", "dialogue", "discourse analysis", "pre-school", "simulation", "wireless"]}
{"id": "kp20k_training_423", "title": "High output impedance current-mode four-function filter with reduced number of active and passive elements using the dual output current conveyor", "abstract": "This paper reports a new single-input multi-output current-mode multifunction filter which can simultaneously realise LP, HP, BP and BR filter functions all at high impedance outputs. The circuit permits orthogonal adjustment of quality factor Q and omega (0), employs only five grounded passive components and no element matching conditions are imposed. A second order all-pass function can easily be obtained. The passive sensitivities are shown to be low", "keywords": ["current conveyors", "multifunction filters", "current-mode circuits"]}
{"id": "kp20k_training_424", "title": "constraint programming for itemset mining", "abstract": "The relationship between constraint-based mining and constraint programming is explored by showing how the typical constraints used in pattern mining can be formulated for use in constraint programming environments. The resulting framework is surprisingly flexible and allows us to combine a wide range of mining constraints in different ways. We implement this approach in off-the-shelf constraint programming systems and evaluate it empirically. The results show that the approach is not only very expressive, but also works well on complex benchmark problems", "keywords": ["constraint programming", "itemset mining"]}
{"id": "kp20k_training_425", "title": "experiences mining open source release histories", "abstract": "Software releases form a critical part of the life cycle of a software project. Typically, each project produces releases in its own way, using various methods of versioning, archiving, announcing and publishing the release. Understanding the release history of a software project can shed light on the project history, as well as the release process used by that project, and how those processes change. However, many factors make automating the retrieval of release history information difficult, such as the many sources of data, a lack of relevant standards and a disparity of tools used to create releases. In spite of the large amount of raw data available, no attempt has been made to create a release history database of a large number of projects in the open source ecosystem. This paper presents our experiences, including the tools, techniques and pitfalls, in our early work to create a software release history database which will be of use to future researchers who want to study and model the release engineering process in greater depth", "keywords": ["release engineering", "data mining"]}
{"id": "kp20k_training_426", "title": "Balancing throughput and response time in online scientific Clouds via Ant Colony Optimization (SP2013/2013/00006", "abstract": "The Cloud Computing paradigm focuses on the provisioning of reliable and scalable infrastructures (Clouds) delivering execution and storage services. The paradigm, with its promise of virtually infinite resources, seems to suit well in solving resource greedy scientific computing problems. The goal of this work is to study private Clouds to execute scientific experiments coming from multiple users, i.e., our work focuses on the Infrastructure as a Service (IaaS) model where custom Virtual Machines (VM) are launched in appropriate hosts available in a Cloud. Then, correctly scheduling Cloud hosts is very important and it is necessary to develop efficient scheduling strategies to appropriately allocate VMs to physical resources. The job scheduling problem is however NP-complete, and therefore many heuristics have been developed. In this work, we describe and evaluate a Cloud scheduler based on Ant Colony Optimization (ACO). The main performance metrics to study are the number of serviced users by the Cloud and the total number of created VMs in online (non-batch) scheduling scenarios. Besides, the number of intra-Cloud network messages sent are evaluated. Simulated experiments performed using CloudSim and job data from real scientific problems show that our scheduler succeeds in balancing the studied metrics compared to schedulers based on Random assignment and Genetic Algorithms", "keywords": ["cloud computing", "scientific problems", "job scheduling", "swarm intelligence", "ant colony optimization", "genetic algorithms"]}
{"id": "kp20k_training_427", "title": "Exploring the CSCW spectrum using process mining", "abstract": "Process mining techniques allow for extracting information from event logs. For example, the audit trails of a workflow management system or the transaction logs of an enterprise resource planning system can be used to discover models describing processes, organizations, and products. Traditionally, process mining has been applied to structured processes. In this paper, we argue that process mining can also be applied to less structured processes supported by computer supported cooperative work (CSCW) systems. In addition, the ProM framework is described. Using ProM a wide variety of process mining activities are supported ranging from process discovery and verification to conformance checking and social network analysis", "keywords": ["process mining", "business activity monitoring", "business process intelligence", "cscw", "data mining"]}
{"id": "kp20k_training_428", "title": "Effects of spatial and temporal variation in environmental conditions on simulation of wildfire spread", "abstract": "Implementation of a wildfire spread model based on the level set method. Investigation of wildfire propagation under stochastic wind and fuel conditions. Local variation in combustion condition slows the rate of propagation. Local variation in wind direction is found to increase flank spread. A harmonic mean is preferential for spatially varying parameters in spread models", "keywords": ["perimeter propagation", "simulation", "modelling", "fire growth", "level set", "spark"]}
{"id": "kp20k_training_429", "title": "Utilization of spatial decision support systems decision-making in dryland agriculture: A Tifton burclover case study", "abstract": "FSAW delineated Wyoming agricultural land into relative ranks for burclover establishment. Defuzzification produced final output map with crisp scores and calculated centroid. Calculated centroid map demonstrated efficacy of SDSS in agricultural decision-making. Effective land suitability ranking validated value of ex-ante agricultural technologies. Presented information has potential to determine burclover feasibility in Wyoming", "keywords": ["gis geographic information systems", "idw inverse distance weighting", "fsaw fuzzy simple additive weighting", "madm multiple attribute decision-making", "mcdm multiple criteria decision-making", "sdss spatial decision support systems"]}
{"id": "kp20k_training_430", "title": "Propagation engine prototyping with a domain specific language", "abstract": "Constraint propagation is at the heart of constraint solvers. Two main trends co-exist for its implementation: variable-oriented propagation engines and constraint-oriented propagation engines. Those two approaches ensure the same level of local consistency but their efficiency (computation time) can be quite different depending on the instance solved. However, it is usually accepted that there is no best approach in general, and modern constraint solvers implement only one. In this paper, we would like to go a step further providing a solver independent language at the modeling stage to enable the design of propagation engines. We validate our proposal with a reference implementation based on the Choco solver and the MiniZinc constraint modeling language", "keywords": ["propagation", "constraint solver", "domain specific language", "implementation"]}
{"id": "kp20k_training_431", "title": "A Projection Pursuit framework for supervised dimension reduction of high dimensional small sample datasets", "abstract": "The analysis and interpretation of datasets with large number of features and few examples has remained as a challenging problem in the scientific community, owing to the difficulties associated with the curse-of-the-dimensionality phenomenon. Projection Pursuit (PP) has shown promise in circumventing this phenomenon by searching low-dimensional projections of the data where meaningful structures are exposed. However, PP faces computational difficulties in dealing with datasets containing thousands of features (typical in genomics and proteomics) due to the vast quantity of parameters to optimize. In this paper we describe and evaluate a PP framework aimed at relieving such difficulties and thus ease the construction of classifier systems. The framework is a two-stage approach, where the first stage performs a rapid compaction of the data and the second stage implements the PP search using an improved version of the SPP method (Guo et al., 2000, [32]). In an experimental evaluation with eight public microarray datasets we showed that some configurations of the proposed framework can clearly overtake the performance of eight well-established dimension reduction methods in their ability to pack more discriminatory information into fewer dimensions", "keywords": ["projection pursuit", "classification", "gene expression", "dimension reduction"]}
{"id": "kp20k_training_432", "title": "Conservation Functions for 1-D Automata: Efficient Algorithms, New Results, and a Partial Taxonomy", "abstract": "We present theorems that can be used for improved efficiency in the calculation of conservation functions for cellular automata. We report results obtained from implementations of algorithms based on these theorems that show conservation laws for 1-D cellular automata of higher order than any previously known. We introduce the notion of trivial and core conservation functions to distinguish truly new conservation functions from simple extensions of lower-order ones. We then present the complete list of conservation functions up to order 16 for the 256 elementary 1-d binary cellular automata. These include CAs that were not previously known to have nontrivial conservation functions", "keywords": ["cellular automata", "conservation functions", "linear algebra", "classification scheme", "taxonomy"]}
{"id": "kp20k_training_433", "title": "A reference bacterial genome dataset generated on the MinION portable single-molecule nanopore sequencer", "abstract": "The MinION is a new, portable single-molecule sequencer developed by Oxford Nanopore Technologies. It measures four inches in length and is powered from the USB 3.0 port of a laptop computer. The MinION measures the change in current resulting from DNA strands interacting with a charged protein nanopore. These measurements can then be used to deduce the underlying nucleotide sequence", "keywords": ["genomics", "nanopore sequencing"]}
{"id": "kp20k_training_434", "title": "Serial batching scheduling of deteriorating jobs in a two-stage supply chain to minimize the makespan", "abstract": "For the scheduling problem with a buffer, an optimal algorithm is developed for solving it. For the scheduling problem without buffer, some useful properties are derived. A heuristic is designed for solving it, and a novel lower bound is also derived. Two special cases are well analyzed, and two optimal algorithms are developed for solving them, respectively", "keywords": ["batch scheduling", "supply chain", "deterioration", "transportation", "heuristic"]}
{"id": "kp20k_training_435", "title": "Entanglement monotones and maximally entangled states in multipartite qubit systems", "abstract": "We present a method to construct entanglement measures for pure states of multipartite qubit systems. The key element of our approach is an antilinear operator that we call comb in reference to the hairy-ball theorem. For qubits (i.e. spin 1/2) the combs are automatically invariant under SL(2, C). This implies that the filters obtained from the combs are entanglement monotones by construction. We give alternative formulae for the concurrence and the 3-tangle as expectation values of certain antilinear operators. As an application we discuss inequivalent types of genuine four-, five- and six-qubit entanglement", "keywords": ["entanglement monotones", "multipartite entanglement", "antilineax operators"]}
{"id": "kp20k_training_436", "title": "Automatic verification of Java programs with dynamic frames", "abstract": "Framing in the presence of data abstraction is a challenging and important problem in the verification of object-oriented programs Leavens et al. (Formal Aspects Comput (FACS) 19:159-189, 2007). The dynamic frames approach is a promising solution to this problem. However, the approach is formalized in the context of an idealized logical framework. In particular, it is not clear the solution is suitable for use within a program verifier for a Java-like language based on verification condition generation and automated, first-order theorem proving. In this paper, we demonstrate that the dynamic frames approach can be integrated into an automatic verifier based on verification condition generation and automated theorem proving. The approach has been proven sound and has been implemented in a verifier prototype. The prototype has been used to prove correctness of several programming patterns considered challenging in related work", "keywords": ["program verification", "dynamic frames", "frame problem", "data abstraction"]}
{"id": "kp20k_training_437", "title": "Properties of the transmission of pulse sequences in a bistable chain of unidirectionally coupled neurons", "abstract": "We study the propagation of pulse sequences in a chain of neurons with sigmoidal inputoutput relations. The propagating speeds of pulse fronts depend on the widths of the preceding pulses and adjacent pulse fronts interact attractively. Sequences of pulse widths are then modulated through transmission. Equations for changes in pulse width sequences are derived with a kinematical model of propagating pulse fronts. The transmission of pulse width sequences in the chain is expressed as a linear system with additive noise. The gain of the system function increases exponentially with the number of neurons in a high-frequency region. The power spectrum of variations in pulse widths due to spatiotemporal noise also increases in the same manner. Further, the interaction between pulse fronts keeps the coherence and mutual information of initial and transmitted pulse sequences. Results of an experiment on an analog circuit confirm these properties", "keywords": ["transmission line", "chain of neurons", "pulse", "noise"]}
{"id": "kp20k_training_438", "title": "Building geometric feature based maps for indoor service robots", "abstract": "This paper presents an efficient geometric approach to the Simultaneous Localization and Mapping problem based on an Extended Kalman Filter. The map representation and building process is formulated, fully implemented and successfully experimented in different indoor environments with different robots. The use of orthogonal shape constraints is proposed to deal with the inconsistency of the estimation. Built maps are successfully used for the navigation of two different service robots: an interactive tour guide robot, and an assistive walking aid for the frail elderly", "keywords": ["simultaneous localization and mapping", "extended kalman filter", "inconsistency", "service robot"]}
{"id": "kp20k_training_439", "title": "The cross-entropy method with patching for rare-event simulation of large Markov chains", "abstract": "There are various importance sampling schemes to estimate rare event probabilities in Markovian systems such as Markovian reliability models and Jackson networks. In this work, we present a general state-dependent importance sampling method which partitions the state space and applies the cross-entropy method to each partition. We investigate two versions of our algorithm and apply them to several examples of reliability and queueing models. In all these examples we compare our method with other importance sampling schemes. The performance of the importance sampling schemes is measured by the relative error of the estimator and by the efficiency of the algorithm. The results from experiments show considerable improvements both in running time of the algorithm and the variance of the estimator", "keywords": ["cross-entropy", "rare events", "importance sampling", "large-scale markov chains"]}
{"id": "kp20k_training_440", "title": "Combined simulation for process control: extension of a general purpose simulation tool", "abstract": "Combined discrete event and continuous views of production processes are important in designing computer control systems for both process industries and manufacturing. The paper presents an extension of the popular Matlab-Simulink simulation tool to facilitate the simulation of the discrete sequential control logic applied to continuous processes. The control system is modelled as a combined system where the discrete and the continuous parts of the system are separated and an interface is introduced between them. The sequential control logic is represented by a Sequential Function Chart (SFC). A SFC blockset is defined to enable graphical composition of the SFC and its integration into the Simulink environment. A simulation mechanism is implemented which is called periodically from the standard Simulink simulation engine and carries out the correct state transition sequence of the discrete model and executes corresponding SFC actions. Two simulation case studies are given to illustrate the possible application of the developed simulation environment: the simulation of a batch process cell, as an example from the area of process control and an example of a manufacturing system, i.e. the control of a laboratory scale modular production system.  ", "keywords": ["simulation", "hybrid systems", "petri nets"]}
{"id": "kp20k_training_441", "title": "Action recognition feedback-based framework for human pose reconstruction from monocular images", "abstract": "A novel framework based on action recognition feedback for pose reconstruction of articulated human body from monocular images is proposed in this paper. The intrinsic ambiguity caused by perspective projection makes it difficult to accurately recover articulated poses from monocular images. To alleviate such ambiguity, we exploit the high-level motion knowledge as action recognition feedback to discard those implausible estimates and generate more accurate pose candidates using large number of motion constraints during natural human movement. The motion knowledge is represented by both local and global motion constraints. The local spatial constraint captures motion correlation between body parts by multiple relevance vector machines while the global temporal constraint preserves temporal coherence between time-ordered poses via a manifold motion template. Experiments on the CMU Mocap database demonstrate that our method performs better on estimation accuracy than other methods without action recognition feedback", "keywords": ["human pose reconstruction", "action recognition feedback", "motion correlation", "manifold motion template"]}
{"id": "kp20k_training_442", "title": "Burr size reduction in drilling by ultrasonic assistance", "abstract": "Accuracy and surface finish play an important role in modern industry. Undesired projections of materials, known as burrs, reduce the part quality and negatively affect the assembly process. A recent and promising method for reducing burr size in metal cutting is the use of ultrasonic assistance, where high-frequency and low-amplitude vibrations are added in the feed direction during cutting. Note that this cutting process is distinct from ultrasonic machining. This paper presents the design of an ultrasonically vibrated workpiece holder, and a two-stage experimental investigation of ultrasonically assisted drilling of A1100-0 aluminum workpieces. The results of 175 drilling experiments with uncoated and TiN-coated drills are reported and analyzed. The effect of ultrasonic assistance on burr size, chip formation, thrust forces and tool wear is studied. The results demonstrate that under suitable ultrasonic vibration conditions, the burr height and width can be reduced in comparison to conventional drilling", "keywords": ["burr", "drilling", "metal cutting", "ultrasonic assistance", "ultrasonic assisted drilling", "vibration assisted drilling"]}
{"id": "kp20k_training_443", "title": "Selecting Coherent and Relevant Plots in Large Scatterplot Matrices", "abstract": "The scatterplot matrix (SPLOM) is a well-established technique to visually explore high-dimensional data sets. It is characterized by the number of scatterplots (plots) of which it consists of. Unfortunately, this number quadratically grows with the number of the data sets dimensions. Thus, an SPLOM scales very poorly. Consequently, the usefulness of SPLOMs is restricted to a small number of dimensions. For this, several approaches already exist to explore such small SPLOMs. Those approaches address the scalability problem just indirectly and without solving it. Therefore, we introduce a new greedy approach to manage large SPLOMs with more than 100 dimensions. We establish a combined visualization and interaction scheme that produces intuitively interpretable SPLOMs by combining known quality measures, a pre-process reordering and a perception-based abstraction. With this scheme, the user can interactively find large amounts of relevant plots in large SPLOMs", "keywords": ["visual analytics", "quality measure", "high-dimensional data", "scatterplot matrix"]}
{"id": "kp20k_training_444", "title": "The random electrode selection ensemble for EEG signal classification", "abstract": "Pattern classification methods are a crucial direction in the current study of braincomputer interface (BCI) technology. A simple yet effective ensemble approach for electroencephalogram (EEG) signal classification named the random electrode selection ensemble (RESE) is developed, which aims to surmount the instability demerit of the Fisher discriminant feature extraction for BCI applications. Through the random selection of recording electrodes answering for the physiological background of user-intended mental activities, multiple individual classifiers are constructed. In a feature subspace determined by a couple of randomly selected electrodes, principal component analysis (PCA) is first used to carry out dimensionality reduction. Successively Fisher discriminant is adopted for feature extraction, and a Bayesian classifier with a Gaussian mixture model (GMM) approximating the feature distribution is trained. For a test sample the outputs from all the Bayesian classifiers are combined to give the final prediction for its label. Theoretical analysis and classification experiments with real EEG signals indicate that the RESE approach is both effective and efficient", "keywords": ["eeg signal classification", "classifier ensemble", "bayesian classifier", "gaussian mixture model", "fisher discriminant analysis"]}
{"id": "kp20k_training_445", "title": "Robust schur stability of polynomials with polynomial parameter dependency", "abstract": "The paper considers the robust Schur stability verification of polynomials with coefficients depending polynomially on parameters varying in given intervals. A new algorithm is presented which relies on the expansion of a multivariate polynomial into Bernstein polynomials and is based on the decomposition of the family of polynomials into its symmetric and antisymmetric parts. It is shown how the inspection of both polynomial families on the upper half of the unit circle can be reduced to the analysis of two related polynomial families on the real interval [-1, 1]. Then the Bernstein expansion can be applied in order to check whether both polynomial families have a zero in this interval in common", "keywords": ["schur stability", "robust stability", "bernstein polynomials"]}
{"id": "kp20k_training_446", "title": "Use of nano-scale double-gate MOSFETs in low-power tunable current mode analog circuits", "abstract": "Use of independently-driven nano-scale double gate (DG) MOSFETs for low-power analog circuits is emphasized and illustrated. In independent drive configuration, the top gate response of DG-MOSFETs can be altered by application of a control voltage on the bottom gate. We show that this could be a powerful method to conveniently tune the response of conventional CMOS analog circuits especially for current-mode design. Several examples of such circuits, including current mirrors, a differential current amplifier and differential integrators are illustrated and their performance gauged using TCAD simulations. The topologies and biasing schemes explored here show how the nano-scale DG-MOSFETs may pave way for efficient, mismatch-tolerant and smaller circuits with tunable characteristics", "keywords": ["integrated circuits", "tunable analog circuits", "current mode circuits", "mixed-mode simulations", "dg-mosfet"]}
{"id": "kp20k_training_447", "title": "Cooperative triangulation in MSBNs without revealing subnet structures", "abstract": "Multiply sectioned Bayesian networks (MSBNs) provide a coherent framework for probabilistic inference in a cooperative multiagent distributed interpretation system. Inference in MSBNs can be performed effectively using a compiled representation. The compilation involves the triangulation of the collective dependency structure (a graph) defined in terms of the union of a set of local dependency structures (a set of graphs). Privacy of agents eliminates the option to assemble these graphs at a central location and to triangulate their union. Earlier work solved distributed triangulation in a restricted case. The method is conceptually complex and the correctness of its extension to the general case is difficult to justify. In this paper, we present a new method that is conceptually simpler and is efficient. We prove its correctness in the general case and demonstrate its performance experimentally ", "keywords": ["triangulation", "chordal graph", "graph theory", "distributed computation", "bayesian networks", "multiply sectioned bayesian networks", "multiagent systems", "cooperation and coordination", "approximate reasoning"]}
{"id": "kp20k_training_448", "title": "the complexity of parallel evaluation of linear recurrence", "abstract": "The concept of computers such as C.mmp and ILLIAC IV is to achieve computational speed-up by performing several operations simultaneously with parallel processors. This type of computer organization is referred to as a parallel computer. In this paper, we prove upper bounds on speed-ups achievable by parallel computers for a particular problem, the solution of first order linear recurrences. We consider this problem because it is important in practice and also because it is simply stated so that we might obtain some insight into the nature of parallel computation by studying it", "keywords": ["processor", "concept", "operability", "organization", "order", "parallel computation", "complexity", "paper", "practical", "evaluation", "parallel", "computation"]}
{"id": "kp20k_training_449", "title": "Leukocyte image segmentation using simulated visual attention", "abstract": "Computer-aided automatic analysis of microscopic leukocyte is a powerful diagnostic tool in biomedical fields which could reduce the effects of human error, improve the diagnosis accuracy, save manpower and time. However, it is a challenging to segment entire leukocyte populations due to the changing features extracted in the leukocyte image, and this task remains an unsolved issue in blood cell image segmentation. This paper presents an efficient strategy to construct a segmentation model for any leukocyte image using simulated visual attention via learning by on-line sampling. In the sampling stage, two types of visual attention, bottom-up and top-down together with the movement of the human eye are simulated. We focus on a few regions of interesting and sample high gradient pixels to group training sets. While in the learning stage, the SVM (support vector machine) model is trained in real-time to simulate the visual neuronal system and then classifies pixels and extracts leukocytes from the image. Experimental results show that the proposed method has better performance compared to the marker controlled watershed algorithms with manual intervention and thresholding-based methods", "keywords": ["image segmentation", "visual attention", "machine learning", "leukocyte image", "svm"]}
{"id": "kp20k_training_450", "title": "On the construction of an aggregated measure of the development of interval data", "abstract": "We analyse some possibilities for constructing an aggregated measure of the development of socio-economical objects in terms of their composite phenomenon (i.e., phenomenon described by many statistical features) if the relevant data are expressed as intervals. Such a measure, based on the deviation of the data structure for a given object from the benchmark of development is a useful tool for ordering, comparing and clustering objects. We present the construction of a composite phenomenon when it is described by interval data and discuss various aspects of stimulation and normalization of the diagnostic features as well as a definition of a benchmark of development (based usually on optimum or expected levels of these features). Our investigation includes the following options for the realization of this purpose: transformation of the interval model into a singlevalued version without any significant loss of its statistical properties, standardization of pure intervals as well as definition of the interval ideal object. For the determination of a distance between intervals, the Hausdorff formula is applied. The simulation study conducted and the empirical analysis showed that the first two variants are especially useful in practice", "keywords": ["multifeature objects", "aggregated measure of development", "interval data", "hausdorff distance"]}
{"id": "kp20k_training_451", "title": "user requirements for a web based spreadsheet-mediated collaboration", "abstract": "This paper reports the initial results of a research project to investigate how to develop a web based spreadsheet mediated business collaboration system that could notably enhance the business processes presently carried out by Small to Medium sized Enterprises. Using a scenario-based design approach, a set of user's requirements were extracted from an appropriate field study. These requirements were then analysed in the context of well-known usability principles, and a set of design implications were derived based on a selected set of HCI design patterns related to cooperative interaction design. Starting from that knowledge, suitable interactive collaboration scenarios have been drawn, from which a list of user interface requirements for a web based spreadsheet mediated collaboration system has been formulated", "keywords": ["artifact mediated collaboration", "hci design patterns", "usability principles", "scenario-based design"]}
{"id": "kp20k_training_452", "title": "Automated estimation and analyses of meteorological drought characteristics from monthly rainfall data", "abstract": "The paper describes a new software package for automated estimation, display and analyses of various drought indices  continuous functions of precipitation that allow quantitative assessment of meteorological drought events to be made. The software at present allows up to five different drought indices to be estimated. They include the Decile Index (DI), the Effective Drought Index (EDI), the Standardized Precipitation Index (SPI) and deviations from the long-term mean and median value. Each index can be estimated from point and spatially averaged rainfall data and a number of options are provided for months' selection and the type of the analysis, including a running mean, single value or multiple annual values. The software also allows spell/run analysis to be performed and maps of a specific index to be constructed. The software forms part of the comprehensive computer package, developed earlier and designed to perform the multitude of water resources analyses and hydro-meteorological data processing. The 7-step procedure of setting up and running a typical drought assessment application is described in detail. The examples of applications are given primarily in the specific context of South Asia where the software has been used", "keywords": ["drought indices", "monthly rainfall time series", "spatsim"]}
{"id": "kp20k_training_453", "title": "Introduction to the special issue on statistical signal extraction and filtering", "abstract": "The papers of the Special Issue on Statistical Signal Extraction and Filtering are introduced briefly and the invitation to contribute to the next issue to be devoted to this topic is reiterated. There follows an account of the history and the current developments in the areas of WienerKolmogorov and Kalman filtering, which is a leading topic of the present issue. Other topics will be treated in like manner in subsequent introductions", "keywords": ["statistical signal extraction", "kalman filtering", "wienerkolmogorov filtering"]}
{"id": "kp20k_training_454", "title": "Worst-case optimal approximation algorithms for maximizing triplet consistency within phylogenetic networks", "abstract": "The study of phylogenetic networks is of great interest to computational evolutionary biology and numerous different types of such structures are known. This article addresses the following question concerning rooted versions of phylogenetic networks. What is the maximum value of p?[0,1] p ? [ 0 , 1 ] such that for every input set T  of rooted triplets, there exists some network N such that at least p|T| p | T | of the triplets are consistent with N ? We call an algorithm that computes such a network (where p is maximum) worst-case optimal. Here we prove that the set containing all triplets (the full triplet set) in some sense defines p . Moreover, given a network N that obtains a fraction p? p ? for the full triplet set (for any p? p ? ), we show how to efficiently modify N to obtain a fraction ?p p ? for any given triplet set T . We demonstrate the power of this insight by presenting a worst-case optimal result for level-1 phylogenetic networks improving considerably upon the 5/12 fraction obtained recently by Jansson, Nguyen and Sung. For level-2 phylogenetic networks we show that p?0.61 p ? 0.61 . We emphasize that, because we are taking |T| | T | as a (trivial) upper bound on the size of an optimal solution for each specific input T, the results in this article do not exclude the existence of approximation algorithms that achieve approximation ratio better than p. Finally, we note that all the results in this article also apply to weighted triplet sets", "keywords": ["triplet", "phylogenetic network", "level-k network"]}
{"id": "kp20k_training_455", "title": "Direct search of feasible region and application to a crashworthy helicopter seat", "abstract": "The paper proposes a novel approach to identify the feasible region for a constrained optimisation problem. In engineering applications the search for the feasible region turns out to be extremely useful in the understanding of the problem as the feasible region defines the portion of the domain where design parameters can be ranged to fulfil the constraints imposed on performances, manufacturing and regulations. The search for the feasible region is not a trivial task as non-convex, irregular and disjointed shapes can be found. The algorithm presented in this paper moves from the above considerations and proposes a recursive feasible-infeasible segment bisection algorithm combined with Support Vector Machine (SVM) techniques to reduce the overall computational effort. The method is discussed and then illustrated by means of three simple analytical test cases in the first part of the paper. A real-world application is finally presented: the search for the survivability zone of a crashworthy helicopter seat under different crash conditions. A finite element model, including an anthropomorphic dummy, is adopted to simulate impacts that are characterised by different deceleration pulses and the proposed algorithm is used to investigate the influence of pulse shape on impact survivability", "keywords": ["feasible region", "crashworthiness", "support vector machine", "direct search"]}
{"id": "kp20k_training_456", "title": "feasibly constructive proofs and the propositional calculus (preliminary version", "abstract": "The motivation for this work comes from two general sources. The first source is the basic open question in complexity theory of whether P equals NP (see [1] and [2]). Our approach is to try to show they are not equal, by trying to show that the set of tautologies is not in NP (of course its complement is in NP ). This is equivalent to showing that no proof system (in the general sense defined in [3]) for the tautologies is super in the sense that there is a short proof for every tautology. Extended resolution is an example of a powerful proof system for tautologies that can simulate most standard proof systems (see [3]). The Main Theorem (5.5) in this paper describes the power of extended resolution in a way that may provide a handle for showing it is not super. The second motivation comes from constructive mathematics. A constructive proof of, say, a statement @@@@A must provide an effective means of finding a proof of A for each value of x, but nothing is said about how long this proof is as a function of x. If the function is exponential or super exponential, then for short values of x the length of the proof of the instance of A may exceed the number of electrons in the universe. In section 2, I introduce the system PV for number theory, and it is this system which I suggest properly formalizes the notion of a feasibly constructive proof", "keywords": ["value", "mathematics", "systems", "examples", "values", "motivation", "functional", "standardization", "theory", "version", "power", "general", "complexity", "paper", "effect"]}
{"id": "kp20k_training_457", "title": "SINA: Semantic interpretation of user queries for question answering on interlinked data", "abstract": "The architectural choices underlying Linked Data have led to a compendium of data sources which contain both duplicated and fragmented information on a large number of domains. One way to enable non-experts users to access this data compendium is to provide keyword search frameworks that can capitalize on the inherent characteristics of Linked Data. Developing such systems is challenging for three main reasons. First, resources across different datasets or even within the same dataset can be homonyms. Second, different datasets employ heterogeneous schemas and each one may only contain a part of the answer for a certain user query. Finally, constructing a federated formal query from keywords across different datasets requires exploiting links between the different datasets on both the schema and instance levels. We present Sina, a scalable keyword search system that can answer user queries by transforming user-supplied keywords or natural-languages queries into conjunctive SPARQL queries over a set of interlinked data sources. Sina uses a hidden Markov model to determine the most suitable resources for a user-supplied query from different datasets. Moreover, our framework is able to construct federated queries by using the disambiguated resources and leveraging the link structure underlying the datasets to query. We evaluate Sina over three different datasets. We can answer 25 queries from the QALD-1 correctly. Moreover, we perform as well as the best question answering system from the QALD-3 competition by answering 32 questions correctly while also being able to answer queries on distributed sources. We study the runtime of SINA in its mono-core and parallel implementations and draw preliminary conclusions on the scalability of keyword search on Linked Data", "keywords": ["keyword search", "question answering", "hidden markov model", "sparql", "rdf", "disambiguation"]}
{"id": "kp20k_training_458", "title": "Generalized median string computation by means of string embedding in vector spaces", "abstract": "In structural pattern recognition the median string has been established as a useful tool to represent a set of strings. However, its exact computation is complex and of high computational burden. In this paper we propose a new approach for the computation of median string based on string embedding. Strings are embedded into a vector space and the median is computed in the vector domain. We apply three different inverse transformations to go from the vector domain back to the string domain in order to obtain a final approximation of the median string. All of them are based on the weighted mean of a pair of strings. Experiments show that we succeed to compute good approximations of the median string", "keywords": ["string", "generalized median", "embedding", "vector space", "lower bound"]}
{"id": "kp20k_training_459", "title": "efficient indexing of the historical, present, and future positions of moving objects", "abstract": "Although significant effort has been put into the development of efficient spatio-temporal indexing techniques for moving objects, little attention has been given to the development of techniques that efficiently support queries about the past, present, and future positions of objects. The provisioning of such techniques is challenging, both because of the nature of the data, which reflects continuous movement, and because of the types of queries to be supported. This paper proposes the BB x-index structure, which indexes the positions of moving objects, given as linear functions of time, at any time. The index stores linearized moving-object locations in a forest of B + -trees. The index supports queries that select objects based on temporal and spatial constraints, such as queries that retrieve all objects whose positions fall within a spatial range during a set of time intervals. Empirical experiments are reported that offer insight into the query and update performance of the proposed technique", "keywords": ["indexing", "b-tree", "mobile objects"]}
{"id": "kp20k_training_460", "title": "towards model-driven unit testing", "abstract": "The Model-Driven Architecture (MDA) approach for constructing software systems advocates a stepwise refinement and transformation process starting from high-level models to concrete program code. In contrast to numerous research efforts that try to generate executable function code from models, we propose a novel approach termed model-driven monitoring. On the model level the behavior of an operation is specified with a pair of UML composite structure diagrams (visual contract), a visual notation for pre- and post-conditions. The specified behavior is implemented by a programmer manually. An automatic translation from our visual contracts to JML assertions allows for monitoring the hand-coded programs during their execution. In this paper we present how we extend our approach to allow for model-driven unit testing, where we utilize the generated JML assertions as test oracles. Further, we present an idea how to generate sufficient test cases from our visual contracts with the help of model-checking techniques", "keywords": ["test case generation", "visual contracts", "model checking", "design by contract"]}
{"id": "kp20k_training_461", "title": "Sliding window-based frequent pattern mining over data streams", "abstract": "Finding frequent patterns in a continuous stream of transactions is critical for many applications such as retail market data analysis, network monitoring, web usage mining, and stock market prediction. Even though numerous frequent pattern mining algorithms have been developed over the past decade, new solutions for handling stream data are still required due to the continuous, unbounded, and ordered sequence of data elements generated at a rapid rate in a data stream. Therefore, extracting frequent patterns from more recent data can enhance the analysis of stream data. In this paper, we propose an efficient technique to discover the complete set of recent frequent patterns from a high-speed data stream over a sliding window. We develop a Compact Pattern Stream tree (CPS-tree) to capture the recent stream data content and efficiently remove the obsolete, old stream data content. We also introduce the concept of dynamic tree restructuring in our CPS-tree to produce a highly compact frequency-descending tree structure at runtime. The complete set of recent frequent patterns is obtained from the CPS-tree of the current window using an FP-growth mining technique. Extensive experimental analyses show that our CPS-tree is highly efficient in terms of memory and time complexity when finding recent frequent patterns from a high-speed data stream", "keywords": ["frequent pattern", "data stream", "sliding window", "tree restructuring"]}
{"id": "kp20k_training_462", "title": "modeling cryptographic properties of voice and voice-based entity authentication", "abstract": "Strong and/or multi-factor entity authentication protocols are of crucial importancein building successful identity management architectures. Popular mechanisms to achieve these types of entity authentication are biometrics, and, in particular, voice, for which there are especially interesting business cases in the telecommunication and financial industries, among others. Despite several studies on the suitability of voice within entity authentication protocols, there has been little or no formal analysis of any such methods. In this paper we embark into formal modeling of seemingly cryptographic properties of voice. The goal is to define a formal abstraction for voice, in terms of algorithms with certain properties, that are of both combinatorial and cryptographic type. While we certainly do not expect to achieve the perfect mathematical model for a human phenomenon, we do hope that capturing some properties of voice in a formal model would help towards the design and analysis of voice-based cryptographic protocols, as for entity authentication. In particular, in this model we design and formally analyze two voice-based entity authentication schemes, the first being a voice-based analogue of the conventional password-transmission entity authentication scheme. We also design and analyze, in the recently introduced bounded-retrieval model [4], one voice-and-password-based entity authentication scheme that is additionally secure against intrusions and brute-force attacks, including dictionary attacks", "keywords": ["biometrics", "modeling human factors", "voice", "entity authentication"]}
{"id": "kp20k_training_463", "title": "Inference of finite-state transducers from regular languages", "abstract": "Finite-state transducers are models that are being used in different areas of pattern recognition and computational linguistics. One of these areas is machine translation, where the approaches that are based on building models automatically from training examples are becoming more and more attractive. Finite-state transducers are very adequate to be used in constrained tasks where training samples of pairs of sentences are available. A technique to infer finite-state transducers is proposed in this work. This technique is based on formal relations between finite-state transducers and finite-state grammars. Given a training corpus of inputoutput pairs of sentences, the proposed approach uses statistical alignment methods to produce a set of conventional strings from which a stochastic finite-state grammar is inferred. This grammar is finally transformed into a resulting finite-state transducer. The proposed methods are assessed through series of machine translation experiments within the framework of the EUTRANS project", "keywords": ["machine translation", "grammatical inference", "formal language theory", "stochastic finite-state transducers", "natural language processing"]}
{"id": "kp20k_training_464", "title": "Particle swarm optimization with preference order ranking for multi-objective optimization", "abstract": "A new optimality criterion based on preference order (PO) scheme is used to identify the best compromise in multi-objective particle swarm optimization (MOPSO). This scheme is more efficient than Pareto ranking scheme, especially when the number of objectives is very large. Meanwhile, a novel updating formula for the particles velocity is introduced to improve the search ability of the algorithm. The proposed algorithm has been compared with NSGA-II and other two MOPSO algorithms. The experimental results indicate that the proposed approach is effective on the highly complex multi-objective optimization problems", "keywords": ["particle swarm", "preference order", "pareto dominance", "multi-objective optimization", "best compromise"]}
{"id": "kp20k_training_465", "title": "real-time deformation using modal analysis on graphics hardware", "abstract": "This paper presents an approach for fast simulating deformable objects that is suitable for interactive applications in computer graphics. Linear modal analysis is often used to simulate small-amplitude deformation. Compared to traditional linear modal analysis where the CPU has been used to calculate the nodal displacements, the vertex program of GPU has been found widely adopted in the current applications. However the calculation suffers from great errors due to the limitation of the number of the input registers on GPU vertex pipeline. In our approach, we solve this problem by the fragment program. A series of 2D floating point textures are used to hold the model displacement matrix, the fragment program multiplies this matrix with the modal amplitude and sums up the results. Experiments show that the proposed technique fully utilizes the parallelism nature of GPU, and runs in real-time even for the complex models", "keywords": ["graphics hardware", "physically based modeling", "deformation", "modal analysis"]}
{"id": "kp20k_training_466", "title": "STABILITY ANALYSIS OF A CLASS OF GENERAL PERIODIC NEURAL NETWORKS WITH DELAYS AND IMPULSES", "abstract": "Based on the inequality analysis, matrix theory and spectral theory, a class of general periodic neural networks with delays and impulses is studied. Some sufficient conditions are established for the existence and globally exponential stability of a unique periodic solution. Furthermore, the results are applied to some typical impulsive neural network systems as special cases, with a real-life example to show feasibility of our results", "keywords": ["neural network", "periodic solution", "delay", "impulse", "global exponential stability"]}
{"id": "kp20k_training_467", "title": "Neuroprotective properties of resveratrol and derivatives", "abstract": "Stilbenoid compounds consist of a family of resveratrol derivatives. They have demonstrated promising activities in vitro and in vivo that indicate they may be useful in the prevention of a wide range of pathologies, such as cardiovascular diseases and cancers, as well have anti-aging effects. More recently stilbenoid compounds have shown promise in the treatment and prevention of neurodegenerative disorders, such as Huntingtons, Parkinsons, and Alzheimer's diseases. This paper primarily focuses on the impact of stilbenoids in Alzheimer's disease and more specifically on the inhibition of ?-amyloid peptide aggregation", "keywords": ["stilbenoid", "alzheimers disease", "amyloid peptide", "inhibition of aggregation"]}
{"id": "kp20k_training_468", "title": "A new fuzzy multicriteria decision making method and its application in diversion of water", "abstract": "Taking account of uncertainty in multicriteria decision making problems is crucial due to the fact that depending on how it is done, ranking of alternatives can be completely different. This paper utilizes linguistic values to evaluate the performance of qualitative criteria and proposes using appropriate shapes of fuzzy numbers to evaluate the performance of quantitative criteria for each problem with respect to its particular conditions. In addition, a process to determine the weights of criteria using fuzzy numbers, which considers their competition to gain greater weights and their influence on each other is described. A new fuzzy methodology is proposed to solve such a problem that utilizes parametric form of fuzzy numbers. The case study of diversion of water into Lake Urmia watershed, which is defined using triangular, trapezoidal, and bell-shape fuzzy numbers demonstrates the utility of the proposed method.  ", "keywords": ["multicriteria decision making", "fuzzy numbers with different shapes", "water resource planning and management"]}
{"id": "kp20k_training_469", "title": "Scheduling divisible workloads on heterogeneous platforms", "abstract": "In this paper, we discuss several algorithms for scheduling divisible workloads on heterogeneous systems. Our main contributions are (i) new optimality results for single-round algorithms and (ii) the design of an asymptotically optimal multi-round algorithm. This multi-round algorithm automatically performs resource selection, a difficult task that was previously left to the user. Because it is periodic, it is simpler to implement, and more robust to changes in the speeds of the processors and/or communication links. On the theoretical side, to the best of our knowledge, this is the first published result assessing the absolute performance of a multi-round algorithm. On the practical side, extensive simulations reveal that our multi-round algorithm outperforms existing solutions on a large variety of platforms, especially when the communication-to-computation ratio is not very high (the difficult case", "keywords": ["scheduling", "divisible tasks", "multi-round algorithms", "asymptotical optimality"]}
{"id": "kp20k_training_470", "title": "A connective ethnography of peer knowledge sharing and diffusion in a tween virtual world", "abstract": "Prior studies have shown how knowledge diffusion occurs in classrooms and structured small groups around assigned tasks yet have not begun to account for widespread knowledge sharing in more native, unstructured group settings found in online games and virtual worlds. In this paper, we describe and analyze how an insider gaming practice spread across a group of tween players ages 912years in an after-school gaming club that simultaneously participated in a virtual world called Whyville.net. In order to understand how this practice proliferated, we followed the club members as they interacted with each other and members of the virtual world at large. Employing connective ethnography to trace the movements in learning and teaching this practice, we coordinated data records from videos, tracking data, field notes, and interviews. We found that club members took advantage of the different spaces, people, and times available to them across Whyville, the club, and even home and classroom spaces. By using an insider gaming practice, namely teleporting, rather than the more traditional individual person as our analytical lens, we were able to examine knowledge sharing and diffusion across the gaming spaces, including events in local small groups as well as encounters in the virtual world. In the discussion, we address methodological issues and design implications of our findings", "keywords": ["virtual worlds", "knowledge sharing", "knowledge diffusion", "connective ethnography", "peer pedagogy"]}
{"id": "kp20k_training_471", "title": "Unsupervised classification of SAR images using normalized gamma process mixtures", "abstract": "We propose an image prior for the model-based nonparametric classification of synthetic aperture radar (SAR) images that allows working with infinite number of mixture components. In order to enclose the spatial interactions of the pixel labels, the prior is derived by incorporating a conditional multinomial auto-logistic random field into the Normalized Gamma Process prior. In this way, we obtain an image classification prior that is free from the limitation on the number of classes and includes the smoothing constraint into classification problem. In this model, we introduced a hyper-parameter that can control the preservation of the important classes and the extinction of the weak ones. The recall rates reported on the synthetic and the real TerraSAR-X images show that the proposed model is capable of accurately classifying the pixels. Unlike the existing methods, it applies a simple iterative update scheme without performing a hierarchical clustering strategy. We demonstrate that the estimation accuracy of the proposed method in number of classes outperforms the conventional finite mixture models", "keywords": ["normalized gamma process mixtures", "nonparametric bayesian", "image classification", "sar images"]}
{"id": "kp20k_training_472", "title": "Two Couple-Resolution Blocking Protocols on Adaptive Query Splitting for RFID Tag Identification", "abstract": "How to accelerate tag identification is an important issue in Radio Frequency Identification (RFID) systems. In some cases, the RFID reader repeatedly identifies the same tags since these tags always stay in its communication range. An anticollision protocol, called the adaptive query splitting protocol (AQS), was proposed to handle these cases. This protocol reserves information obtained from the last process of tag identification so that the reader can quickly identify these staying tags again. This paper proposes two blocking protocols, a couple-resolution blocking protocol (CRB) and an enhanced couple-resolution blocking protocol (ECRB), based on AQS. CRB and ECRB not only have the above-mentioned capability as AQS but also use the blocking technique, which prohibits unrecognized tags from colliding with staying tags, to reduce the number of collisions. Moreover, CRB adopts a couple-resolution technique to couple staying tags by simultaneously transmitting two ID prefixes from the reader, while ECRB allows the reader to send only one ID prefix to interrogate a couple of staying tags. Thus, they only need half time to identify staying tags. We formally analyze the identification delay of CRB and ECRB in the worst and average cases. Our analytic and simulation results show that they obviously outperform AQS, and ECRB needs less transmitted bits than CRB", "keywords": ["anticollision", "blocking protocol", "couple-resolution", "rfid", "tag identification"]}
{"id": "kp20k_training_473", "title": "an active measurement system for shared environments", "abstract": "Testbeds composed of end hosts deployed across the Internet enable researchers to simultaneously conduct a wide variety of experiments. Active measurement studies of Internet path properties that require precisely crafted probe streams can be problematic in these environments. The reason is that load on the host systems from concurrently executing experiments (as is typical in PlanetLab) can significantly alter probe stream timings. In this paper we measure and characterize how packet streams from our local PlanetLab nodes are affected by experimental concurrency. We find that the effects can be extreme. We then set up a simple PlanetLab deployment in a laboratory testbed to evaluate these effects in a controlled fashion. We find that even relatively low load levels can cause serious problems in probe streams. Based on these results, we develop a novel system called  MAD  that can operate as a Linux kernel module or as a stand-alone daemon to support real-time scheduling of probe streams.  MAD  coordinates probe packet emission for all active measurement experiments on a node. We demonstrate the capabilities of  MAD , showing that it performs effectively even under very high levels of multiplexing and host system load", "keywords": ["active measurement", "mad"]}
{"id": "kp20k_training_474", "title": "Policy-based inconsistency management in relational databases", "abstract": "We define inconsistency management policies (IMPs) for real world applications. We show how IMPs relate to belief revision postulates, CQA, and relational algebra operators. We present several approaches to efficiently implement an IMP-based framework", "keywords": ["inconsistency management", "relational databases"]}
{"id": "kp20k_training_475", "title": "A new delay-dependent stability criterion for linear neutral systems with norm-bounded uncertainties in all system matrices", "abstract": "This paper deals with the problem of robust stability for a class of uncertain linear neutral systems. The uncertainties under consideration are of norm-bounded type and appear in all system matrices. A new delay-dependent stability criterion is obtained and formulated in the form of linear matrix inequalities (LMIs). Neither model transformation nor bounding technique for cross terms is involved through derivation of the stability criterion. Numerical examples show that the results obtained in this paper significantly improve the estimate of the stability limit over some existing results in the literature", "keywords": ["linear systems", "neutral systems", "stability", "time delay", "uncertainty", "linear matrix inequality"]}
{"id": "kp20k_training_476", "title": "NEUTRALIZATION: NEW INSIGHTS INTO THE PROBLEM OF EMPLOYEE INFORMATION SYSTEMS SECURITY POLICY VIOLATIONS", "abstract": "Employees' failure to comply with information systems security policies is a major concern for information technology security managers. In efforts to understand this problem, IS security researchers have traditionally viewed violations of IS security policies through the lens of deterrence theory. In this article, we show that neutralization theory. a theory prominent in Criminology but not yet applied in the context of IS, provides a compelling explanation for IS security policy violations and offers new insight into how employees rationalize this behavior. In doing so, we propose a theoretical model in which the effects of neutralization techniques are tested alongside those of sanctions described by deterrence theory. Our empirical results highlight neutralization as an important factor to take into account with regard to developing and implementing organizational security policies and practices", "keywords": ["neutralization theory", "deterrence theory", "is security policies", "is security", "compliance"]}
{"id": "kp20k_training_477", "title": "Simplifying complex environments using incremental textured depth meshes", "abstract": "We present an incremental algorithm to compute image-based simplifications of a large environment. We use an optimization-based approach to generate samples based on scene visibility, and from each viewpoint create textured depth meshes (TDMs) using sampled range panoramas of the environment. The optimization function minimizes artifacts such as skins and cracks in the reconstruction. We also present an encoding scheme for multiple TDMs that exploits spatial coherence among different viewpoints. The resulting simplifications, incremental textured depth meshes (ITDMs), reduce preprocessing, storage, rendering costs and visible artifacts. Our algorithm has been applied to large, complex synthetic environments comprising millions of primitives. It is able to render them at 20 - 40 frames a second on a PC with little loss in visual fidelity", "keywords": ["interactive display", "simplification", "textured depth meshes", "spatial encoding", "walkthrough"]}
{"id": "kp20k_training_478", "title": "A Neural Approach to the Underdetermined-Order Recursive Least-Squares Adaptive Filtering", "abstract": "The incorporation of the neural architectures in adaptive filtering applications has been addressed in detail. In particular, the Underdetermined-Order Recursive Least-Squares (URLS) algorithm, which lies between the well-known Normalized Least Mean Square and Recursive Least Squares algorithms, is reformulated via a neural architecture. The response of the neural network is seen to be identical to that of the algorithmic approach. Together with the advantage of simple circuit realization, this neural network avoids the drawbacks of digital computation such as error propagation and matrix inversion, which is ill-conditioned in most cases. It is numerically attractive because the quadratic optimization problem performs an implicit matrix inversion. Also, the neural network offers the flexibility of easy alteration of the prediction order of the URLS algorithm which may be crucial in some applications. It is rather difficult to achieve in the digital implementation, as one would have to use Levinson recursions. The neural network can easily be integrated into a digital system through appropriate digital-to-analog and analog-to-digital converters", "keywords": ["adaptive filtering", "underdetermined recursive least squares", "neural networks", "analog adaptive filter"]}
{"id": "kp20k_training_479", "title": "Bottleneck flows in unit capacity networks", "abstract": "The bottleneck network flow problem (BNFP) is a generalization of several well-studied bottleneck problems such as the bottleneck transportation problem (BTP), bottleneck assignment problem (BAP), bottleneck path problem (BPP), and so on. The BNFP can easily be solved as a sequence of O(log n) maximum flow problems on almost unit capacity networks. We observe that this algorithm runs in O(min{m(3/2) . n(2/3) m} log n) time by showing that the maximum flow problem on an almost unit capacity graph can be solved in O(min{m(3/2) . n(2/3)m}) time. We then propose a faster algorithm to solve the unit capacity BNFP in O(min{m(n log n)(2/3). m(3/2) root log n}) time, an improvement by a factor of at least 3 root log n. For dense graphs, the improvement is by a factor of root log n. On unit capacity simple graphs, we show that BNFP can be solved in O root n log n) time, an improvement by a factor of root log n. As a consequence we have an O(m root n log n) algorithm for the BTP with unit arc capacities.  ", "keywords": ["algorithms", "combinatorial problems", "graphs", "network flows", "minimum cost flow", "unit capacity"]}
{"id": "kp20k_training_480", "title": "Taylor's decomposition on four points for solving third-order linear time-varying systems", "abstract": "In the present paper, the use of three-step difference schemes generated by Taylor's decomposition on four points for the numerical solutions of third-order time-varying linear dynamical systems is presented. The method is illustrated for the numerical analysis of an up-converter used in communication systems", "keywords": ["taylor's decomposition on four points", "third-order differential equation", "three-step difference schemes", "approximation order", "periodically time-varying systems"]}
{"id": "kp20k_training_481", "title": "BEM formulation for von Krmn plates", "abstract": "This work deals with nonlinear geometric plates in the context of von Krmn's theory. The formulation is written such that only the boundary in-plane displacement and deflection integral equations for boundary collocations are required. At internal points, only out-of-plane rotation, curvature and in-plane internal force representations are used. Thus, only integral representations of these values are derived. The nonlinear system of equations is derived by approximating all densities in the domain integrals as single values, which therefore reduces the computational effort needed to evaluate the domain value influences. Hyper-singular equations are avoided by approximating the domain values using only internal nodes. The solution is obtained using a Newton scheme for which a consistent tangent operator was derived", "keywords": ["bending plates", "geometrical nonlinearities"]}
{"id": "kp20k_training_482", "title": "On X-Variable Filling and Flipping for Capture-Power Reduction in Linear Decompressor-Based Test Compression Environment", "abstract": "Excessive test power consumption and growing test data volume are both serious concerns for the semiconductor industry. Various low-power X-filling techniques and test data compression schemes were developed accordingly to address the above problems. These methods, however, often exploit the very same \"don't-care\" bits in the test cubes to achieve different objectives and hence may contradict each other. In this paper, we propose novel techniques to reduce scan capture power in linear decompressor-based test compression environment, by employing algorithmic solutions to fill and flip X-variables supplied to the linear decompressor. Experimental results on benchmark circuits demonstrate that our proposed techniques significantly outperform existing solutions", "keywords": ["capture-power reduction", "linear decompressor-based test compression", "x-filling"]}
{"id": "kp20k_training_483", "title": "WWW-based access to object-oriented clinical databases: the KHOSPAD project", "abstract": "KHOSPAD is a project aiming at improving the quality of the process of patient care concerning general practitionerpatienthospital relationships, using current information and networking technologies. The studied application field is a cardiology division, with hemodynamic laboratory and the population of PTCA patients. Data related to PTCA patients are managed by ARCADIA, an object-oriented database management system developed for the considered clinical setting. We defined a remotely accessible view of ARCADIA medical record, suitable for general practitioners (GPs) caring patients after PTCA, during the follow-up period. Using a PC, a modem and Internet, an authorized GP can consult remotely the medical records of his PTCA patients. Main features of the application are related to the management and display of complex data, specifically characterized by multimedia and temporal features, based on an object-oriented temporal data model", "keywords": ["object-oriented clinical databases", "temporal databases", "www", "internet", "java", "software architecture", "temporal data visualization"]}
{"id": "kp20k_training_484", "title": "Fuzzy R-subgroups with thresholds of near-rings and implication operators", "abstract": "Using the belongs to relation (q) and quasi-coincidence with relation (q) between fuzzy points and fuzzy sets, the concept of (alpha, beta)-fuzzy R-subgroup of a near-ring where alpha , beta are any two of {epsilon, q, epsilon boolean AND q , epsilon boolean OR q} with alpha not equal epsilon boolean AND q is introduced and related properties are investigated. We also introduce the notion of a fuzzy R-subgroup with thresholds which is a generalization of an ordinary fuzzy R-subgroup and an (epsilon, epsilon boolean OR q)-fuzzy R-subgroup. Finally, we give the definition of an implication-based fuzzy R-subgroup", "keywords": ["fuzzy set", "fuzzy point", "near-ring", "fuzzy r-subgroup", "fuzzy r-subgroup", "level set"]}
{"id": "kp20k_training_485", "title": "A time accurate pseudo-wavelet scheme for two-dimensional turbulence", "abstract": "In this paper, we propose a wavelet-Taylor-Galerkin method for solving the two-dimensional Navier-Stokes equations. The discretization in time is performed before the spatial discretization by introducing second-order generalization of the standard time stepping schemes with the help of Taylor series expansion in time step. Wavelet-Taylor-Galerkin schemes taking advantage of the wavelet bases capabilities to compress both functions and operators are presented. Results for two-dimensional turbulence are shown", "keywords": ["taylor-galerkin method", "wavelets", "navier-stokes equations", "turbulence"]}
{"id": "kp20k_training_486", "title": "Audio-augmented paper for therapy and educational intervention for children with autistic spectrum disorder", "abstract": "Physical tokens are artifacts which sustain cooperation between the children and therapists. Therapists anchor children's attention through physical tokens. Therapists controlled children's attention through physical tokens. The environment provides to the therapists the control of the flow of the therapeutic activity. The environment provides a good mean to stimulate fun and consequently to help children's attention on listening tasks", "keywords": ["autism spectrum disorder", "social competence", "social story", "audio-augmented paper", "interaction design", "tangible user interface"]}
{"id": "kp20k_training_487", "title": "TREATING EPILEPSY VIA ADAPTIVE NEUROSTIMULATION: A REINFORCEMENT LEARNING APPROACH", "abstract": "This paper presents a now methodology for automatically learning an optimal neurostimulation strategy for the treatment of epilepsy. The technical challenge is to automatically modulate neurostimulation parameters, as a function of the observed EEG signal, so as to minimize the frequency and duration of seizures. The methodology leverages recent techniques from the machine learning literature, in particular the reinforcement learning paradigm, to formalize this optimization problem. We present an algorithm which is able to automatically learn an adaptive neurostimulation strategy directly from labeled training data acquired from animal brain tissues. Our results suggest that this methodology can be used to automatically find a stimulation strategy which effectively reduces the incidence of seizures, while also minimizing the amount of stimulation applied. This work highlights the crucial role that modern machine learning techniques can play in the optimization of treatment strategies for patients with chronic disorders such as epilepsy", "keywords": ["epilepsy", "neurostimulation", "reinforcement learning"]}
{"id": "kp20k_training_488", "title": "Load-Balanced Parallel Streamline Generation on Large Scale Vector Fields", "abstract": "Because of the ever increasing size of output data from scientific simulations, supercomputers are increasingly relied upon to generate visualizations. One use of supercomputers is to generate field lines from large scale flow fields. When generating field lines in parallel, the vector field is generally decomposed into blocks, which are then assigned to processors. Since various regions of the vector field can have different flow complexity, processors will require varying amounts of computation time to trace their particles, causing load imbalance, and thus limiting the performance speedup. To achieve load-balanced streamline generation, we propose a workload-aware partitioning algorithm to decompose the vector field into partitions with near equal workloads. Since actual workloads are unknown beforehand, we propose a workload estimation algorithm to predict the workload in the local vector field. A graph-based representation of the vector field is employed to generate these estimates. Once the workloads have been estimated, our partitioning algorithm is hierarchically applied to distribute the workload to all partitions. We examine the performance of our workload estimation and workload-aware partitioning algorithm in several timings studies, which demonstrates that by employing these methods, better scalability can be achieved with little overhead", "keywords": ["flow visualization", "parallel processing", "3d vector field visualization", "streamlines"]}
{"id": "kp20k_training_489", "title": "An integrated research tool for X-ray imaging simulation", "abstract": "This paper presents a software simulation package of the entire X-ray projection radiography process including beam generation, absorber structure and composition, irradiation set up, radiation transport through the absorbing medium, image formation and dose calculation. Phantoms are created as composite objects from geometrical or voxelized primitives and can be subjected to simulated irradiation process. The acquired projection images represent the two-dimensional spatial distribution of the energy absorbed in the detector and are formed at any geometry, taking into account energy spectrum, beam geometry and detector response. This software tool is the evolution of a previously presented system, with new functionalities, user interface and an expanded range of applications. This has been achieved mainly by the use of combinatorial geometry for phantom design and the implementation of a Monte Carlo code for the simulation of the radiation interaction at the absorber and the detector", "keywords": ["monte carlo", "imaging", "simulation", "projection radiography"]}
{"id": "kp20k_training_490", "title": "Requirements and solutions to software encapsulation and engineering in next generation manufacturing systems: OOONEIDA approach", "abstract": "This paper addresses the solutions enabling agile development, deployment and reconfiguration of software-intensive automation systems both in discrete manufacturing and process technologies. As the key enabler for reaching the required level of flexibility of such systems, the paper discusses the issues of encapsulation, integration and re-use of the automation intellectual property (IP). The goals can be fulfilled by the use of a vendor-independent concept of a reusable portable and scalable software module (function block), as well as by a vendor-independent automation device model. This paper also discusses the requirements of the methodology for the application of such modules in the time- and cost-effective specification, design, validation, realization and deployment of intelligent mechatronic components in distributed industrial automation and control systems. A new global initiative OOONEIDA is presented, that targets these goals through the development of the automation object concept based on the recognized industrial standards IEC61131, IEC61499, IEC61804 and unified modelling language (UML); and through the creation of the technological infrastructure for a new, open-knowledge economy for automation components and automated industrial products. In particular, a web-based repository for standardized automation solutions will be developed to serve as an electronic-commerce facility in industrial automation businesses", "keywords": ["industrial automation", "intelligent manufacturing systems"]}
{"id": "kp20k_training_491", "title": "Robust camera pose and scene structure analysis for service robotics", "abstract": "Successful path planning and object manipulation in service robotics applications rely both on a good estimation of the robot's position and orientation (pose) in the environment, as well as on a reliable understanding of the visualized scene. In this paper a robust real-time camera pose and a scene structure estimation system is proposed. First, the pose of the camera is estimated through the analysis of the so-called tracks. The tracks include key features from the imaged scene and geometric constraints which are used to solve the pose estimation problem. Second, based on the calculated pose of the camera, i.e. robot, the scene is analyzed via a robust depth segmentation and object classification approach. In order to reliably segment the object's depth, a feedback control technique at an image processing level has been used with the purpose of improving the robustness of the robotic vision system with respect to external influences, such as cluttered scenes and variable illumination conditions. The control strategy detailed in this paper is based on the traditional open-loop mathematical model of the depth estimation process. In order to control a robotic system, the obtained visual information is classified into objects of interest and obstacles. The proposed scene analysis architecture is evaluated through experimental results within a robotic collision avoidance system.  ", "keywords": ["robot vision systems", "feedback control", "stereo vision", "robustness", "3d reconstruction"]}
{"id": "kp20k_training_492", "title": "NML, a schematic extension of F. Esteva and L. Godo's logic MTL", "abstract": "A schematic extension NML of F.Esteva and L.Godo's Logic MTL is introduced in this paper. Based on a new left-continuous but discontinuous t-norm, which was proposed by S.Jenei and can be regarded as a kind of distorted nilpotent minimum, the semantics of NML is interpreted and the standard completeness theorem of NML is proved. The fact that the maximum and the minimum are definable from the negation and implication in NML and NM is discovered, which also leads to a modification of the NM axiom system.  ", "keywords": ["non-classical logics", "left-continuous t-norm", "mtl system", "nm system", "lukasiewicz system", "nml system"]}
{"id": "kp20k_training_493", "title": "verifying safety properties of concurrent java programs using 3-valued logic", "abstract": "We provide a parametric framework for verifying safety properties of concurrent Java programs. The framework combines thread-scheduling information with information about the shape of the heap. This leads to error-detection algorithms that are more precise than existing techniques. The framework also provides the most precise shape-analysis algorithm for concurrent programs. In contrast to existing verification techniques, we do not put a bound on the number of allocated objects. The framework even produces interesting results when analyzing Java programs with an unbounded number of threads. The framework is applied to successfully verify the following properties of a concurrent program: Concurrent manipulation of linked-list based ADT preserves the ADT datatype invariant [19]. The program does not perform inconsistent updates due to interference. The program does not reach a deadlock. The program does not produce run-time errors due to illegal thread interactions. We also find bugs in erroneous versions of such implementations. A prototype of our framework has been implemented", "keywords": ["precise", "deadlock", " framework ", "scheduling", "concurrent program", "invariance", "object", "timing", "shape", "informal", "errors", "error detection", "thread", "interaction", "program", "shape analysis", "prototype", "parametric", "implementation", "concurrency", "verification", "version", "logic", "manipulation", "algorithm", "interference", "update", "bugs"]}
{"id": "kp20k_training_494", "title": "Automatic discovery of theorems in elementary geometry", "abstract": "We present here a further development of the well-known approach to automatic theorem proving in elementary geometry via algorithmic commutative algebra and algebraic geometry. Rather than confirming/refuting geometric statements (automatic proving) or finding geometric formulae holding among prescribed geometric magnitudes (automatic derivation), in this paper we consider (following Kapur and Mundy) the problem of dealing automatically with arbitrary geometric statements (i.e., theses that do not follow, in general, from the given hypotheses) aiming to find complementary hypotheses for the statements to become true. First we introduce some standard algebraic geometry notions in automatic proving, both for self-containment and in order to focus our own contribution. Then we present a rather successful but noncomplete method for automatic discovery that, roughly, proceeds adding the given conjectural thesis to the collection of hypotheses and then derives some special consequences from this new set of conditions. Several examples are discussed in detail", "keywords": ["automatic theorem proving", "elementary geometry", "grobner basis"]}
{"id": "kp20k_training_495", "title": "Using support vector machines with a novel hybrid feature selection method for diagnosis of erythemato-squamous diseases", "abstract": "In this paper, we developed a diagnosis model based on support vector machines (SVM) with a novel hybrid feature selection method to diagnose erythemato-squamous diseases. Our proposed hybrid feature selection method, named improved F-score and Sequential Forward Search (IFSFS), combines the advantages of filter and wrapper methods to select the optimal feature subset from the original feature set. In our IFSFS, we improved the original F-score from measuring the discrimination of two sets of real numbers to measuring the discrimination between more than two sets of real numbers. The improved F-score and Sequential Forward Search (SFS) are combined to find the optimal feature subset in the process of feature selection, where, the improved F-score is an evaluation criterion of filter method, and SFS is an evaluation system of wrapper method. The best parameters of kernel function of SVM are found out by grid search technique. Experiments have been conducted on different training-test partitions of the erythemato-squamous diseases dataset taken from UCI (University of California Irvine) machine learning database. Our experimental results show that the proposed SVM-based model with IFSFS achieves 98.61% classification accuracy and contains 21 features. With these results, we conclude our method is very promising compared to the previously reported results.  ", "keywords": ["support vector machines ", "feature selection", "sequential forward search ", "erythemato-squamous diseases"]}
{"id": "kp20k_training_496", "title": "Domain-specific languages: From design to implementation application to video device drivers generation", "abstract": "Domain-Specific languages (DSL) have many potential advantages in terms of software engineering ranging from increased productivity to the application of formal methods. Although they have been used in practice for decades, there has been little study of methodology or implementation tools for the DSL approach. In this paper, we present our DSL approach and its application to a realistic domain: the generation of video display device drivers. The presentation focuses on the validation of our proposed framework for domain-specific languages, from design to implementation. The framework leads to a flexible design and structure, and provides automatic generation of efficient implementations of DSL programs. Additionally, we describe an example of a complete DSL for video display adaptors and the benefits of the DSL approach for this application. This demonstrates some of the generally claimed benefits of using DSLs: increased productivity, higher-level abstraction, and easier verification. This DSL has been fully implemented with our approach and is available. Compose project URL: http://www.irisa.fr/compose/gal", "keywords": ["gal", "video cards", "device drivers", "domain-specific language", "partial evaluation"]}
{"id": "kp20k_training_497", "title": "mapping visual notations to mof compliant models with qvt relations", "abstract": "Model-centric methodologies rely on the definition of domain-specific modeling languages for being able to create domain-specific models. With MOF the OMG adopted a standard which provides the essential constructs for the definition of semantic language constructs (abstract syntax). However, there are no specifications on how to define the notations (concrete syntax) for abstract syntax elements. Usually, the concrete syntax of MOF compliant languages is described informally. We propose to define MOF-based metamodels for abstract syntax and concrete syntax and to connect them by model transformations specified with QVT Relations in a flexible, declarative way. Using a QVT based transformation engine one can easily implement a Model View Controller architecture by integrating modeling tools and metadata repositories", "keywords": ["visual languages", "model transformation", "domain specific languages", "ocl", "qvt relations"]}
{"id": "kp20k_training_498", "title": "Financial early warning system model and data mining application for risk detection", "abstract": "One of the biggest problems of SMEs is their tendencies to financial distress because of insufficient finance background. In this study, an early warning system (EWS) model based on data mining for financial risk detection is presented. CHAID algorithm has been used for development of the EWS. Developed EWS can be served like a tailor made financial advisor in decision making process of the firms with its automated nature to the ones who have inadequate financial background. Besides, an application of the model implemented which covered 7853 SMEs based on Turkish Central Bank (TCB) 2007 data. By using EWS model, 31 risk profiles, 15 risk indicators, 2 early warning signals, and 4 financial road maps has been determined for financial risk mitigation", "keywords": ["chaid", "data mining", "early warning systems", "financial risk", "financial distress", "smes"]}
{"id": "kp20k_training_499", "title": "A new wavelet algorithm to enhance and detect microcalcifications", "abstract": "We have proposed a new thresholding technique applied over wavelet coefficients for mammogram enhancement. We have utilized Shannon entropy to find the best t in the wavelet domain. We have utilized Tsallis entropy to find the best t in the wavelet domain. The proposed technique has better FROC test with 96.5% true positives and 0.36 false positives", "keywords": ["wavelet transform", "shannon entropy", "tsallis entropy", "otsu", "microcalcifications and mammograms"]}
{"id": "kp20k_training_500", "title": "An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition", "abstract": "This article provides an overview of the first BioASQ challenge, a competition on large-scale biomedical semantic indexing and question answering (QA), which took place between March and September 2013. BioASQ assesses the ability of systems to semantically index very large numbers of biomedical scientific articles, and to return concise and user-understandable answers to given natural language questions by combining information from biomedical articles and ontologies", "keywords": ["bioasq competition", "hierarchical text classification", "semantic indexing", "information retrieval", "passage retrieval", "question answering", "multi-document text summarization"]}
{"id": "kp20k_training_501", "title": "Regreening the Metropolis: Pathways to More Ecological Cities: Keynote Address", "abstract": "Eighty percent of the American population now lives in metropolitan regions whose geographic extent continues to expand even as many core cities and inner-tier suburbs lose middle-class populations, jobs, and tax base. Urban sprawl and the socioeconomic polarizing of metropolitan America have been fostered by public policies including (1) federal subsidies for new infrastructure on the urban fringe; (2) tax policies that favor home ownership over rental properties; (3) local zoning codes; and (4) federal and state neglect of older urban neighborhoods. In the face of diminished access to nature outside of metropolitan areas, locally based efforts to protect and restore greenspaces within urban areas seek to make older communities more habitable and more ecological. Some pathways to more ecological cities include the following", "keywords": ["urban ecology", "city nature", "urban biodiversity", "spirit of place"]}
{"id": "kp20k_training_502", "title": "A modified runs test for symmetry", "abstract": "We propose a modification of a ModarresGastwirth test for the hypothesis of symmetry about a known center. By means of a Monte Carlo Study we show that the modified test overtakes the original ModarresGastwirth test for a wide spectrum of asymmetrical alternatives coming from the lambda family and for all assayed sample sizes. We also show that our test is the best runs test among the runs tests we have compared", "keywords": ["runs test", "test of symmetry", "generalized lambda family", "power", "primary secondary "]}
{"id": "kp20k_training_503", "title": "Probability-based approaches to VLSI circuit partitioning", "abstract": "Iterative-improvement two-way min-cut partitioning is an important phase in most circuit placement tools, and finds use in many other computer-aided design (CAD) applications. Most iterative improvement techniques for circuit netlists like the Fiduccia-Mattheyses (FM) method compute the gains of nodes using local netlist information that is only concerned with the immediate improvement in the cutset, This can lead to misleading gain information. Krishnamurthy suggested a lookahead (LA) gain calculation method to ameliorate this situation; however, as we show, it leaves room for improvement. We present here a probabilistic gain computation approach called probabilistic partitioner (PROP) that is capable of capturing the future implications of moving a node at the current time. We also propose an extended algorithm SHRINK-PROP that increases the provability of removing recently \"perturbed\" nets (nets whose nodes have been moved for the first time) from the cutset, Experimental results on medium- to large-size ACM/SIGDA benchmark circuits show that PROP and SHRINK-PROP outperform previous iterative-improvement methods like FM (bq. about 30% and 37%, respectively) and LA (by about 27% and 34%, respectively). Both PROP and SHRINK-PROP also obtain much better cutsizes than many recent state-of-the-art partitioners like EIG1, WINDOW MELO, PARABOLI, GFM and CMetis (by 4.5% to 67%). Our empirical timing results reveal that PROP is appreciably Faster than most recent techniques, We also obtain results on the more recent ISPD-98 benchmark suite that show similar substantial mincut improvements by PROP and SHRINK-PROP over FM (24% and 31%, respectively). it is also noteworthy that SHRINK-PROP's results are within 2.5% of those obtained by hMetis. one of the best multilevel partitioners. However. the multilevel paradigm is orthogonal to SHRINK-PROP. Further, since it is a \"flat\" partitioner, it has advantages over hMetis in partition-driven placement applications", "keywords": ["clustering effect", "iterative improvement", "min-cut partitioning", "probabilistic gain", "vlsi circuit"]}
{"id": "kp20k_training_504", "title": "ShengBTE: A solver of the Boltzmann transport equation for phonons", "abstract": "ShengBTE is a software package for computing the lattice thermal conductivity of crystalline bulk materials and nanowires with diffusive boundary conditions. It is based on a full iterative solution to the Boltzmann transport equation. Its main inputs are sets of second- and third-order interatomic force constants, which can be calculated using third-party ab-initio packages. Dirac delta distributions arising from conservation of energy are approximated by Gaussian functions. A locally adaptive algorithm is used to determine each process-specific broadening parameter, which renders the method fully parameter free. The code is free software, written in Fortran and parallelized using MPI. A complementary Python script to help compute third-order interatomic force constants from a minimum number of ab-initio calculations, using a real-space finite-difference approach, is also publicly available for download. Here we discuss the design and implementation of both pieces of software and present results for three example systems: Si, InAs and lonsdaleite. Program title: ShengBTE Catalogue identifier: AESL_v1_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AESL_v1_0.html Program obtainable from: CPC Program Library, Queens University, Belfast, N. Ireland Licensing provisions: GNU General Public License, version 3 No. of lines in distributed program, including test data, etc.: 292052 No. of bytes in distributed program, including test data, etc.: 1989781 Distribution format: tar.gz Programming language: Fortran 90, MPI. Computer: Non-specific. Operating system: Unix/Linux. Has the code been vectorized or parallelized?: Yes, parallelized using MPI. RAM: Up to several GB Classification: 7.9. External routines: LAPACK, MPI, spglib (http://spglib.sourceforge.net/) Nature of problem: Calculation of thermal conductivity and related quantities, determination of scattering rates for allowed three-phonon processes Solution method: Iterative solution, locally adaptive Gaussian broadening Running time: Up to several hours on several tens of processors", "keywords": ["boltzmann transport equation", "thermal conductivity", "phonon"]}
{"id": "kp20k_training_505", "title": "Tracing impact in a usability improvement process", "abstract": "Analyzing usability improvement processes as they take place in real-life organizations is necessary to understand the practice of usability work. This paper describes a case study where the usability of an information system is improved and a relationship between the improvements and the evaluation efforts is established. Results show that evaluation techniques complemented each other by suggesting different kinds of usability improvement. Among the techniques applied, a combination of questionnaires and Metaphors of Human Thinking (MOT) showed the largest mean impact and MOT produced the largest number of impacts. Logging of real-life use of the system over 6 months indicated six aspects of improved usability, where significant differences among evaluation techniques were found. Concerning five of the six aspects Think Aloud evaluations and the above-mentioned combination of questionnaire and MOT performed equally well, and better than MOT. Based on the evaluations 40 redesign proposals were developed and 30 of these were implemented. Four of the implemented redesigns where considered especially important. These evolved with inspiration from multiple evaluations and were informed by stakeholders with different kinds of expertise. Our results suggest that practitioners should not rely on isolated evaluations. Instead complementing techniques should be combined, and people with different expertise should be involved.  ", "keywords": ["usability engineering", "case study", "usability improvement process", "metaphors of human thinking", "think loud", "questionnaire"]}
{"id": "kp20k_training_506", "title": "HOMAN, a learning based negotiation method for holonic multi-agent systems", "abstract": "Holonic multi-agent systems are a special category of multi-agent systems that best fit to environments with numerous agents and high complexity. Like in general multi-agent systems, the agents in the holonic system may negotiate with each other. These systems have their own characteristics and structure, for which a specific negotiation mechanism is required. This mechanism should be simple, fast and operable in real world applications. It would be better to equip negotiators with a learning method which can efficiently use the available information. The learning method should itself be fast, too. Additionally, this mechanism should match the special characteristics of the holonic multi-agent systems. In this paper, we introduce such a negotiation method. Experimental results demonstrate the efficiency of this new approach", "keywords": ["holonic multi-agent systems", "negotiation", "semi-cooperative", "agreement", "regression"]}
