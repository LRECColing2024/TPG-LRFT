{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return data\n",
    "\n",
    "def save_as_jsonl(data_list, file_path):\n",
    "    \"\"\"\n",
    "    Save a list of dictionaries as a jsonl file.\n",
    "    \n",
    "    :param data_list: List of dictionaries.\n",
    "    :param file_path: Path to the jsonl file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        for entry in data_list:\n",
    "            json_str = json.dumps(entry, ensure_ascii=False)\n",
    "            file.write(json_str + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp20k_train = read_jsonl('kp20k/train.json')\n",
    "kptimes_train = read_jsonl('kptimes/train.json')\n",
    "stackexchange_train = read_jsonl('stackexchange/train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/user01/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/user01/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def meng17_tokenize(text):\n",
    "    '''\n",
    "    The tokenizer used in Meng et al. ACL 2017\n",
    "    parse the feed-in text, filtering and tokenization\n",
    "    keep [_<>,\\(\\)\\.\\'%], replace digits with <digit>, split by [^a-zA-Z0-9_<>,\\(\\)\\.\\'%]\n",
    "    :param text:\n",
    "    :return: a list of tokens\n",
    "    '''\n",
    "    # remove line breakers\n",
    "    text = re.sub(r'[\\r\\n\\t]', ' ', text)\n",
    "    # pad spaces to the left and right of special punctuations\n",
    "    text = re.sub(r'[_<>,\\(\\)\\.\\'%]', ' \\g<0> ', text)\n",
    "    # tokenize by non-letters (new-added + # & *, but don't pad spaces, to make them as one whole word)\n",
    "    tokens = list(filter(lambda w: len(w) > 0, re.split(r'[^a-zA-Z0-9_<>,#&\\+\\*\\(\\)\\.\\']', text)))\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def extract_candidates(text):\n",
    "\n",
    "    GRAMMAR_EN = \"\"\"  NP:\n",
    "{<NN.*|JJ>*<NN.*>}\"\"\"   # Adjective(s)(optional) + Noun(s)\n",
    "    keyphrase_candidate = set()\n",
    "    \n",
    "\n",
    "    np_parser = nltk.RegexpParser(GRAMMAR_EN)  # Noun phrase parser\n",
    "    \n",
    "    tag = nltk.pos_tag_sents(nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(text))\n",
    "\n",
    "    \n",
    "    trees = np_parser.parse_sents(tag)  # Generator with one tree per sentence\n",
    "    #print(text)\n",
    "\n",
    "    for tree in trees:\n",
    "        for subtree in tree.subtrees(filter=lambda t: t.label() == 'NP'):  # For each nounphrase\n",
    "            # Concatenate the token with a space\n",
    "            keyphrase_candidate.add(' '.join(word for word, tag in subtree.leaves()))\n",
    "    \n",
    "    #print(keyphrase_candidate)\n",
    "    keyphrase_candidate = {kp for kp in keyphrase_candidate if len(kp.split()) <= 4}\n",
    "    #print(keyphrase_candidate)\n",
    "  \n",
    "    return list(keyphrase_candidate)\n",
    "\n",
    "\n",
    "def title_candidates_extraction(title, text):\n",
    "    \n",
    "    cans = extract_candidates(title)\n",
    "    candidates = []\n",
    "    for can in cans:\n",
    "        candidates.append(can.lower())\n",
    "\n",
    "    candidates = list(set(candidates))\n",
    "    \n",
    "    present_phrases = []\n",
    "    absent_phrases = []\n",
    "    text_low = text.lower()\n",
    "    tokenized_text = meng17_tokenize(text_low)\n",
    "    stem_text = [ stemmer.stem(word) for word in tokenized_text ]\n",
    "    stem_text = ' '.join(stem_text)\n",
    "\n",
    "    # stem_text = ' '.join(meng17_tokenize(text_low))\n",
    "    # print(stem_text)\n",
    "\n",
    "\n",
    "    for p in candidates:\n",
    "        tokenized_p = meng17_tokenize(p.lower())\n",
    "        stem_p = [ stemmer.stem(word) for word in tokenized_p ]\n",
    "        stem_p = ' '.join(stem_p)\n",
    "        # print(stem_p)\n",
    "    \n",
    "        if stem_p not in stem_text:\n",
    "            absent_phrases.append(p)\n",
    "        else:\n",
    "            present_phrases.append(p)\n",
    "\n",
    "    return present_phrases, absent_phrases\n",
    "\n",
    "\n",
    "def preprocess(dataset, name='kp20k'):\n",
    "\n",
    "    processed_dataset = []\n",
    "\n",
    "    for data in tqdm(dataset):\n",
    "\n",
    "        temp = {}\n",
    "        if name == 'kp20k':\n",
    "            temp['title'] = data['title']\n",
    "            title_present_phrase, title_absent_phrase = title_candidates_extraction(data['title'], data['abstract'])\n",
    "            temp['abstract'] = data['abstract']\n",
    "            temp['title_present_phrase'] = title_present_phrase\n",
    "            temp['title_absent_phrase'] = title_absent_phrase\n",
    "            temp['keyphrases'] = data['keywords']\n",
    "        elif name == 'kptimes':\n",
    "            temp['title'] = data['title']\n",
    "            title_present_phrase, title_absent_phrase = title_candidates_extraction(data['title'], data['abstract'])\n",
    "            temp['abstract'] =data['abstract']\n",
    "            temp['title_present_phrase'] = title_present_phrase\n",
    "            temp['title_absent_phrase'] = title_absent_phrase\n",
    "            temp['keyphrases'] = data['keyword'].split(';')\n",
    "        elif name =='stackexchange':\n",
    "            temp['title'] = data['title']\n",
    "            title_present_phrase, title_absent_phrase = title_candidates_extraction(data['title'], data['question'])\n",
    "            temp['abstract'] = data['question']\n",
    "            temp['title_present_phrase'] = title_present_phrase\n",
    "            temp['title_absent_phrase'] = title_absent_phrase\n",
    "            temp['keyphrases'] = data['tags'].split(';')\n",
    "        processed_dataset.append(temp)\n",
    "\n",
    "    return processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/514154 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 514154/514154 [29:51<00:00, 286.99it/s] \n",
      "100%|██████████| 259923/259923 [57:26<00:00, 75.41it/s]  \n",
      "100%|██████████| 298965/298965 [19:25<00:00, 256.43it/s] \n"
     ]
    }
   ],
   "source": [
    "kp20k = preprocess(kp20k_train, 'kp20k')\n",
    "kptimes = preprocess(kptimes_train, 'kptimes')\n",
    "stackexchange = preprocess(stackexchange_train, 'stackexchange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_data(data_list):\n",
    "    \"\"\"\n",
    "    Calculate and aggregate the statistics from the given data list.\n",
    "    \n",
    "    :param data_list: List of dictionaries with the given structure.\n",
    "    :return: A dictionary with the aggregated statistics.\n",
    "    \"\"\"\n",
    "    \n",
    "    total_title_absent = 0\n",
    "    total_title_present = 0\n",
    "    count_title_absent_1 = 0\n",
    "    count_title_absent_2 = 0\n",
    "    count_title_absent_3 = 0\n",
    "    count_title_absent_4 = 0\n",
    "    count_title_absent_5 = 0\n",
    "    total_absent_docs = 0\n",
    "    \n",
    "\n",
    "    for entry in data_list:\n",
    "        total_title_absent += len(entry['title_absent_phrase'])\n",
    "        total_title_present += len(entry['title_present_phrase'])\n",
    "\n",
    "        if len(entry['title_absent_phrase']) > 0:\n",
    "            total_absent_docs += 1\n",
    "\n",
    "        if len(entry['title_absent_phrase']) == 1:\n",
    "            count_title_absent_1 += 1\n",
    "        elif len(entry['title_absent_phrase']) == 2:\n",
    "            count_title_absent_2 += 1\n",
    "        elif len(entry['title_absent_phrase']) == 3:\n",
    "            count_title_absent_3 += 1\n",
    "        elif len(entry['title_absent_phrase']) == 4:\n",
    "            count_title_absent_4 += 1\n",
    "        elif len(entry['title_absent_phrase']) >= 5:\n",
    "            count_title_absent_5 += 1\n",
    "            \n",
    "\n",
    "    avg_title_absent = total_title_absent / len(data_list) if data_list else 0\n",
    "    avg_title_present = total_title_present / len(data_list) if data_list else 0\n",
    "\n",
    "    total_avg = avg_title_absent + avg_title_present\n",
    "    percent_title_present = (avg_title_present / total_avg) * 100 if total_avg else 0\n",
    "    percent_title_absent = 100 - percent_title_present\n",
    "\n",
    "    return {\n",
    "        'avg_title_present': avg_title_present,\n",
    "        'avg_title_absent': avg_title_absent,\n",
    "        'percent_title_present': percent_title_present,\n",
    "        'percent_title_absent': percent_title_absent,\n",
    "        'count_title_absent_1': count_title_absent_1,\n",
    "        'count_title_absent_2': count_title_absent_2,\n",
    "        'count_title_absent_3': count_title_absent_3,\n",
    "        'count_title_absent_4': count_title_absent_4,\n",
    "        'count_title_absent_5': count_title_absent_5,\n",
    "        'total_docs': len(data_list),\n",
    "        'total_absent_docs': total_absent_docs,\n",
    "        'absent_ratio': total_absent_docs / len(data_list) * 100,\n",
    "        'absent2_ratio': (count_title_absent_2 + count_title_absent_3 + count_title_absent_4 + count_title_absent_5)/ len(data_list) * 100,\n",
    "        'absent3_ratio': (count_title_absent_3 + count_title_absent_4 + count_title_absent_5) / len(data_list) * 100,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg_title_present': 1.7642885205599879,\n",
       " 'avg_title_absent': 1.2462297288361075,\n",
       " 'percent_title_present': 58.60414634303914,\n",
       " 'percent_title_absent': 41.39585365696086,\n",
       " 'count_title_absent_1': 195884,\n",
       " 'count_title_absent_2': 127783,\n",
       " 'count_title_absent_3': 43086,\n",
       " 'count_title_absent_4': 10671,\n",
       " 'count_title_absent_5': 3252,\n",
       " 'total_docs': 514154,\n",
       " 'total_absent_docs': 380676,\n",
       " 'absent_ratio': 74.03929561960035,\n",
       " 'absent2_ratio': 35.94098266278197,\n",
       " 'absent3_ratio': 11.087923073631636}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kp20k_stat = aggregate_data(kp20k)\n",
    "kp20k_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg_title_present': 1.330470947165122,\n",
       " 'avg_title_absent': 1.2827144962161872,\n",
       " 'percent_title_present': 50.91375931757719,\n",
       " 'percent_title_absent': 49.08624068242281,\n",
       " 'count_title_absent_1': 105937,\n",
       " 'count_title_absent_2': 80820,\n",
       " 'count_title_absent_3': 19193,\n",
       " 'count_title_absent_4': 1926,\n",
       " 'count_title_absent_5': 108,\n",
       " 'total_docs': 259923,\n",
       " 'total_absent_docs': 207984,\n",
       " 'absent_ratio': 80.01754365716,\n",
       " 'absent2_ratio': 39.260473294014,\n",
       " 'absent3_ratio': 8.1666493538471}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kptimes_stat = aggregate_data(kptimes)\n",
    "kptimes_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg_title_present': 1.5148361848376901,\n",
       " 'avg_title_absent': 0.955640292341913,\n",
       " 'percent_title_present': 61.31757168427237,\n",
       " 'percent_title_absent': 38.68242831572763,\n",
       " 'count_title_absent_1': 130333,\n",
       " 'count_title_absent_2': 55508,\n",
       " 'count_title_absent_3': 11637,\n",
       " 'count_title_absent_4': 1911,\n",
       " 'count_title_absent_5': 347,\n",
       " 'total_docs': 298965,\n",
       " 'total_absent_docs': 199736,\n",
       " 'absent_ratio': 66.80915826267288,\n",
       " 'absent2_ratio': 23.214423093004196,\n",
       " 'absent3_ratio': 4.647701235930627}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stackexchange_stat = aggregate_data(stackexchange)\n",
    "stackexchange_stat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
